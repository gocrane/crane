{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 The goal of Crane is to provide a one-stop-shop project to help Kubernetes users to save cloud resource usage with a rich set of functionalities: Time Series Prediction based on monitoring data Usage and Cost visibility Usage & Cost Optimization including: R2 (Resource Re-allocation) R3 (Request & Replicas Recommendation) Effective Pod Autoscaling (Effective Horizontal & Vertical Pod Autoscaling) Cost Optimization Enhanced QoS based on Pod PriorityClass Load-aware Scheduling Features \u00b6 Time Series Prediction \u00b6 TimeSeriesPrediction defines metric spec to predict kubernetes resources like Pod or Node. The prediction module is the core component that other crane components relied on, like EHPA and Analytics . Please see this document to learn more. Effective HorizontalPodAutoscaler \u00b6 EffectiveHorizontalPodAutoscaler helps you manage application scaling in an easy way. It is compatible with native HorizontalPodAutoscaler but extends more features like prediction-driven autoscaling. Please see this document to learn more. Analytics \u00b6 Analytics model analyzes the workload and provide recommendations about resource optimize. Two Recommendations are currently supported: ResourceRecommend : Replicas recommendation analyze the actual application usage and give advice for replicas and HPA configurations. HPARecommend : Resource recommendation allows you to obtain recommended values for resources in a cluster and use them to improve the resource utilization of the cluster. Please see this document to learn more. QoS Ensurance \u00b6 Kubernetes is capable of starting multiple pods on same node, and as a result, some of the user applications may be impacted when there are resources(e.g. cpu) consumption competition. To mitigate this, Crane allows users defining PrioirtyClass for the pods and QoSEnsurancePolicy, and then detects disruption and ensure the high priority pods not being impacted by resource competition. Avoidance Actions: Disable Schedule : disable scheduling by setting node taint and condition Throttle : throttle the low priority pods by squeezing cgroup settings Evict : evict low priority pods Please see this document to learn more. Load-aware Scheduling \u00b6 Native scheduler of kubernetes can only schedule pods by resource request, which can easily cause a series of load uneven problems. In contrast, Crane-scheduler can get the actual load of kubernetes nodes from Prometheus, and achieve more efficient scheduling. Please see this document to learn more. Repositories \u00b6 Crane is composed of the following components: craned - main crane control plane. Predictor - Predicts resources metrics trends based on historical data. AnalyticsController - Analyzes resources and generate related recommendations. RecommendationController - Recommend Pod resource requests and autoscaler. ClusterNodePredictionController - Create Predictor for nodes. EffectiveHPAController - Effective HPA for horizontal scaling. EffectiveVPAController - Effective VPA for vertical scaling. metric-adaptor - Metric server for driving the scaling. crane-agent - Ensure critical workloads SLO based on abnormally detection. gocrane/api - This repository defines component-level APIs for the Crane platform. gocrane/fadvisor - Financial advisor which collect resource prices from cloud API. gocrane/crane-scheduler - A Kubernetes scheduler which can schedule pod based on actual node load.","title":"Introduction"},{"location":"#introduction","text":"The goal of Crane is to provide a one-stop-shop project to help Kubernetes users to save cloud resource usage with a rich set of functionalities: Time Series Prediction based on monitoring data Usage and Cost visibility Usage & Cost Optimization including: R2 (Resource Re-allocation) R3 (Request & Replicas Recommendation) Effective Pod Autoscaling (Effective Horizontal & Vertical Pod Autoscaling) Cost Optimization Enhanced QoS based on Pod PriorityClass Load-aware Scheduling","title":"Introduction"},{"location":"#features","text":"","title":"Features"},{"location":"#time-series-prediction","text":"TimeSeriesPrediction defines metric spec to predict kubernetes resources like Pod or Node. The prediction module is the core component that other crane components relied on, like EHPA and Analytics . Please see this document to learn more.","title":"Time Series Prediction"},{"location":"#effective-horizontalpodautoscaler","text":"EffectiveHorizontalPodAutoscaler helps you manage application scaling in an easy way. It is compatible with native HorizontalPodAutoscaler but extends more features like prediction-driven autoscaling. Please see this document to learn more.","title":"Effective HorizontalPodAutoscaler"},{"location":"#analytics","text":"Analytics model analyzes the workload and provide recommendations about resource optimize. Two Recommendations are currently supported: ResourceRecommend : Replicas recommendation analyze the actual application usage and give advice for replicas and HPA configurations. HPARecommend : Resource recommendation allows you to obtain recommended values for resources in a cluster and use them to improve the resource utilization of the cluster. Please see this document to learn more.","title":"Analytics"},{"location":"#qos-ensurance","text":"Kubernetes is capable of starting multiple pods on same node, and as a result, some of the user applications may be impacted when there are resources(e.g. cpu) consumption competition. To mitigate this, Crane allows users defining PrioirtyClass for the pods and QoSEnsurancePolicy, and then detects disruption and ensure the high priority pods not being impacted by resource competition. Avoidance Actions: Disable Schedule : disable scheduling by setting node taint and condition Throttle : throttle the low priority pods by squeezing cgroup settings Evict : evict low priority pods Please see this document to learn more.","title":"QoS Ensurance"},{"location":"#load-aware-scheduling","text":"Native scheduler of kubernetes can only schedule pods by resource request, which can easily cause a series of load uneven problems. In contrast, Crane-scheduler can get the actual load of kubernetes nodes from Prometheus, and achieve more efficient scheduling. Please see this document to learn more.","title":"Load-aware Scheduling"},{"location":"#repositories","text":"Crane is composed of the following components: craned - main crane control plane. Predictor - Predicts resources metrics trends based on historical data. AnalyticsController - Analyzes resources and generate related recommendations. RecommendationController - Recommend Pod resource requests and autoscaler. ClusterNodePredictionController - Create Predictor for nodes. EffectiveHPAController - Effective HPA for horizontal scaling. EffectiveVPAController - Effective VPA for vertical scaling. metric-adaptor - Metric server for driving the scaling. crane-agent - Ensure critical workloads SLO based on abnormally detection. gocrane/api - This repository defines component-level APIs for the Crane platform. gocrane/fadvisor - Financial advisor which collect resource prices from cloud API. gocrane/crane-scheduler - A Kubernetes scheduler which can schedule pod based on actual node load.","title":"Repositories"},{"location":"CONTRIBUTING/","text":"Contributing to Crane \u00b6 Welcome to Crane! This document is a guideline about how to contribute to Crane. Become a contributor \u00b6 You can contribute to Crane in several ways. Here are some examples: Contribute to the Crane codebase. Report bugs. Suggest enhancements. Write technical documentation and blog posts, for users and contributors. Organize meetups and user groups in your local area. Help others by answering questions about Crane. For more ways to contribute, check out the Open Source Guides . Report bugs \u00b6 Before submitting a new issue, try to make sure someone hasn't already reported the problem. Look through the existing issues for similar issues. Report a bug by submitting a bug report . Make sure that you provide as much information as possible on how to reproduce the bug. Suggest enhancements \u00b6 If you have an idea to improve Crane, submit an feature request .","title":"Contributing"},{"location":"CONTRIBUTING/#contributing-to-crane","text":"Welcome to Crane! This document is a guideline about how to contribute to Crane.","title":"Contributing to Crane"},{"location":"CONTRIBUTING/#become-a-contributor","text":"You can contribute to Crane in several ways. Here are some examples: Contribute to the Crane codebase. Report bugs. Suggest enhancements. Write technical documentation and blog posts, for users and contributors. Organize meetups and user groups in your local area. Help others by answering questions about Crane. For more ways to contribute, check out the Open Source Guides .","title":"Become a contributor"},{"location":"CONTRIBUTING/#report-bugs","text":"Before submitting a new issue, try to make sure someone hasn't already reported the problem. Look through the existing issues for similar issues. Report a bug by submitting a bug report . Make sure that you provide as much information as possible on how to reproduce the bug.","title":"Report bugs"},{"location":"CONTRIBUTING/#suggest-enhancements","text":"If you have an idea to improve Crane, submit an feature request .","title":"Suggest enhancements"},{"location":"code-standards/","text":"Code standards \u00b6 This doc describes the code standards and suggestion for crane project, mainly for new contributor of the project import need to be organized \u00b6 import should be categorized with blank line as system imports, community imports and crane apis and crane imports, like the following example import ( \"reflect\" \"sync\" \"time\" vpa \"k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/util\" \"github.com/gocrane/api/prediction/v1alpha1\" \"github.com/gocrane/crane/pkg/utils\" \"github.com/gocrane/crane/pkg/prediction/config\" ) logs standard \u00b6 logs are required for troubleshooting purpose log message should always start with capital letter log message should be a complete sentence that contains enough context, for example: object key, action, parameters, status, error message by default, you don't need to set log level set 4 for debug level. set 6 for more detail debug level. set 10 for massive data log level. can use klog.KObj() to contain object key to let we know which object the message is printed for klog . Infof ( \"Failed to setup webhook %s\" , \"value\" ) klog . V ( 4 ). Infof ( \"Debug info %s\" , \"value\" ) klog . Errorf ( \"Failed to get scale, ehpa %s error %v\" , klog . KObj ( ehpa ), err ) klog . Error ( error ) klog . ErrorDepth ( 5 , fmt . Errorf ( \"failed to get ehpa %s: %v\" , klog . KObj ( ehpa ), err )) event is needed for critical reconcile loop \u00b6 event is to let user know what happens on serverside, only print info we want user to know consider failure paths and success paths event do not need the object key c . Recorder . Event ( ehpa , v1 . EventTypeNormal , \"FailedGetSubstitute\" , err . Error ()) comment \u00b6 every interface should have comments to clarify comment should be a complete sentence // Interface is a source of monitoring metric that provides metrics that can be used for // prediction, such as 'cpu usage', 'memory footprint', 'request per second (qps)', etc. type Interface interface { // GetTimeSeries returns the metric time series that meet the given // conditions from the specified time range. GetTimeSeries ( metricName string , Conditions [] common . QueryCondition , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // GetLatestTimeSeries returns the latest metric values that meet the given conditions. GetLatestTimeSeries ( metricName string , Conditions [] common . QueryCondition ) ([] * common . TimeSeries , error ) // QueryTimeSeries returns the time series based on a promql like query string. QueryTimeSeries ( queryExpr string , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // QueryLatestTimeSeries returns the latest metric values that meet the given query. QueryLatestTimeSeries ( queryExpr string ) ([] * common . TimeSeries , error ) } functions \u00b6 function name should clarify what do this function do, for example: verb + noun similar functions should be refactored, merge or divide them common functions should move to common folder like utils variable \u00b6 variable name should clarify what do this variable does, better not use too short name and too simple name better to use more meaningful variable name for tmp variable, for example: foo loop folder and file \u00b6 folder name should be letter with lower case and number file name should be letter and number and _ unit test \u00b6 Test-driven developing Complex function that include condition decide should add unit test for it don't forget to run make fmt before you submit code \u00b6","title":"Code Standard"},{"location":"code-standards/#code-standards","text":"This doc describes the code standards and suggestion for crane project, mainly for new contributor of the project","title":"Code standards"},{"location":"code-standards/#import-need-to-be-organized","text":"import should be categorized with blank line as system imports, community imports and crane apis and crane imports, like the following example import ( \"reflect\" \"sync\" \"time\" vpa \"k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/util\" \"github.com/gocrane/api/prediction/v1alpha1\" \"github.com/gocrane/crane/pkg/utils\" \"github.com/gocrane/crane/pkg/prediction/config\" )","title":"import need to be organized"},{"location":"code-standards/#logs-standard","text":"logs are required for troubleshooting purpose log message should always start with capital letter log message should be a complete sentence that contains enough context, for example: object key, action, parameters, status, error message by default, you don't need to set log level set 4 for debug level. set 6 for more detail debug level. set 10 for massive data log level. can use klog.KObj() to contain object key to let we know which object the message is printed for klog . Infof ( \"Failed to setup webhook %s\" , \"value\" ) klog . V ( 4 ). Infof ( \"Debug info %s\" , \"value\" ) klog . Errorf ( \"Failed to get scale, ehpa %s error %v\" , klog . KObj ( ehpa ), err ) klog . Error ( error ) klog . ErrorDepth ( 5 , fmt . Errorf ( \"failed to get ehpa %s: %v\" , klog . KObj ( ehpa ), err ))","title":"logs standard"},{"location":"code-standards/#event-is-needed-for-critical-reconcile-loop","text":"event is to let user know what happens on serverside, only print info we want user to know consider failure paths and success paths event do not need the object key c . Recorder . Event ( ehpa , v1 . EventTypeNormal , \"FailedGetSubstitute\" , err . Error ())","title":"event is needed for critical reconcile loop"},{"location":"code-standards/#comment","text":"every interface should have comments to clarify comment should be a complete sentence // Interface is a source of monitoring metric that provides metrics that can be used for // prediction, such as 'cpu usage', 'memory footprint', 'request per second (qps)', etc. type Interface interface { // GetTimeSeries returns the metric time series that meet the given // conditions from the specified time range. GetTimeSeries ( metricName string , Conditions [] common . QueryCondition , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // GetLatestTimeSeries returns the latest metric values that meet the given conditions. GetLatestTimeSeries ( metricName string , Conditions [] common . QueryCondition ) ([] * common . TimeSeries , error ) // QueryTimeSeries returns the time series based on a promql like query string. QueryTimeSeries ( queryExpr string , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // QueryLatestTimeSeries returns the latest metric values that meet the given query. QueryLatestTimeSeries ( queryExpr string ) ([] * common . TimeSeries , error ) }","title":"comment"},{"location":"code-standards/#functions","text":"function name should clarify what do this function do, for example: verb + noun similar functions should be refactored, merge or divide them common functions should move to common folder like utils","title":"functions"},{"location":"code-standards/#variable","text":"variable name should clarify what do this variable does, better not use too short name and too simple name better to use more meaningful variable name for tmp variable, for example: foo loop","title":"variable"},{"location":"code-standards/#folder-and-file","text":"folder name should be letter with lower case and number file name should be letter and number and _","title":"folder and file"},{"location":"code-standards/#unit-test","text":"Test-driven developing Complex function that include condition decide should add unit test for it","title":"unit test"},{"location":"code-standards/#dont-forget-to-run-make-fmt-before-you-submit-code","text":"","title":"don't forget to run make fmt before you submit code"},{"location":"installation/","text":"Installation \u00b6 Prerequisites \u00b6 Kubernetes 1.18+ Helm 3.1.0 Steps \u00b6 Helm Installation \u00b6 Please refer to Helm's documentation for installation. Installing prometheus and grafana with helm chart \u00b6 Note If you already deployed prometheus, grafana in your environment, then skip this step. Network Problems If your network is hard to connect GitHub resources, you can try the mirror repo. Like GitHub Release, GitHub Raw Content raw.githubusercontent.com . But mirror repo has a certain latency . Mirror Repo Crane use prometheus to be the default metric provider. Using following command to install prometheus components: prometheus-server, node-exporter, kube-state-metrics. Main Mirror helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/prometheus/override_values.yaml \\ --create-namespace prometheus-community/prometheus helm repo add prometheus-community https://finops-helm.pkg.coding.net/gocrane/prometheus-community helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/prometheus/override_values.yaml?download = false \\ --create-namespace prometheus-community/prometheus Fadvisor use grafana to present cost estimates. Using following command to install a grafana. Main Mirror helm repo add grafana https://grafana.github.io/helm-charts helm install grafana \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/grafana/override_values.yaml \\ -n crane-system \\ --create-namespace grafana/grafana helm repo add grafana https://finops-helm.pkg.coding.net/gocrane/grafana helm install grafana \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false \\ -n crane-system \\ --create-namespace grafana/grafana Deploying Crane and Fadvisor \u00b6 Main Mirror helm repo add crane https://gocrane.github.io/helm-charts helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor helm repo add crane https://finops-helm.pkg.coding.net/gocrane/gocrane helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor Deploying Crane-scheduler(optional) \u00b6 helm install scheduler -n crane-system --create-namespace crane/scheduler Verify Installation \u00b6 Check deployments are all available by running: kubectl get deploy -n crane-system The output is similar to: NAME READY STATUS RESTARTS AGE crane-agent-8h7df 1 /1 Running 0 119m crane-agent-8qf5n 1 /1 Running 0 119m crane-agent-h9h5d 1 /1 Running 0 119m craned-5c69c684d8-dxmhw 2 /2 Running 0 20m grafana-7fddd867b4-kdxv2 1 /1 Running 0 41m metric-adapter-94b6f75b-k8h7z 1 /1 Running 0 119m prometheus-kube-state-metrics-6dbc9cd6c9-dfmkw 1 /1 Running 0 45m prometheus-node-exporter-bfv74 1 /1 Running 0 45m prometheus-node-exporter-s6zps 1 /1 Running 0 45m prometheus-node-exporter-x5rnm 1 /1 Running 0 45m prometheus-server-5966b646fd-g9vxl 2 /2 Running 0 45m you can see this to learn more. Customize Installation \u00b6 Deploy Crane by apply YAML declaration. Main Mirror git clone https://github.com/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter git clone https://e.coding.net/finops/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter The following command will configure prometheus http address for crane if you want to customize it. Specify CUSTOMIZE_PROMETHEUS if you have existing prometheus server. export CUSTOMIZE_PROMETHEUS = if [ $CUSTOMIZE_PROMETHEUS ] ; then sed -i '' \"s/http:\\/\\/prometheus-server.crane-system.svc.cluster.local:8080/ ${ CUSTOMIZE_PROMETHEUS } /\" deploy/craned/deployment.yaml ; fi Get your Kubernetes Cost Report \u00b6 Get the Grafana URL to visit by running these commands in the same shell: export POD_NAME = $( kubectl get pods --namespace crane-system -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl --namespace crane-system port-forward $POD_NAME 3000 visit Cost Report here with account(admin:admin).","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#prerequisites","text":"Kubernetes 1.18+ Helm 3.1.0","title":"Prerequisites"},{"location":"installation/#steps","text":"","title":"Steps"},{"location":"installation/#helm-installation","text":"Please refer to Helm's documentation for installation.","title":"Helm Installation"},{"location":"installation/#installing-prometheus-and-grafana-with-helm-chart","text":"Note If you already deployed prometheus, grafana in your environment, then skip this step. Network Problems If your network is hard to connect GitHub resources, you can try the mirror repo. Like GitHub Release, GitHub Raw Content raw.githubusercontent.com . But mirror repo has a certain latency . Mirror Repo Crane use prometheus to be the default metric provider. Using following command to install prometheus components: prometheus-server, node-exporter, kube-state-metrics. Main Mirror helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/prometheus/override_values.yaml \\ --create-namespace prometheus-community/prometheus helm repo add prometheus-community https://finops-helm.pkg.coding.net/gocrane/prometheus-community helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/prometheus/override_values.yaml?download = false \\ --create-namespace prometheus-community/prometheus Fadvisor use grafana to present cost estimates. Using following command to install a grafana. Main Mirror helm repo add grafana https://grafana.github.io/helm-charts helm install grafana \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/grafana/override_values.yaml \\ -n crane-system \\ --create-namespace grafana/grafana helm repo add grafana https://finops-helm.pkg.coding.net/gocrane/grafana helm install grafana \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false \\ -n crane-system \\ --create-namespace grafana/grafana","title":"Installing prometheus and grafana with helm chart"},{"location":"installation/#deploying-crane-and-fadvisor","text":"Main Mirror helm repo add crane https://gocrane.github.io/helm-charts helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor helm repo add crane https://finops-helm.pkg.coding.net/gocrane/gocrane helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor","title":"Deploying Crane and Fadvisor"},{"location":"installation/#deploying-crane-scheduleroptional","text":"helm install scheduler -n crane-system --create-namespace crane/scheduler","title":"Deploying Crane-scheduler(optional)"},{"location":"installation/#verify-installation","text":"Check deployments are all available by running: kubectl get deploy -n crane-system The output is similar to: NAME READY STATUS RESTARTS AGE crane-agent-8h7df 1 /1 Running 0 119m crane-agent-8qf5n 1 /1 Running 0 119m crane-agent-h9h5d 1 /1 Running 0 119m craned-5c69c684d8-dxmhw 2 /2 Running 0 20m grafana-7fddd867b4-kdxv2 1 /1 Running 0 41m metric-adapter-94b6f75b-k8h7z 1 /1 Running 0 119m prometheus-kube-state-metrics-6dbc9cd6c9-dfmkw 1 /1 Running 0 45m prometheus-node-exporter-bfv74 1 /1 Running 0 45m prometheus-node-exporter-s6zps 1 /1 Running 0 45m prometheus-node-exporter-x5rnm 1 /1 Running 0 45m prometheus-server-5966b646fd-g9vxl 2 /2 Running 0 45m you can see this to learn more.","title":"Verify Installation"},{"location":"installation/#customize-installation","text":"Deploy Crane by apply YAML declaration. Main Mirror git clone https://github.com/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter git clone https://e.coding.net/finops/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter The following command will configure prometheus http address for crane if you want to customize it. Specify CUSTOMIZE_PROMETHEUS if you have existing prometheus server. export CUSTOMIZE_PROMETHEUS = if [ $CUSTOMIZE_PROMETHEUS ] ; then sed -i '' \"s/http:\\/\\/prometheus-server.crane-system.svc.cluster.local:8080/ ${ CUSTOMIZE_PROMETHEUS } /\" deploy/craned/deployment.yaml ; fi","title":"Customize Installation"},{"location":"installation/#get-your-kubernetes-cost-report","text":"Get the Grafana URL to visit by running these commands in the same shell: export POD_NAME = $( kubectl get pods --namespace crane-system -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl --namespace crane-system port-forward $POD_NAME 3000 visit Cost Report here with account(admin:admin).","title":"Get your Kubernetes Cost Report"},{"location":"mirror/","text":"Mirror Repo \u00b6 About mirror repo \u00b6 Because of various network issues, it is difficult to access GitHub resources such as GitHub Repo, GitHub Release, GitHub Raw Content raw.githubusercontent.com in some regions. For a better experience, GoCrane offers several additional mirror repositories for you, but with some latency. Helm Resources \u00b6 Tips Sync the latest version of upstream every six hours Origin Mirror Type Public https://gocrane.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/gocrane Helm Public https://prometheus-community.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/prometheus-community Helm Public https://grafana.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/grafana Helm Public Git Resources \u00b6 Tips Sync upstream repository every day Origin Mirror Type Public https://github.com/gocrane/crane.git https://e.coding.net/finops/gocrane/crane.git Git Public https://github.com/gocrane/helm-charts.git https://e.coding.net/finops/gocrane/helm-charts.git Git Public https://github.com/gocrane/api.git https://e.coding.net/finops/gocrane/api.git Git Public https://github.com/gocrane/crane-scheduler.git https://e.coding.net/finops/gocrane/crane-scheduler.git Git Public https://github.com/gocrane/fadvisor.git https://e.coding.net/finops/gocrane/fadvisor.git Git Public Get the raw file contents of the Coding repo \u00b6 Here you'll find out how to get the contents of a source file directly from the Coding Git repository via an HTTP request. Coding Git Repo - Key Params \u00b6 Similar to regular API requests, the Coding Git repository provides a corresponding API interface. The following is an overview of the related parameters. Example Using https:// finops .coding.net/public/ gocrane / helm-charts /git/files /main/integration/grafana/override_values.yaml as an example. Click Here Params Description example team Name of the team finops project Name of the project gocrane repo Name of the Git Repo helm-charts branch Name of the branch main file path The path to the file in the repo /integration/grafana/override_values.yaml Constructing HTTP requests \u00b6 By filling in the following URL construction rules according to the properties mentioned above, you can obtain a URL that can directly access the content of the source file. https://<team>.coding.net/p/<project>/d/<repo>/git/raw/<branch>/<file path>?download = false https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false Tips Try this command. curl https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false","title":"Mirror Repo"},{"location":"mirror/#mirror-repo","text":"","title":"Mirror Repo"},{"location":"mirror/#about-mirror-repo","text":"Because of various network issues, it is difficult to access GitHub resources such as GitHub Repo, GitHub Release, GitHub Raw Content raw.githubusercontent.com in some regions. For a better experience, GoCrane offers several additional mirror repositories for you, but with some latency.","title":"About mirror repo"},{"location":"mirror/#helm-resources","text":"Tips Sync the latest version of upstream every six hours Origin Mirror Type Public https://gocrane.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/gocrane Helm Public https://prometheus-community.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/prometheus-community Helm Public https://grafana.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/grafana Helm Public","title":"Helm Resources"},{"location":"mirror/#git-resources","text":"Tips Sync upstream repository every day Origin Mirror Type Public https://github.com/gocrane/crane.git https://e.coding.net/finops/gocrane/crane.git Git Public https://github.com/gocrane/helm-charts.git https://e.coding.net/finops/gocrane/helm-charts.git Git Public https://github.com/gocrane/api.git https://e.coding.net/finops/gocrane/api.git Git Public https://github.com/gocrane/crane-scheduler.git https://e.coding.net/finops/gocrane/crane-scheduler.git Git Public https://github.com/gocrane/fadvisor.git https://e.coding.net/finops/gocrane/fadvisor.git Git Public","title":"Git Resources"},{"location":"mirror/#get-the-raw-file-contents-of-the-coding-repo","text":"Here you'll find out how to get the contents of a source file directly from the Coding Git repository via an HTTP request.","title":"Get the raw file contents of the Coding repo"},{"location":"mirror/#coding-git-repo-key-params","text":"Similar to regular API requests, the Coding Git repository provides a corresponding API interface. The following is an overview of the related parameters. Example Using https:// finops .coding.net/public/ gocrane / helm-charts /git/files /main/integration/grafana/override_values.yaml as an example. Click Here Params Description example team Name of the team finops project Name of the project gocrane repo Name of the Git Repo helm-charts branch Name of the branch main file path The path to the file in the repo /integration/grafana/override_values.yaml","title":"Coding Git Repo - Key Params"},{"location":"mirror/#constructing-http-requests","text":"By filling in the following URL construction rules according to the properties mentioned above, you can obtain a URL that can directly access the content of the source file. https://<team>.coding.net/p/<project>/d/<repo>/git/raw/<branch>/<file path>?download = false https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false Tips Try this command. curl https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false","title":"Constructing HTTP requests"},{"location":"proposals/20220228-advanced-cpuset-manger/","text":"Advanced CPUSet Manager \u00b6 Static CPU manager is supported by kubelet, when a guaranteed Pod is running on a node, kubelet allocate specific cpu cores to the processes exclusively, which generally keeps the cpu utilization of the node low. This proposal provides a new mechanism to manage cpusets, which allows sharing cpu cores with other processes while binds cpuset.It also allows to revise cpuset when pod is running and relaxes restrictions of binding cpus in kubelet. Table of Contents \u00b6 Advanced CPUSet Manager Table of Contents Motivation Goals Non-Goals/Future Work Proposal Relax restrictions of cpuset allocation Add new annotation to describe the requirement of cpuset contorl manger Advanced CPU Manager component User Stories Story 1 Story 2 Risks and Mitigations Motivation \u00b6 Some latency-sensitive applications have lower lantency and cpu usage when running with specific cores, which results in fewer context switchs and higer cache affinity. But kubelet will always exclude assigned cores in shared cores, which may waste resources.Offline and other online pods can running on the cores actually. In our experiment, for the most part, it is barely noticeable for performance of service. Goals \u00b6 Provide a new mechanism to manage cpuset bypass Provide a new cpuset manager method \"shared\" Allow revise cpuset when pod running Relax restrictions of binding cpus Non-Goals/Future Work \u00b6 Solve the conflicts with kubelet static cpuset manager, you need to set kubelet cpuset manager to \"none\" Numa manager will support in future, CCX/CCD manager also be considered Proposal \u00b6 Relax restrictions of cpuset allocation \u00b6 Kubelet allocate cpus for containers should meet the conditions: requests and limits are specified for all the containers and they are equal the container's resource limit for the limit of CPU is an integer greater than or equal to one and equal to request request of CPU. In Crane, only need to meet condition No.2 Add new annotation to describe the requirement of cpuset contorl manger \u00b6 apiVersion : v1 kind : Pod metadata : annotations : qos.gocrane.io/cpu-manager : none/exclusive/share Provide three polices for cpuset manager: - none: containers of this pod shares a set of cpus which not allocated to exclusive containers - exclusive: containers of this pod monopolize the allocated CPUs , other containers not allowed to use. - share: containers of this pod runs in theallocated CPUs , but other containers can also use. Advanced CPU Manager component \u00b6 Crane-agent use podLister informs to sense the creation of pod. Crane-agent allocate cpus when pod is binded, and loop in cycle to addContainer(change cpuset) until the containers are created Update/Delete pod will handle in reconcile state. state.State referenced from kubelet and topology_cpu_assignment copied from kubelet User Stories \u00b6 Users can update pod annotaion to control cpuset policy flexibly Story 1 \u00b6 make pod from none to share without recreating pod Story 2 \u00b6 make pod from exclusive to share, so offline process can use these CPUs Risks and Mitigations \u00b6 kubelet cpu manger policy need to be set to none, otherwise will be conflicted with crane-agent if crane-agent can not allocate CPUs for pods, it will not refuse to start pod as kubelet","title":"Advanced CpuSet Manager"},{"location":"proposals/20220228-advanced-cpuset-manger/#advanced-cpuset-manager","text":"Static CPU manager is supported by kubelet, when a guaranteed Pod is running on a node, kubelet allocate specific cpu cores to the processes exclusively, which generally keeps the cpu utilization of the node low. This proposal provides a new mechanism to manage cpusets, which allows sharing cpu cores with other processes while binds cpuset.It also allows to revise cpuset when pod is running and relaxes restrictions of binding cpus in kubelet.","title":"Advanced CPUSet Manager"},{"location":"proposals/20220228-advanced-cpuset-manger/#table-of-contents","text":"Advanced CPUSet Manager Table of Contents Motivation Goals Non-Goals/Future Work Proposal Relax restrictions of cpuset allocation Add new annotation to describe the requirement of cpuset contorl manger Advanced CPU Manager component User Stories Story 1 Story 2 Risks and Mitigations","title":"Table of Contents"},{"location":"proposals/20220228-advanced-cpuset-manger/#motivation","text":"Some latency-sensitive applications have lower lantency and cpu usage when running with specific cores, which results in fewer context switchs and higer cache affinity. But kubelet will always exclude assigned cores in shared cores, which may waste resources.Offline and other online pods can running on the cores actually. In our experiment, for the most part, it is barely noticeable for performance of service.","title":"Motivation"},{"location":"proposals/20220228-advanced-cpuset-manger/#goals","text":"Provide a new mechanism to manage cpuset bypass Provide a new cpuset manager method \"shared\" Allow revise cpuset when pod running Relax restrictions of binding cpus","title":"Goals"},{"location":"proposals/20220228-advanced-cpuset-manger/#non-goalsfuture-work","text":"Solve the conflicts with kubelet static cpuset manager, you need to set kubelet cpuset manager to \"none\" Numa manager will support in future, CCX/CCD manager also be considered","title":"Non-Goals/Future Work"},{"location":"proposals/20220228-advanced-cpuset-manger/#proposal","text":"","title":"Proposal"},{"location":"proposals/20220228-advanced-cpuset-manger/#relax-restrictions-of-cpuset-allocation","text":"Kubelet allocate cpus for containers should meet the conditions: requests and limits are specified for all the containers and they are equal the container's resource limit for the limit of CPU is an integer greater than or equal to one and equal to request request of CPU. In Crane, only need to meet condition No.2","title":"Relax restrictions of cpuset allocation"},{"location":"proposals/20220228-advanced-cpuset-manger/#add-new-annotation-to-describe-the-requirement-of-cpuset-contorl-manger","text":"apiVersion : v1 kind : Pod metadata : annotations : qos.gocrane.io/cpu-manager : none/exclusive/share Provide three polices for cpuset manager: - none: containers of this pod shares a set of cpus which not allocated to exclusive containers - exclusive: containers of this pod monopolize the allocated CPUs , other containers not allowed to use. - share: containers of this pod runs in theallocated CPUs , but other containers can also use.","title":"Add new annotation to describe the  requirement of cpuset contorl manger"},{"location":"proposals/20220228-advanced-cpuset-manger/#advanced-cpu-manager-component","text":"Crane-agent use podLister informs to sense the creation of pod. Crane-agent allocate cpus when pod is binded, and loop in cycle to addContainer(change cpuset) until the containers are created Update/Delete pod will handle in reconcile state. state.State referenced from kubelet and topology_cpu_assignment copied from kubelet","title":"Advanced CPU Manager component"},{"location":"proposals/20220228-advanced-cpuset-manger/#user-stories","text":"Users can update pod annotaion to control cpuset policy flexibly","title":"User Stories"},{"location":"proposals/20220228-advanced-cpuset-manger/#story-1","text":"make pod from none to share without recreating pod","title":"Story 1"},{"location":"proposals/20220228-advanced-cpuset-manger/#story-2","text":"make pod from exclusive to share, so offline process can use these CPUs","title":"Story 2"},{"location":"proposals/20220228-advanced-cpuset-manger/#risks-and-mitigations","text":"kubelet cpu manger policy need to be set to none, otherwise will be conflicted with crane-agent if crane-agent can not allocate CPUs for pods, it will not refuse to start pod as kubelet","title":"Risks and Mitigations"},{"location":"roadmaps/roadmap-1h-2022/","text":"Crane Roadmap for H1 2022 \u00b6 Please refer the following sections for Crane release plan of H1 2022, new release will be cut on monthly basis. Please let us know if you have urgent needs which are not presented in the plan. 0.1.0 [released] \u00b6 Predictor to support Moving Windows and DSP algorithms Resource Request Recommendation and Effective Horizontal Pod Autoscaler Grafana Dashboard to view resource utilization and cost trends fadvisor to support billing 0.2.0\uff1a[released] \u00b6 Multiple Metric Adaptor support Node QoS Ensurance for CPU Operation Metrics about R3 and EPA applied ratio 0.3.0 [released] \u00b6 UI with cost visibility and usage optimizations. Request Recommendation adapts with Virtual Kubelet Multiple Triggers for EPA Node QoS Ensurance for Mem Prediction with CPU, Memory, and Business Metrics Scalability to support 1K TSP and 1K EPA 0.4.0 [released] \u00b6 UI to support EPA. 0.5.0 [May] \u00b6 Resource and Replicas Recommendation Load-aware Scheduler 0.6.0 [June] \u00b6 Scalability to support 3k TSP and 3k EPA Algorithm and QoS Documentation EHPA grafana dashboard 0.7.0 [July] \u00b6 Support apiservice router for multiple metric adapters Prediction with Business Metrics 0.8.0 [August] \u00b6 Algorithm estimate notebook","title":"1H 2022"},{"location":"roadmaps/roadmap-1h-2022/#crane-roadmap-for-h1-2022","text":"Please refer the following sections for Crane release plan of H1 2022, new release will be cut on monthly basis. Please let us know if you have urgent needs which are not presented in the plan.","title":"Crane Roadmap for H1 2022"},{"location":"roadmaps/roadmap-1h-2022/#010-released","text":"Predictor to support Moving Windows and DSP algorithms Resource Request Recommendation and Effective Horizontal Pod Autoscaler Grafana Dashboard to view resource utilization and cost trends fadvisor to support billing","title":"0.1.0 [released]"},{"location":"roadmaps/roadmap-1h-2022/#020released","text":"Multiple Metric Adaptor support Node QoS Ensurance for CPU Operation Metrics about R3 and EPA applied ratio","title":"0.2.0\uff1a[released]"},{"location":"roadmaps/roadmap-1h-2022/#030-released","text":"UI with cost visibility and usage optimizations. Request Recommendation adapts with Virtual Kubelet Multiple Triggers for EPA Node QoS Ensurance for Mem Prediction with CPU, Memory, and Business Metrics Scalability to support 1K TSP and 1K EPA","title":"0.3.0 [released]"},{"location":"roadmaps/roadmap-1h-2022/#040-released","text":"UI to support EPA.","title":"0.4.0 [released]"},{"location":"roadmaps/roadmap-1h-2022/#050-may","text":"Resource and Replicas Recommendation Load-aware Scheduler","title":"0.5.0 [May]"},{"location":"roadmaps/roadmap-1h-2022/#060-june","text":"Scalability to support 3k TSP and 3k EPA Algorithm and QoS Documentation EHPA grafana dashboard","title":"0.6.0 [June]"},{"location":"roadmaps/roadmap-1h-2022/#070-july","text":"Support apiservice router for multiple metric adapters Prediction with Business Metrics","title":"0.7.0 [July]"},{"location":"roadmaps/roadmap-1h-2022/#080-august","text":"Algorithm estimate notebook","title":"0.8.0 [August]"},{"location":"tutorials/analytics-and-recommendation/","text":"Analytics and Recommendation \u00b6 Analytics and Recommendation provide capacity that analyzes the workload in k8s cluster and provide recommendations about resource optimize. Two Recommendations are currently supported: ResourceRecommend : Replicas recommendation analyze the actual application usage and give advice for replicas and HPA configurations. HPARecommend : Resource recommendation allows you to obtain recommended values for resources in a cluster and use them to improve the resource utilization of the cluster. Architecture \u00b6 An analytical process \u00b6 Users create Analytics object and config ResourceSelector to select resources to be analyzed. Multiple types of resource selection (based on Group,Kind, and Version) are supported. Analyze each selected resource in parallel and try to execute analysis and give recommendation. Each analysis process is divided into two stages: inspecting and advising: Inspecting: Filter resources that don't match the recommended conditions. For example, for hpa recommendation, the workload that has many not running pod is excluded Advising: Analysis and calculation based on algorithm model then provide the recommendation result. If you paas the above two stages, it will create Recommendation object and display the result in recommendation.Status You can find the failure reasons from analytics.status.recommendations Wait for the next analytics based on the interval Core concept \u00b6 Analytics \u00b6 Analysis defines a scanning analysis task. Two task types are supported: resource recommendation and hpa recommendation. Crane regularly runs analysis tasks and produces recommended results. Recommendation \u00b6 The recommendation shows the results of an Analytics . The recommended result is a YAML configuration that allows users to take appropriate optimization actions, such as adjusting the resource configuration of the application. Configuration \u00b6 Different analytics uses different computing models. Crane provides a default computing model and a corresponding configuration that users can modify to customize the recommended effect. You can modify the default configuration globally or modify the configuration of a single analytics task.","title":"Analytics Overview"},{"location":"tutorials/analytics-and-recommendation/#analytics-and-recommendation","text":"Analytics and Recommendation provide capacity that analyzes the workload in k8s cluster and provide recommendations about resource optimize. Two Recommendations are currently supported: ResourceRecommend : Replicas recommendation analyze the actual application usage and give advice for replicas and HPA configurations. HPARecommend : Resource recommendation allows you to obtain recommended values for resources in a cluster and use them to improve the resource utilization of the cluster.","title":"Analytics and Recommendation"},{"location":"tutorials/analytics-and-recommendation/#architecture","text":"","title":"Architecture"},{"location":"tutorials/analytics-and-recommendation/#an-analytical-process","text":"Users create Analytics object and config ResourceSelector to select resources to be analyzed. Multiple types of resource selection (based on Group,Kind, and Version) are supported. Analyze each selected resource in parallel and try to execute analysis and give recommendation. Each analysis process is divided into two stages: inspecting and advising: Inspecting: Filter resources that don't match the recommended conditions. For example, for hpa recommendation, the workload that has many not running pod is excluded Advising: Analysis and calculation based on algorithm model then provide the recommendation result. If you paas the above two stages, it will create Recommendation object and display the result in recommendation.Status You can find the failure reasons from analytics.status.recommendations Wait for the next analytics based on the interval","title":"An analytical process"},{"location":"tutorials/analytics-and-recommendation/#core-concept","text":"","title":"Core concept"},{"location":"tutorials/analytics-and-recommendation/#analytics","text":"Analysis defines a scanning analysis task. Two task types are supported: resource recommendation and hpa recommendation. Crane regularly runs analysis tasks and produces recommended results.","title":"Analytics"},{"location":"tutorials/analytics-and-recommendation/#recommendation","text":"The recommendation shows the results of an Analytics . The recommended result is a YAML configuration that allows users to take appropriate optimization actions, such as adjusting the resource configuration of the application.","title":"Recommendation"},{"location":"tutorials/analytics-and-recommendation/#configuration","text":"Different analytics uses different computing models. Crane provides a default computing model and a corresponding configuration that users can modify to customize the recommended effect. You can modify the default configuration globally or modify the configuration of a single analytics task.","title":"Configuration"},{"location":"tutorials/dynamic-scheduler-plugin/","text":"Dynamic-scheduler: a load-aware scheduler plugin \u00b6 Introduction \u00b6 Native scheduler of kubernetes can only schedule pods by resource request, which can easily cause a series of load uneven problems: for some nodes, the actual load is not much different from the resource request, which will lead to a very high probability of stability problems. for others, the actual load is much smaller than the resource request, which will lead to a huge waste of resources. To solve these problems, Dynamic scheduler builds a simple but efficient model based on actual node utilization data\uff0cand filters out those nodes with high load to balance the cluster. Design Details \u00b6 Architecture \u00b6 As shown above, Dynamic scheduler relies on Prometheus and Node-exporter to collect and aggregate metrics data, and it consists of two components: Note Node-annotator is currently a module of Crane-scheduler-controller . Node-annotator periodically pulls data from Prometheus and marks them with timestamp on the node in the form of annotations. Dynamic plugin reads the load data directly from the node's annotation, filters and scores candidates based on a simple algorithm. Scheduler Policy \u00b6 Dynamic provides a default scheduler policy and supports user-defined policies. The default policy reies on following metrics: cpu_usage_avg_5m cpu_usage_max_avg_1h cpu_usage_max_avg_1d mem_usage_avg_5m mem_usage_max_avg_1h mem_usage_max_avg_1d At the scheduling Filter stage, the node will be filtered if the actual usage rate of this node is greater than the threshold of any of the above metrics. And at the Score stage, the final score is the weighted sum of these metrics' values. Hot Value \u00b6 In the production cluster, scheduling hotspots may occur frequently because the load of the nodes can not increase immediately after the pod is created. Therefore, we define an extra metrics named Hot Value , which represents the scheduling frequency of the node in recent times. And the final priority of the node is the final score minus the Hot Value .","title":"Dynamic-scheduler: a load-aware scheduler plugin"},{"location":"tutorials/dynamic-scheduler-plugin/#dynamic-scheduler-a-load-aware-scheduler-plugin","text":"","title":"Dynamic-scheduler: a load-aware scheduler plugin"},{"location":"tutorials/dynamic-scheduler-plugin/#introduction","text":"Native scheduler of kubernetes can only schedule pods by resource request, which can easily cause a series of load uneven problems: for some nodes, the actual load is not much different from the resource request, which will lead to a very high probability of stability problems. for others, the actual load is much smaller than the resource request, which will lead to a huge waste of resources. To solve these problems, Dynamic scheduler builds a simple but efficient model based on actual node utilization data\uff0cand filters out those nodes with high load to balance the cluster.","title":"Introduction"},{"location":"tutorials/dynamic-scheduler-plugin/#design-details","text":"","title":"Design Details"},{"location":"tutorials/dynamic-scheduler-plugin/#architecture","text":"As shown above, Dynamic scheduler relies on Prometheus and Node-exporter to collect and aggregate metrics data, and it consists of two components: Note Node-annotator is currently a module of Crane-scheduler-controller . Node-annotator periodically pulls data from Prometheus and marks them with timestamp on the node in the form of annotations. Dynamic plugin reads the load data directly from the node's annotation, filters and scores candidates based on a simple algorithm.","title":"Architecture"},{"location":"tutorials/dynamic-scheduler-plugin/#scheduler-policy","text":"Dynamic provides a default scheduler policy and supports user-defined policies. The default policy reies on following metrics: cpu_usage_avg_5m cpu_usage_max_avg_1h cpu_usage_max_avg_1d mem_usage_avg_5m mem_usage_max_avg_1h mem_usage_max_avg_1d At the scheduling Filter stage, the node will be filtered if the actual usage rate of this node is greater than the threshold of any of the above metrics. And at the Score stage, the final score is the weighted sum of these metrics' values.","title":"Scheduler Policy"},{"location":"tutorials/dynamic-scheduler-plugin/#hot-value","text":"In the production cluster, scheduling hotspots may occur frequently because the load of the nodes can not increase immediately after the pod is created. Therefore, we define an extra metrics named Hot Value , which represents the scheduling frequency of the node in recent times. And the final priority of the node is the final score minus the Hot Value .","title":"Hot Value"},{"location":"tutorials/replicas-recommendation/","text":"Replicas Recommendation \u00b6 Kubernetes' users often set the replicas of workload or HPA configurations based on empirical values. Replicas recommendation analyze the actual application usage and give advice for replicas and HPA configurations. You can refer to and adopt it for your workloads to improve cluster resource utilization. Features \u00b6 Algorithm: The algorithm for calculating the replicas refers to HPA, and supports to customization algo args HPA recommendations: Scan for applications that suitable for configuring horizontal elasticity (EHPA), And give advice for configuration of EHPA, EHPA is a smart horizontal elastic product provided by Crane Support batch analysis: With the ResourceSelector, users can batch analyze multiple workloads Create HPA Analytics \u00b6 Create an Resource Analytics to give recommendation for deployment: nginx-deployment as a sample. Main Mirror kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/nginx-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-replicas.yaml kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/nginx-deployment.yaml?download = false kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/analytics-replicas.yaml?download = false The created Analytics yaml is following: analytics-replicas.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-hpa spec : type : Replicas # This can only be \"Resource\" or \"Replicas\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 600 # analytics selected resources every 10 minutes resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : nginx-deployment config : # defines all the configuration for this analytics ehpa.deployment-min-replicas : \"1\" ehpa.fluctuation-threshold : \"0\" ehpa.min-cpu-usage-threshold : \"0\" You can get created recommendations from analytics status: kubectl get analytics nginx-replicas -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-replicas namespace : default spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 600 config : replicas.fluctuation-threshold : \"0\" replicas.min-cpu-usage-threshold : \"0\" replicas.workload-min-replicas : \"1\" resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : nginx-deployment type : Replicas status : conditions : - lastTransitionTime : \"2022-06-02T09:44:54Z\" message : Analytics is ready reason : AnalyticsReady status : \"True\" type : Ready lastUpdateTime : \"2022-06-02T09:44:54Z\" recommendations : - lastStartTime : \"2022-06-02T09:44:54Z\" message : Success name : nginx-replicas-replicas-7qspm namespace : default targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default uid : c853043c-5ff6-4ee0-a941-e04c8ec3093b Recommendation: Analytics result \u00b6 Use label selector to get related recommendations owns by Analytics . kubectl get recommend -l analysis.crane.io/analytics-name = nginx-replicas -o yaml The output is similar to: apiVersion : v1 items : - apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : creationTimestamp : \"2022-06-02T09:44:54Z\" generateName : nginx-replicas-replicas- generation : 2 labels : analysis.crane.io/analytics-name : nginx-replicas analysis.crane.io/analytics-type : Replicas analysis.crane.io/analytics-uid : e9168c6e-329f-40e9-8d0f-a1ddc35b0d47 app : nginx name : nginx-replicas-replicas-7qspm namespace : default ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : nginx-replicas uid : e9168c6e-329f-40e9-8d0f-a1ddc35b0d47 resourceVersion : \"818959913\" selfLink : /apis/analysis.crane.io/v1alpha1/namespaces/default/recommendations/nginx-replicas-replicas-7qspm uid : c853043c-5ff6-4ee0-a941-e04c8ec3093b spec : adoptionType : StatusAndAnnotation completionStrategy : completionStrategyType : Once targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default type : Replicas status : conditions : - lastTransitionTime : \"2022-06-02T09:44:54Z\" message : Recommendation is ready reason : RecommendationReady status : \"True\" type : Ready lastUpdateTime : \"2022-06-02T09:44:54Z\" recommendedValue : | effectiveHPA: maxReplicas: 3 metrics: - resource: name: cpu target: averageUtilization: 75 type: Utilization type: Resource minReplicas: 3 replicasRecommendation: replicas: 3 kind : List metadata : resourceVersion : \"\" selfLink : \"\" Batch recommendation \u00b6 Use a sample to show how to recommend all Deployments and StatefulSets by one Analytics : apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : workload-replicas namespace : crane-system # The Analytics in Crane-system will select all resource across all namespaces. spec : type : Replicas # This can only be \"Resource\" or \"Replicas\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 - kind : StatefulSet apiVersion : apps/v1 when using crane-system as your namespace\uff0c Analytics selected all namespaces\uff0cwhen namespace not equal crane-system \uff0c Analytics selected the resource that in Analytics namespace resourceSelectors defines the resource to analysis\uff0ckind and apiVersion is mandatory\uff0cname is optional resourceSelectors supoort any resource that are Scale Subresource HPA Recommendation Algorithm model \u00b6 Inspecting \u00b6 Workload with low replicas: If the replicas is too low, may not be suitable for hpa recommendation. Associated configuration: ehpa.deployment-min-replicas | ehpa.statefulset-min-replicas | ehpa.workload-min-replicas Workload with a certain percentage of not running pods: if the workload of Pod mostly can't run normally, may not be suitable for flexibility. Associated configuration: ehpa.pod-min-ready-seconds | ehpa.pod-available-ratio Workload with low CPU usage: The low CPU usage workload means that there is no load pressure. In this case, we can't estimate it. Associated configuration: ehpa.min-cpu-usage-threshold Workload with low fluctuation of CPU usage: dividing of the maximum and minimum usage is defined as the fluctuation rate. If the fluctuation rate is too low, the workload will not benefit much from hpa. Associated configuration: ehpa.fluctuation-threshold Advising \u00b6 In the advising phase, one EffectiveHPA Spec is recommended using the following Algorithm model. The recommended logic for each field is as follows: Recommend TargetUtilization Principle: Use Pod P99 resource utilization to recommend hpa. Because if the application can accept this utilization over P99 time, it can be inferred as a target for elasticity. Get the Pod P99 usage of the past seven days by Percentile algorithm: \\(pod\\_cpu\\_usage\\_p99\\) Corresponding utilization: \\(target\\_pod\\_CPU\\_utilization = \\frac{pod\\_cpu\\_usage\\_p99}{pod\\_cpu\\_request}\\) To prevent over-utilization or under-utilization, target_pod_cpu_utilization needs to be less than ehpa.min-cpu-target-utilization and greater than ehpa. max-cpu-target-utilization \\(ehpa.max\\mbox{-}cpu\\mbox{-}target\\mbox{-}utilization < target\\_pod\\_cpu\\_utilization < ehpa.min\\mbox{-}cpu\\mbox{-}target\\mbox{-}utilization\\) Recommend minReplicas Principle: MinReplicas are recommended for the lowest hourly workload utilization for the past seven days. Calculate the lowest median workload cpu usage of the past seven days: \\(workload\\_cpu\\_usage\\_medium\\_min\\) Corresponding replicas: \\(minReplicas = \\frac{\\mathrm{workload\\_cpu\\_usage\\_medium\\_min} }{pod\\_cpu\\_request \\times ehpa.max-cpu-target-utilization}\\) To prevent the minReplicas being too small, the minReplicas must be greater than or equal to ehpa.default-min-replicas \\(minReplicas \\geq ehpa.default\\mbox{-}min\\mbox{-}replicas\\) Recommend maxReplicas Principle: Use workload's past and future seven days load to recommend maximum replicas. Calculate P95 workload CPU usage for the past seven days and the next seven days: \\(workload\\_cpu\\_usage\\_p95\\) Corresponding replicas: \\(max\\_replicas\\_origin = \\frac{\\mathrm{workload\\_cpu\\_usage\\_p95} }{pod\\_cpu\\_request \\times target\\_cpu\\_utilization}\\) To handle with the peak traffic, Magnify by a certain factor: \\(max\\_replicas = max\\_replicas\\_origin \\times ehpa.max\\mbox{-}replicas\\mbox{-}factor\\) Recommend MetricSpec(except CpuUtilization) If HPA is configured for workload, MetricSpecs other than CpuUtilization are inherited Recommend Behavior If HPA is configured for workload, the corresponding Behavior configuration is inherited Recommend Prediction Try to predict the CPU usage of the workload in the next seven days using DSP If the prediction is successful, add the prediction configuration If the workload is not predictable, do not add the prediction configuration. Configurations for HPA Recommendation \u00b6 Configuration Default Value Description ehpa.deployment-min-replicas 1 hpa recommendations are not made for workloads smaller than this value. ehpa.statefulset-min-replicas 1 hpa recommendations are not made for workloads smaller than this value. ehpa.workload-min-replicas 1 Workload replicas smaller than this value are not recommended for hpa. ehpa.pod-min-ready-seconds 30 specifies the number of seconds in decide whether a POD is ready. ehpa.pod-available-ratio 0.5 Workloads whose Ready pod ratio is smaller than this value are not recommended for hpa. ehpa.default-min-replicas 2 the default minimum minReplicas. ehpa.max-replicas-factor 3 the factor for calculate maxReplicas. ehpa.min-cpu-usage-threshold 10 hpa recommendations are not made for workloads smaller than this value. ehpa.fluctuation-threshold 1.5 hpa recommendations are not made for workloads smaller than this value. ehpa.min-cpu-target-utilization 30 ehpa.max-cpu-target-utilization 75 ehpa.reference-hpa true inherits the existing HPA configuration","title":"Replicas Recommendation"},{"location":"tutorials/replicas-recommendation/#replicas-recommendation","text":"Kubernetes' users often set the replicas of workload or HPA configurations based on empirical values. Replicas recommendation analyze the actual application usage and give advice for replicas and HPA configurations. You can refer to and adopt it for your workloads to improve cluster resource utilization.","title":"Replicas Recommendation"},{"location":"tutorials/replicas-recommendation/#features","text":"Algorithm: The algorithm for calculating the replicas refers to HPA, and supports to customization algo args HPA recommendations: Scan for applications that suitable for configuring horizontal elasticity (EHPA), And give advice for configuration of EHPA, EHPA is a smart horizontal elastic product provided by Crane Support batch analysis: With the ResourceSelector, users can batch analyze multiple workloads","title":"Features"},{"location":"tutorials/replicas-recommendation/#create-hpa-analytics","text":"Create an Resource Analytics to give recommendation for deployment: nginx-deployment as a sample. Main Mirror kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/nginx-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-replicas.yaml kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/nginx-deployment.yaml?download = false kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/analytics-replicas.yaml?download = false The created Analytics yaml is following: analytics-replicas.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-hpa spec : type : Replicas # This can only be \"Resource\" or \"Replicas\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 600 # analytics selected resources every 10 minutes resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : nginx-deployment config : # defines all the configuration for this analytics ehpa.deployment-min-replicas : \"1\" ehpa.fluctuation-threshold : \"0\" ehpa.min-cpu-usage-threshold : \"0\" You can get created recommendations from analytics status: kubectl get analytics nginx-replicas -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-replicas namespace : default spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 600 config : replicas.fluctuation-threshold : \"0\" replicas.min-cpu-usage-threshold : \"0\" replicas.workload-min-replicas : \"1\" resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : nginx-deployment type : Replicas status : conditions : - lastTransitionTime : \"2022-06-02T09:44:54Z\" message : Analytics is ready reason : AnalyticsReady status : \"True\" type : Ready lastUpdateTime : \"2022-06-02T09:44:54Z\" recommendations : - lastStartTime : \"2022-06-02T09:44:54Z\" message : Success name : nginx-replicas-replicas-7qspm namespace : default targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default uid : c853043c-5ff6-4ee0-a941-e04c8ec3093b","title":"Create HPA Analytics"},{"location":"tutorials/replicas-recommendation/#recommendation-analytics-result","text":"Use label selector to get related recommendations owns by Analytics . kubectl get recommend -l analysis.crane.io/analytics-name = nginx-replicas -o yaml The output is similar to: apiVersion : v1 items : - apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : creationTimestamp : \"2022-06-02T09:44:54Z\" generateName : nginx-replicas-replicas- generation : 2 labels : analysis.crane.io/analytics-name : nginx-replicas analysis.crane.io/analytics-type : Replicas analysis.crane.io/analytics-uid : e9168c6e-329f-40e9-8d0f-a1ddc35b0d47 app : nginx name : nginx-replicas-replicas-7qspm namespace : default ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : nginx-replicas uid : e9168c6e-329f-40e9-8d0f-a1ddc35b0d47 resourceVersion : \"818959913\" selfLink : /apis/analysis.crane.io/v1alpha1/namespaces/default/recommendations/nginx-replicas-replicas-7qspm uid : c853043c-5ff6-4ee0-a941-e04c8ec3093b spec : adoptionType : StatusAndAnnotation completionStrategy : completionStrategyType : Once targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default type : Replicas status : conditions : - lastTransitionTime : \"2022-06-02T09:44:54Z\" message : Recommendation is ready reason : RecommendationReady status : \"True\" type : Ready lastUpdateTime : \"2022-06-02T09:44:54Z\" recommendedValue : | effectiveHPA: maxReplicas: 3 metrics: - resource: name: cpu target: averageUtilization: 75 type: Utilization type: Resource minReplicas: 3 replicasRecommendation: replicas: 3 kind : List metadata : resourceVersion : \"\" selfLink : \"\"","title":"Recommendation: Analytics result"},{"location":"tutorials/replicas-recommendation/#batch-recommendation","text":"Use a sample to show how to recommend all Deployments and StatefulSets by one Analytics : apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : workload-replicas namespace : crane-system # The Analytics in Crane-system will select all resource across all namespaces. spec : type : Replicas # This can only be \"Resource\" or \"Replicas\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 - kind : StatefulSet apiVersion : apps/v1 when using crane-system as your namespace\uff0c Analytics selected all namespaces\uff0cwhen namespace not equal crane-system \uff0c Analytics selected the resource that in Analytics namespace resourceSelectors defines the resource to analysis\uff0ckind and apiVersion is mandatory\uff0cname is optional resourceSelectors supoort any resource that are Scale Subresource","title":"Batch recommendation"},{"location":"tutorials/replicas-recommendation/#hpa-recommendation-algorithm-model","text":"","title":"HPA Recommendation Algorithm model"},{"location":"tutorials/replicas-recommendation/#inspecting","text":"Workload with low replicas: If the replicas is too low, may not be suitable for hpa recommendation. Associated configuration: ehpa.deployment-min-replicas | ehpa.statefulset-min-replicas | ehpa.workload-min-replicas Workload with a certain percentage of not running pods: if the workload of Pod mostly can't run normally, may not be suitable for flexibility. Associated configuration: ehpa.pod-min-ready-seconds | ehpa.pod-available-ratio Workload with low CPU usage: The low CPU usage workload means that there is no load pressure. In this case, we can't estimate it. Associated configuration: ehpa.min-cpu-usage-threshold Workload with low fluctuation of CPU usage: dividing of the maximum and minimum usage is defined as the fluctuation rate. If the fluctuation rate is too low, the workload will not benefit much from hpa. Associated configuration: ehpa.fluctuation-threshold","title":"Inspecting"},{"location":"tutorials/replicas-recommendation/#advising","text":"In the advising phase, one EffectiveHPA Spec is recommended using the following Algorithm model. The recommended logic for each field is as follows: Recommend TargetUtilization Principle: Use Pod P99 resource utilization to recommend hpa. Because if the application can accept this utilization over P99 time, it can be inferred as a target for elasticity. Get the Pod P99 usage of the past seven days by Percentile algorithm: \\(pod\\_cpu\\_usage\\_p99\\) Corresponding utilization: \\(target\\_pod\\_CPU\\_utilization = \\frac{pod\\_cpu\\_usage\\_p99}{pod\\_cpu\\_request}\\) To prevent over-utilization or under-utilization, target_pod_cpu_utilization needs to be less than ehpa.min-cpu-target-utilization and greater than ehpa. max-cpu-target-utilization \\(ehpa.max\\mbox{-}cpu\\mbox{-}target\\mbox{-}utilization < target\\_pod\\_cpu\\_utilization < ehpa.min\\mbox{-}cpu\\mbox{-}target\\mbox{-}utilization\\) Recommend minReplicas Principle: MinReplicas are recommended for the lowest hourly workload utilization for the past seven days. Calculate the lowest median workload cpu usage of the past seven days: \\(workload\\_cpu\\_usage\\_medium\\_min\\) Corresponding replicas: \\(minReplicas = \\frac{\\mathrm{workload\\_cpu\\_usage\\_medium\\_min} }{pod\\_cpu\\_request \\times ehpa.max-cpu-target-utilization}\\) To prevent the minReplicas being too small, the minReplicas must be greater than or equal to ehpa.default-min-replicas \\(minReplicas \\geq ehpa.default\\mbox{-}min\\mbox{-}replicas\\) Recommend maxReplicas Principle: Use workload's past and future seven days load to recommend maximum replicas. Calculate P95 workload CPU usage for the past seven days and the next seven days: \\(workload\\_cpu\\_usage\\_p95\\) Corresponding replicas: \\(max\\_replicas\\_origin = \\frac{\\mathrm{workload\\_cpu\\_usage\\_p95} }{pod\\_cpu\\_request \\times target\\_cpu\\_utilization}\\) To handle with the peak traffic, Magnify by a certain factor: \\(max\\_replicas = max\\_replicas\\_origin \\times ehpa.max\\mbox{-}replicas\\mbox{-}factor\\) Recommend MetricSpec(except CpuUtilization) If HPA is configured for workload, MetricSpecs other than CpuUtilization are inherited Recommend Behavior If HPA is configured for workload, the corresponding Behavior configuration is inherited Recommend Prediction Try to predict the CPU usage of the workload in the next seven days using DSP If the prediction is successful, add the prediction configuration If the workload is not predictable, do not add the prediction configuration.","title":"Advising"},{"location":"tutorials/replicas-recommendation/#configurations-for-hpa-recommendation","text":"Configuration Default Value Description ehpa.deployment-min-replicas 1 hpa recommendations are not made for workloads smaller than this value. ehpa.statefulset-min-replicas 1 hpa recommendations are not made for workloads smaller than this value. ehpa.workload-min-replicas 1 Workload replicas smaller than this value are not recommended for hpa. ehpa.pod-min-ready-seconds 30 specifies the number of seconds in decide whether a POD is ready. ehpa.pod-available-ratio 0.5 Workloads whose Ready pod ratio is smaller than this value are not recommended for hpa. ehpa.default-min-replicas 2 the default minimum minReplicas. ehpa.max-replicas-factor 3 the factor for calculate maxReplicas. ehpa.min-cpu-usage-threshold 10 hpa recommendations are not made for workloads smaller than this value. ehpa.fluctuation-threshold 1.5 hpa recommendations are not made for workloads smaller than this value. ehpa.min-cpu-target-utilization 30 ehpa.max-cpu-target-utilization 75 ehpa.reference-hpa true inherits the existing HPA configuration","title":"Configurations for HPA Recommendation"},{"location":"tutorials/resource-recommendation/","text":"Resource Recommendation \u00b6 Resource recommendation allows you to obtain recommended values for resources in a cluster and use them to improve the resource utilization of the cluster. Difference between VPA \u00b6 Resource recommendations are a lightweight implementation of VPA and are more flexible. Algorithm: The algorithm model adopts the Moving Window algorithm of VPA, and supports to customization algo args , providing higher flexibility Support batch analysis: With the ResourceSelector, users can batch analyze multiple workloads without creating VPA objects one by one More portable: It is difficult to use VPA's Auto mode in production because it will cause container reconstruction when updating container resource configuration. Resource recommendation provides suggestions to users and leaves the decision of change to users Create Resource Analytics \u00b6 Create an Resource Analytics to give recommendation for deployment: nginx-deployment as a sample. Main Mirror ```bash kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/nginx-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-resource.yaml kubectl get analytics kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/nginx-deployment.yaml?download = false kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/analytics-resource.yaml?download = false kubectl get analytics The created Analytics yaml is following: analytics-resource.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-resource spec : type : Resource # This can only be \"Resource\" or \"Replicas\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : nginx-deployment The output is: NAME AGE nginx-resource 16m You can get view analytics status by running: kubectl get analytics nginx-resource -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-resource namespace : default spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : nginx-deployment type : Resource status : conditions : - lastTransitionTime : \"2022-05-15T14:38:35Z\" message : Analytics is ready reason : AnalyticsReady status : \"True\" type : Ready lastUpdateTime : \"2022-05-15T14:38:35Z\" recommendations : - lastStartTime : \"2022-05-15T14:38:35Z\" message : Success name : nginx-resource-resource-w45nq namespace : default targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default uid : 750cb3bd-0b87-4f87-acbe-57e621af0a1e Recommendation: Analytics result \u00b6 You can get recommendations that created by above Analytics by running. kubectl get recommend -l analysis.crane.io/analytics-name = nginx-resource -o yaml The output is similar to: apiVersion : v1 items : - apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : creationTimestamp : \"2022-06-15T15:26:25Z\" generateName : nginx-resource-resource- generation : 1 labels : analysis.crane.io/analytics-name : nginx-resource analysis.crane.io/analytics-type : Resource analysis.crane.io/analytics-uid : 9e78964b-f8ae-40de-9740-f9a715d16280 app : nginx name : nginx-resource-resource-t4xpn namespace : default ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : nginx-resource uid : 9e78964b-f8ae-40de-9740-f9a715d16280 resourceVersion : \"2117439429\" selfLink : /apis/analysis.crane.io/v1alpha1/namespaces/default/recommendations/nginx-resource-resource-t4xpn uid : 8005e3e0-8fe9-470b-99cf-5ce9dd407529 spec : adoptionType : StatusAndAnnotation completionStrategy : completionStrategyType : Once targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default type : Resource status : recommendedValue : | resourceRequest: containers: - containerName: nginx target: cpu: 100m memory: 100Mi kind : List metadata : resourceVersion : \"\" selfLink : \"\" The status.recommendedValue.ResourceRequest is recommended by crane's recommendation engine. Batch recommendation \u00b6 Use a sample to show how to recommend all Deployments and StatefulSets by one Analytics : apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : workload-resource namespace : crane-system # The Analytics in Crane-system will select all resource across all namespaces. spec : type : Resource # This can only be \"Resource\" or \"Replicas\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 - kind : StatefulSet apiVersion : apps/v1 when using crane-system as your namespace\uff0c Analytics selected all namespaces\uff0cwhen namespace not equal crane-system \uff0c Analytics selected the resource that in Analytics namespace resourceSelectors defines the resource to analysis\uff0ckind and apiVersion is mandatory\uff0cname is optional resourceSelectors supoort any resource that are Scale Subresource Resource Recommendation Algorithm model \u00b6 Inspecting \u00b6 Workload with not pods: if the workload has no pods exist means that it's not a available workload. Advising \u00b6 VPA's Moving Window algorithm was used to calculate the CPU and Memory of each container and give the corresponding recommended values","title":"Resource Recommendation"},{"location":"tutorials/resource-recommendation/#resource-recommendation","text":"Resource recommendation allows you to obtain recommended values for resources in a cluster and use them to improve the resource utilization of the cluster.","title":"Resource Recommendation"},{"location":"tutorials/resource-recommendation/#difference-between-vpa","text":"Resource recommendations are a lightweight implementation of VPA and are more flexible. Algorithm: The algorithm model adopts the Moving Window algorithm of VPA, and supports to customization algo args , providing higher flexibility Support batch analysis: With the ResourceSelector, users can batch analyze multiple workloads without creating VPA objects one by one More portable: It is difficult to use VPA's Auto mode in production because it will cause container reconstruction when updating container resource configuration. Resource recommendation provides suggestions to users and leaves the decision of change to users","title":"Difference between VPA"},{"location":"tutorials/resource-recommendation/#create-resource-analytics","text":"Create an Resource Analytics to give recommendation for deployment: nginx-deployment as a sample. Main Mirror ```bash kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/nginx-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-resource.yaml kubectl get analytics kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/nginx-deployment.yaml?download = false kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/analytics-resource.yaml?download = false kubectl get analytics The created Analytics yaml is following: analytics-resource.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-resource spec : type : Resource # This can only be \"Resource\" or \"Replicas\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : nginx-deployment The output is: NAME AGE nginx-resource 16m You can get view analytics status by running: kubectl get analytics nginx-resource -o yaml The output is similar to: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-resource namespace : default spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : nginx-deployment type : Resource status : conditions : - lastTransitionTime : \"2022-05-15T14:38:35Z\" message : Analytics is ready reason : AnalyticsReady status : \"True\" type : Ready lastUpdateTime : \"2022-05-15T14:38:35Z\" recommendations : - lastStartTime : \"2022-05-15T14:38:35Z\" message : Success name : nginx-resource-resource-w45nq namespace : default targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default uid : 750cb3bd-0b87-4f87-acbe-57e621af0a1e","title":"Create Resource Analytics"},{"location":"tutorials/resource-recommendation/#recommendation-analytics-result","text":"You can get recommendations that created by above Analytics by running. kubectl get recommend -l analysis.crane.io/analytics-name = nginx-resource -o yaml The output is similar to: apiVersion : v1 items : - apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : creationTimestamp : \"2022-06-15T15:26:25Z\" generateName : nginx-resource-resource- generation : 1 labels : analysis.crane.io/analytics-name : nginx-resource analysis.crane.io/analytics-type : Resource analysis.crane.io/analytics-uid : 9e78964b-f8ae-40de-9740-f9a715d16280 app : nginx name : nginx-resource-resource-t4xpn namespace : default ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : nginx-resource uid : 9e78964b-f8ae-40de-9740-f9a715d16280 resourceVersion : \"2117439429\" selfLink : /apis/analysis.crane.io/v1alpha1/namespaces/default/recommendations/nginx-resource-resource-t4xpn uid : 8005e3e0-8fe9-470b-99cf-5ce9dd407529 spec : adoptionType : StatusAndAnnotation completionStrategy : completionStrategyType : Once targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default type : Resource status : recommendedValue : | resourceRequest: containers: - containerName: nginx target: cpu: 100m memory: 100Mi kind : List metadata : resourceVersion : \"\" selfLink : \"\" The status.recommendedValue.ResourceRequest is recommended by crane's recommendation engine.","title":"Recommendation: Analytics result"},{"location":"tutorials/resource-recommendation/#batch-recommendation","text":"Use a sample to show how to recommend all Deployments and StatefulSets by one Analytics : apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : workload-resource namespace : crane-system # The Analytics in Crane-system will select all resource across all namespaces. spec : type : Resource # This can only be \"Resource\" or \"Replicas\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 - kind : StatefulSet apiVersion : apps/v1 when using crane-system as your namespace\uff0c Analytics selected all namespaces\uff0cwhen namespace not equal crane-system \uff0c Analytics selected the resource that in Analytics namespace resourceSelectors defines the resource to analysis\uff0ckind and apiVersion is mandatory\uff0cname is optional resourceSelectors supoort any resource that are Scale Subresource","title":"Batch recommendation"},{"location":"tutorials/resource-recommendation/#resource-recommendation-algorithm-model","text":"","title":"Resource Recommendation Algorithm model"},{"location":"tutorials/resource-recommendation/#inspecting","text":"Workload with not pods: if the workload has no pods exist means that it's not a available workload.","title":"Inspecting"},{"location":"tutorials/resource-recommendation/#advising","text":"VPA's Moving Window algorithm was used to calculate the CPU and Memory of each container and give the corresponding recommended values","title":"Advising"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/","text":"Crane-scheduler \u00b6 Overview \u00b6 Crane-scheduler is a collection of scheduler plugins based on scheduler framework , including: Dynamic scheduler: a load-aware scheduler plugin Get Started \u00b6 Install Prometheus \u00b6 Make sure your kubernetes cluster has Prometheus installed. If not, please refer to Install Prometheus . Configure Prometheus Rules \u00b6 Configure the rules of Prometheus to get expected aggregated data: apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : name : example-record spec : groups : - name : cpu_mem_usage_active interval : 30s rules : - record : cpu_usage_active expr : 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[30s])) * 100) - record : mem_usage_active expr : 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) - name : cpu-usage-5m interval : 5m rules : - record : cpu_usage_max_avg_1h expr : max_over_time(cpu_usage_avg_5m[1h]) - record : cpu_usage_max_avg_1d expr : max_over_time(cpu_usage_avg_5m[1d]) - name : cpu-usage-1m interval : 1m rules : - record : cpu_usage_avg_5m expr : avg_over_time(cpu_usage_active[5m]) - name : mem-usage-5m interval : 5m rules : - record : mem_usage_max_avg_1h expr : max_over_time(mem_usage_avg_5m[1h]) - record : mem_usage_max_avg_1d expr : max_over_time(mem_usage_avg_5m[1d]) - name : mem-usage-1m interval : 1m rules : - record : mem_usage_avg_5m expr : avg_over_time(mem_usage_active[5m]) \ufe0fTroubleshooting The sampling interval of Prometheus must be less than 30 seconds, otherwise the above rules(such as cpu_usage_active) may not take effect. 2. Update the configuration of Prometheus service discovery to ensure that node_exporters/telegraf are using node name as instance name: - job_name : kubernetes-node-exporter tls_config : ca_file : /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify : true bearer_token_file : /var/run/secrets/kubernetes.io/serviceaccount/token scheme : https kubernetes_sd_configs : ... # Host name - source_labels : [ __meta_kubernetes_node_name ] target_label : instance ... Note This step can be skipped if the node name itself is the host IP. Install Crane-scheduler \u00b6 There are two options: Install Crane-scheduler as a second scheduler Replace native Kube-scheduler with Crane-scheduler Install Crane-scheduler as a second scheduler \u00b6 Main Mirror helm repo add crane https://gocrane.github.io/helm-charts helm install scheduler -n crane-system --create-namespace --set global.prometheusAddr = \"REPLACE_ME_WITH_PROMETHEUS_ADDR\" crane/scheduler helm repo add crane https://finops-helm.pkg.coding.net/gocrane/gocrane helm install scheduler -n crane-system --create-namespace --set global.prometheusAddr = \"REPLACE_ME_WITH_PROMETHEUS_ADDR\" crane/scheduler Replace native Kube-scheduler with Crane-scheduler \u00b6 Backup /etc/kubernetes/manifests/kube-scheduler.yaml cp /etc/kubernetes/manifests/kube-scheduler.yaml /etc/kubernetes/ Modify configfile of kube-scheduler( scheduler-config.yaml ) to enable Dynamic scheduler plugin and configure plugin args: scheduler-config.yaml apiVersion : kubescheduler.config.k8s.io/v1beta2 kind : KubeSchedulerConfiguration ... profiles : - schedulerName : default-scheduler plugins : filter : enabled : - name : Dynamic score : enabled : - name : Dynamic weight : 3 pluginConfig : - name : Dynamic args : policyConfigPath : /etc/kubernetes/policy.yaml ... Create /etc/kubernetes/policy.yaml , using as scheduler policy of Dynamic plugin: /etc/kubernetes/policy.yaml apiVersion : scheduler.policy.crane.io/v1alpha1 kind : DynamicSchedulerPolicy spec : syncPolicy : ##cpu usage - name : cpu_usage_avg_5m period : 3m - name : cpu_usage_max_avg_1h period : 15m - name : cpu_usage_max_avg_1d period : 3h ##memory usage - name : mem_usage_avg_5m period : 3m - name : mem_usage_max_avg_1h period : 15m - name : mem_usage_max_avg_1d period : 3h predicate : ##cpu usage - name : cpu_usage_avg_5m maxLimitPecent : 0.65 - name : cpu_usage_max_avg_1h maxLimitPecent : 0.75 ##memory usage - name : mem_usage_avg_5m maxLimitPecent : 0.65 - name : mem_usage_max_avg_1h maxLimitPecent : 0.75 priority : ##cpu usage - name : cpu_usage_avg_5m weight : 0.2 - name : cpu_usage_max_avg_1h weight : 0.3 - name : cpu_usage_max_avg_1d weight : 0.5 ##memory usage - name : mem_usage_avg_5m weight : 0.2 - name : mem_usage_max_avg_1h weight : 0.3 - name : mem_usage_max_avg_1d weight : 0.5 hotValue : - timeRange : 5m count : 5 - timeRange : 1m count : 2 Modify kube-scheduler.yaml and replace kube-scheduler image with Crane-scheduler\uff1a kube-scheduler.yaml ... image : docker.io/gocrane/crane-scheduler:0.0.23 ... Install crane-scheduler-controller : Main Mirror kubectl apply -f https://raw.githubusercontent.com/gocrane/crane-scheduler/main/deploy/controller/rbac.yaml kubectl apply -f https://raw.githubusercontent.com/gocrane/crane-scheduler/main/deploy/controller/deployment.yaml kubectl apply -f https://finops.coding.net/p/gocrane/d/crane-scheduler/git/raw/main/deploy/controller/rbac.yaml?download = false kubectl apply -f https://finops.coding.net/p/gocrane/d/crane-scheduler/git/raw/main/deploy/controller/deployment.yaml?download = false Schedule Pods With Crane-scheduler \u00b6 Test Crane-scheduler with following example: apiVersion : apps/v1 kind : Deployment metadata : name : cpu-stress spec : selector : matchLabels : app : cpu-stress replicas : 1 template : metadata : labels : app : cpu-stress spec : schedulerName : crane-scheduler hostNetwork : true tolerations : - key : node.kubernetes.io/network-unavailable operator : Exists effect : NoSchedule containers : - name : stress image : docker.io/gocrane/stress:latest command : [ \"stress\" , \"-c\" , \"1\" ] resources : requests : memory : \"1Gi\" cpu : \"1\" limits : memory : \"1Gi\" cpu : \"1\" Note Change crane-scheduler to default-scheduler if crane-scheduler is used as default. There will be the following event if the test pod is successfully scheduled: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 28s crane-scheduler Successfully assigned default/cpu-stress-7669499b57-zmrgb to vm-162-247-ubuntu","title":"Load-aware Scheduling"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#crane-scheduler","text":"","title":"Crane-scheduler"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#overview","text":"Crane-scheduler is a collection of scheduler plugins based on scheduler framework , including: Dynamic scheduler: a load-aware scheduler plugin","title":"Overview"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#get-started","text":"","title":"Get Started"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#install-prometheus","text":"Make sure your kubernetes cluster has Prometheus installed. If not, please refer to Install Prometheus .","title":"Install Prometheus"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#configure-prometheus-rules","text":"Configure the rules of Prometheus to get expected aggregated data: apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : name : example-record spec : groups : - name : cpu_mem_usage_active interval : 30s rules : - record : cpu_usage_active expr : 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[30s])) * 100) - record : mem_usage_active expr : 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) - name : cpu-usage-5m interval : 5m rules : - record : cpu_usage_max_avg_1h expr : max_over_time(cpu_usage_avg_5m[1h]) - record : cpu_usage_max_avg_1d expr : max_over_time(cpu_usage_avg_5m[1d]) - name : cpu-usage-1m interval : 1m rules : - record : cpu_usage_avg_5m expr : avg_over_time(cpu_usage_active[5m]) - name : mem-usage-5m interval : 5m rules : - record : mem_usage_max_avg_1h expr : max_over_time(mem_usage_avg_5m[1h]) - record : mem_usage_max_avg_1d expr : max_over_time(mem_usage_avg_5m[1d]) - name : mem-usage-1m interval : 1m rules : - record : mem_usage_avg_5m expr : avg_over_time(mem_usage_active[5m]) \ufe0fTroubleshooting The sampling interval of Prometheus must be less than 30 seconds, otherwise the above rules(such as cpu_usage_active) may not take effect. 2. Update the configuration of Prometheus service discovery to ensure that node_exporters/telegraf are using node name as instance name: - job_name : kubernetes-node-exporter tls_config : ca_file : /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify : true bearer_token_file : /var/run/secrets/kubernetes.io/serviceaccount/token scheme : https kubernetes_sd_configs : ... # Host name - source_labels : [ __meta_kubernetes_node_name ] target_label : instance ... Note This step can be skipped if the node name itself is the host IP.","title":"Configure Prometheus Rules"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#install-crane-scheduler","text":"There are two options: Install Crane-scheduler as a second scheduler Replace native Kube-scheduler with Crane-scheduler","title":"Install Crane-scheduler"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#install-crane-scheduler-as-a-second-scheduler","text":"Main Mirror helm repo add crane https://gocrane.github.io/helm-charts helm install scheduler -n crane-system --create-namespace --set global.prometheusAddr = \"REPLACE_ME_WITH_PROMETHEUS_ADDR\" crane/scheduler helm repo add crane https://finops-helm.pkg.coding.net/gocrane/gocrane helm install scheduler -n crane-system --create-namespace --set global.prometheusAddr = \"REPLACE_ME_WITH_PROMETHEUS_ADDR\" crane/scheduler","title":"Install Crane-scheduler as a second scheduler"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#replace-native-kube-scheduler-with-crane-scheduler","text":"Backup /etc/kubernetes/manifests/kube-scheduler.yaml cp /etc/kubernetes/manifests/kube-scheduler.yaml /etc/kubernetes/ Modify configfile of kube-scheduler( scheduler-config.yaml ) to enable Dynamic scheduler plugin and configure plugin args: scheduler-config.yaml apiVersion : kubescheduler.config.k8s.io/v1beta2 kind : KubeSchedulerConfiguration ... profiles : - schedulerName : default-scheduler plugins : filter : enabled : - name : Dynamic score : enabled : - name : Dynamic weight : 3 pluginConfig : - name : Dynamic args : policyConfigPath : /etc/kubernetes/policy.yaml ... Create /etc/kubernetes/policy.yaml , using as scheduler policy of Dynamic plugin: /etc/kubernetes/policy.yaml apiVersion : scheduler.policy.crane.io/v1alpha1 kind : DynamicSchedulerPolicy spec : syncPolicy : ##cpu usage - name : cpu_usage_avg_5m period : 3m - name : cpu_usage_max_avg_1h period : 15m - name : cpu_usage_max_avg_1d period : 3h ##memory usage - name : mem_usage_avg_5m period : 3m - name : mem_usage_max_avg_1h period : 15m - name : mem_usage_max_avg_1d period : 3h predicate : ##cpu usage - name : cpu_usage_avg_5m maxLimitPecent : 0.65 - name : cpu_usage_max_avg_1h maxLimitPecent : 0.75 ##memory usage - name : mem_usage_avg_5m maxLimitPecent : 0.65 - name : mem_usage_max_avg_1h maxLimitPecent : 0.75 priority : ##cpu usage - name : cpu_usage_avg_5m weight : 0.2 - name : cpu_usage_max_avg_1h weight : 0.3 - name : cpu_usage_max_avg_1d weight : 0.5 ##memory usage - name : mem_usage_avg_5m weight : 0.2 - name : mem_usage_max_avg_1h weight : 0.3 - name : mem_usage_max_avg_1d weight : 0.5 hotValue : - timeRange : 5m count : 5 - timeRange : 1m count : 2 Modify kube-scheduler.yaml and replace kube-scheduler image with Crane-scheduler\uff1a kube-scheduler.yaml ... image : docker.io/gocrane/crane-scheduler:0.0.23 ... Install crane-scheduler-controller : Main Mirror kubectl apply -f https://raw.githubusercontent.com/gocrane/crane-scheduler/main/deploy/controller/rbac.yaml kubectl apply -f https://raw.githubusercontent.com/gocrane/crane-scheduler/main/deploy/controller/deployment.yaml kubectl apply -f https://finops.coding.net/p/gocrane/d/crane-scheduler/git/raw/main/deploy/controller/rbac.yaml?download = false kubectl apply -f https://finops.coding.net/p/gocrane/d/crane-scheduler/git/raw/main/deploy/controller/deployment.yaml?download = false","title":"Replace native Kube-scheduler with Crane-scheduler"},{"location":"tutorials/scheduling-pods-based-on-actual-node-load/#schedule-pods-with-crane-scheduler","text":"Test Crane-scheduler with following example: apiVersion : apps/v1 kind : Deployment metadata : name : cpu-stress spec : selector : matchLabels : app : cpu-stress replicas : 1 template : metadata : labels : app : cpu-stress spec : schedulerName : crane-scheduler hostNetwork : true tolerations : - key : node.kubernetes.io/network-unavailable operator : Exists effect : NoSchedule containers : - name : stress image : docker.io/gocrane/stress:latest command : [ \"stress\" , \"-c\" , \"1\" ] resources : requests : memory : \"1Gi\" cpu : \"1\" limits : memory : \"1Gi\" cpu : \"1\" Note Change crane-scheduler to default-scheduler if crane-scheduler is used as default. There will be the following event if the test pod is successfully scheduled: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 28s crane-scheduler Successfully assigned default/cpu-stress-7669499b57-zmrgb to vm-162-247-ubuntu","title":"Schedule Pods With Crane-scheduler"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/","text":"EffectiveHorizontalPodAutoscaler \u00b6 EffectiveHorizontalPodAutoscaler helps you manage application scaling in an easy way. It is compatible with HorizontalPodAutoscaler but extends more features. EffectiveHorizontalPodAutoscaler supports prediction-driven autoscaling. With this capability, user can forecast the incoming peak flow and scale up their application ahead, also user can know when the peak flow will end and scale down their application gracefully. Besides that, EffectiveHorizontalPodAutoscaler also defines several scale strategies to support different scaling scenarios. Features \u00b6 A EffectiveHorizontalPodAutoscaler sample yaml looks like below: apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache spec : scaleTargetRef : #(1) apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 #(2) maxReplicas : 10 #(3) scaleStrategy : Auto #(4) metrics : #(5) - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 prediction : #(6) predictionWindowSeconds : 3600 #(7) predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" ScaleTargetRef is the reference to the workload that should be scaled. MinReplicas is the lower limit replicas to the scale target which the autoscaler can scale down to. MaxReplicas is the upper limit replicas to the scale target which the autoscaler can scale up to. ScaleStrategy indicates the strategy to scaling target, value can be \"Auto\" and \"Preview\". Metrics contains the specifications for which to use to calculate the desired replica count. Prediction defines configurations for predict resources.If unspecified, defaults don't enable prediction. PredictionWindowSeconds is the time window to predict metrics in the future. Prediction-driven autoscaling \u00b6 Most of online applications follow regular pattern. We can predict future trend of hours or days. DSP is a time series prediction algorithm that applicable for application metrics prediction. The following shows a sample EffectiveHorizontalPodAutoscaler yaml with prediction enabled. apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : prediction : predictionWindowSeconds : 3600 predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" Metric conversion \u00b6 When user defines spec.metrics in EffectiveHorizontalPodAutoscaler and prediction configuration is enabled, EffectiveHPAController will convert it to a new metric and configure the background HorizontalPodAutoscaler. This is a source EffectiveHorizontalPodAutoscaler yaml for metric definition. apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 It's converted to underlying HorizontalPodAutoscaler metrics yaml. apiVersion : autoscaling/v2beta1 kind : HorizontalPodAutoscaler spec : metrics : - pods : metric : name : crane_pod_cpu_usage selector : matchLabels : autoscaling.crane.io/effective-hpa-uid : f9b92249-eab9-4671-afe0-17925e5987b8 target : type : AverageValue averageValue : 100m type : Pods - resource : name : cpu target : type : Utilization averageUtilization : 50 type : Resource In this sample, the resource metric defined by user is converted into two metrics: prediction metric and origin metric. prediction metric is custom metrics that provided by component MetricAdapter. Since custom metric doesn't support targetAverageUtilization , it's converted to targetAverageValue based on target pod cpu request. origin metric is equivalent to user defined metrics in EffectiveHorizontalPodAutoscaler, to fall back to baseline user defined in case of some unexpected situation e.g. business traffic sudden growth. HorizontalPodAutoscaler will calculate on each metric, and propose new replicas based on that. The largest one will be picked as the new scale. Horizontal scaling process \u00b6 There are six steps of prediction and scaling process: EffectiveHPAController create HorizontalPodAutoscaler and TimeSeriesPrediction instance PredictionCore get historic metric from prometheus and persist into TimeSeriesPrediction HPAController read metrics from KubeApiServer KubeApiServer forward requests to MetricAdapter and MetricServer HPAController calculate all metric results and propose a new scale replicas for target HPAController scale target with Scale Api Below is the process flow. Use case \u00b6 Let's take one use case that using EffectiveHorizontalPodAutoscaler in production cluster. We did a profiling on the load history of one application in production and replayed it in staging environment. With the same application, we leverage both EffectiveHorizontalPodAutoscaler and HorizontalPodAutoscaler to manage the scale and compare the result. From the red line in below chart, we can see its actual total cpu usage is high at ~8am, ~12pm, ~8pm and low in midnight. The green line shows the prediction cpu usage trend. Below is the comparison result between EffectiveHorizontalPodAutoscaler and HorizontalPodAutoscaler. The red line is the replica number generated by HorizontalPodAutoscaler and the green line is the result from EffectiveHorizontalPodAutoscaler. We can see significant improvement with EffectiveHorizontalPodAutoscaler: Scale up in advance before peek flow Scale down gracefully after peek flow Fewer replicas changes than HorizontalPodAutoscaler ScaleStrategy \u00b6 EffectiveHorizontalPodAutoscaler provides two strategies for scaling: Auto and Preview . User can change the strategy at runtime, and it will take effect on the fly. Auto \u00b6 Auto strategy achieves automatic scaling based on metrics. It is the default strategy. With this strategy, EffectiveHorizontalPodAutoscaler will create and control a HorizontalPodAutoscaler instance in backend. We don't recommend explicit configuration on the underlying HorizontalPodAutoscaler because it will be overridden by EffectiveHPAController. If user delete EffectiveHorizontalPodAutoscaler, HorizontalPodAutoscaler will be cleaned up too. Preview \u00b6 Preview strategy means EffectiveHorizontalPodAutoscaler won't change target's replicas automatically, so you can preview the calculated replicas and control target's replicas by themselves. User can switch from default strategy to this one by applying spec.scaleStrategy to Preview . It will take effect immediately, During the switch, EffectiveHPAController will disable HorizontalPodAutoscaler if exists and scale the target to the value spec.specificReplicas , if user not set spec.specificReplicas , when ScaleStrategy is change to Preview, it will just stop scaling. A sample preview configuration looks like following: apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : scaleStrategy : Preview # ScaleStrategy indicate the strategy to scaling target, value can be \"Auto\" and \"Preview\". specificReplicas : 5 # SpecificReplicas specify the target replicas. status : expectReplicas : 4 # expectReplicas is the calculated replicas that based on prediction metrics or spec.specificReplicas. currentReplicas : 4 # currentReplicas is actual replicas from target HorizontalPodAutoscaler compatible \u00b6 EffectiveHorizontalPodAutoscaler is designed to be compatible with k8s native HorizontalPodAutoscaler, because we don't reinvent the autoscaling part but take advantage of the extension from HorizontalPodAutoscaler and build a high level autoscaling CRD. EffectiveHorizontalPodAutoscaler support all abilities from HorizontalPodAutoscaler like metricSpec and behavior. EffectiveHorizontalPodAutoscaler will continue support incoming new feature from HorizontalPodAutoscaler. EffectiveHorizontalPodAutoscaler status \u00b6 This is a yaml from EffectiveHorizontalPodAutoscaler.Status apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler status : conditions : - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : the HPA controller was able to get the target's current scale reason : SucceededGetScale status : \"True\" type : AbleToScale - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : Effective HPA is ready reason : EffectiveHorizontalPodAutoscalerReady status : \"True\" type : Ready currentReplicas : 1 expectReplicas : 0 Cron-based autoscaling \u00b6 EffectiveHorizontalPodAutoscaler supports cron based autoscaling. Besides based on monitoring metrics, sometimes there are differences between holiday and weekdays in workload traffic, and a simple prediction algorithm may not work relatively well. Then you can make up for the lack of prediction by setting the weekend cron to have a larger number of replicas. For some non-web traffic applications, for example, some applications do not need to work on weekends, and then want to reduce the workload replicas to 1, you can also configure cron to reduce the cost for your service. Following are cron main fields in the ehpa spec: CronSpec: You can set multiple cron autoscaling configurations, cron cycle can set the start time and end time of the cycle, and the number of replicas of the workload can be continuously guaranteed to the set target value within the time range. Name: cron identifier TargetReplicas: the target number of replicas of the workload in this cron time range. Start: The start time of the cron, in the standard linux crontab format End: the end time of the cron, in the standard linux crontab format Current cron autoscaling capabilities from some manufacturers and communities have some shortcomings. The cron capability is provided separately, has no global view of autoscaling, poor compatibility with HPA, and conflicts with other scale trigger. The semantics and behavior of cron do not match very well, and are even very difficult to understand when used, which can easily mislead users and lead to autoscaling failures. The following figure shows the comparison between the current EHPA cron autoscaling implementation and other cron capabilities. To address the above issues, the cron autoscaling implemented by EHPA is designed on the basis of compatibility with HPA, and cron, as an indicator of HPA, acts on the workload object together with other indicators. In addition, the setting of cron is also very simple. When cron is configured separately, the default scaling of the workload will not be performed when it is not in the active time range. Cron working without other metrics \u00b6 You can just configure cron itself to work, assume you have no other metrics configured. apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache-local spec : # ScaleTargetRef is the reference to the workload that should be scaled. scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 # MinReplicas is the lower limit replicas to the scale target which the autoscaler can scale down to. maxReplicas : 100 # MaxReplicas is the upper limit replicas to the scale target which the autoscaler can scale up to. scaleStrategy : Auto # ScaleStrategy indicate the strategy to scaling target, value can be \"Auto\" and \"Manual\". # Better to setting cron to fill the one complete time period such as one day, one week # Below is one day cron scheduling, it #(targetReplicas) #80 -------- --------- ---------- # | | | | | | #10 ------------ ----- -------- ---------- #(time) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Local timezone means you use the server's(or maybe is a container's) timezone which the craned running in. for example, if your craned started as utc timezone, then it is utc. if it started as Asia/Shanghai, then it is Asia/Shanghai. crons : - name : \"cron1\" timezone : \"Local\" description : \"scale down\" start : \"0 0 ? * *\" end : \"0 6 ? * *\" targetReplicas : 10 - name : \"cron2\" timezone : \"Local\" description : \"scale up\" start : \"0 6 ? * *\" end : \"0 9 ? * *\" targetReplicas : 80 - name : \"cron3\" timezone : \"Local\" description : \"scale down\" start : \"00 9 ? * *\" end : \"00 11 ? * *\" targetReplicas : 10 - name : \"cron4\" timezone : \"Local\" description : \"scale up\" start : \"00 11 ? * *\" end : \"00 14 ? * *\" targetReplicas : 80 - name : \"cron5\" timezone : \"Local\" description : \"scale down\" start : \"00 14 ? * *\" end : \"00 17 ? * *\" targetReplicas : 10 - name : \"cron6\" timezone : \"Local\" description : \"scale up\" start : \"00 17 ? * *\" end : \"00 20 ? * *\" targetReplicas : 80 - name : \"cron7\" timezone : \"Local\" description : \"scale down\" start : \"00 20 ? * *\" end : \"00 00 ? * *\" targetReplicas : 10 CronSpec has following fields. name defines the name of the cron, cron name must be unique in the same ehpa description defines the details description of the cron. it can be empty. timezone defines the timezone of the cron which the crane to schedule in. If unspecified, default use UTC timezone. you can set it to Local which means you use timezone of the container of crane service running in. Also, America/Los_Angeles is ok. start defines the cron start time schedule, which is crontab format. see https://en.wikipedia.org/wiki/Cron end defines the cron end time schedule, which is crontab format. see https://en.wikipedia.org/wiki/Cron targetReplicas defines the target replicas the workload to scale when the cron is active, which means current time is between start and end. Above means each day, the workload needs to keep the replicas hourly. #80 -------- --------- ---------- # | | | | | | #1 ------------ ----- -------- ---------- #(time) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Remember not to set start time is after end . For example, when you set following: crons: - name: \"cron2\" timezone: \"Local\" description: \"scale up\" start: \"0 9 ? * *\" end: \"0 6 ? * *\" targetReplicas: 80 Above is not valid because the start will be always later than end. The hpa controller will always get the workload's desired replica to scale, which means keep the original replicas. Horizontal scaling process \u00b6 There are six steps of cron-driven and scaling process: EffectiveHPAController creates HorizontalPodAutoscaler which is injected to external cron metrics in spec. HPAController reads cron external metrics from KubeApiServer KubeApiServer forwards requests to MetricAdapter and MetricServer The MetricAdapter finds the cron scaler for target hpa, and detect if the cron scaler is active, which means the current time is between the cron start and end schedule time. It will return the TargetReplicas specified in the CronSpec . HPAController calculates all metric results and propose a new scale replicas for target by selecting the largest one. HPAController scales target with Scale Api When use ehpa, users can configure only cron metric, let the ehpa to be used as cron hpa. Multiple crons of one ehpa will be transformed to one external metric. HPA will fetch this external cron metric and calculates target replicas when reconcile. HPA will select the largest proposal replicas to scale the workload from multiple metrics. Cron working with other metrics together \u00b6 EffectiveHorizontalPodAutoscaler is compatible with HorizontalPodAutoscaler(Which is kubernetes built in). So if you configured metrics for HPA such as cpu or memory, then the HPA will scale by the real time metric it observed. With EHPA, users can configure CronMetric\u3001PredictionMetric\u3001OriginalMetric at the same time. We highly recomend you configure metrics of all dimensions. They are represtenting the cron replicas, prior predicted replicas, posterior observed replicas. This is a powerful feature. Because HPA always pick the largest replicas calculated by all dimensional metrics to scale. Which will gurantee your workload's QoS, when you configure three types of autoscaling at the same time, the replicas caculated by real metric observed is largest, then it will use the max one. Although the replicas caculated by prediction metric is smaller for some unexpected reason. So you don't be worried about the QoS. Mechanism \u00b6 When metrics adapter deal with the external cron metric requests, metrics adapter will do following steps. graph LR A[Start] --> B{Active Cron?}; B -->|Yes| C(largest targetReplicas) --> F; B -->|No| D{Work together with other metrics?}; D -->|Yes| G(minimum replicas) --> F; D -->|No| H(current replicas) --> F; F[Result workload replicas]; No active cron now, there are two cases: no other hpa metrics work with cron together, then return current workload replicas to keep the original desired replicas other hpa metrics work with cron together, then return min value to remove the cron impact for other metrics. when cron is working with other metrics together, it should not return workload's original desired replicas, because there maybe other metrics want to trigger the workload to scale in. hpa controller select max replicas computed by all metrics(this is hpa default policy in hard code), cron will impact the hpa. so we should remove the cron effect when cron is not active, it should return min value. Has active ones. we use the largest targetReplicas specified in cron spec. Basically, there should not be more then one active cron at the same time period, it is not a best practice. HPA will get the cron external metric value, then it will compute the replicas by itself. Use Case \u00b6 When you need to keep the workload replicas to minimum at midnight, you configured cron. And you need the HPA to get the real metric observed by metrics server to do scale based on real time observed metric. At last you configure a prediction-driven metric to do scale up early and scale down lately by predicting way. apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache-multi-dimensions spec : # ScaleTargetRef is the reference to the workload that should be scaled. scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 # MinReplicas is the lower limit replicas to the scale target which the autoscaler can scale down to. maxReplicas : 100 # MaxReplicas is the upper limit replicas to the scale target which the autoscaler can scale up to. scaleStrategy : Auto # ScaleStrategy indicate the strategy to scaling target, value can be \"Auto\" and \"Manual\". # Metrics contains the specifications for which to use to calculate the desired replica count. metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 # Prediction defines configurations for predict resources. # If unspecified, defaults don't enable prediction. prediction : predictionWindowSeconds : 3600 # PredictionWindowSeconds is the time window to predict metrics in the future. predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" crons : - name : \"cron1\" description : \"scale up\" start : \"0 0 ? * 6\" end : \"00 23 ? * 0\" targetReplicas : 100 FAQ \u00b6 error: unable to get metric crane_pod_cpu_usage \u00b6 When checking the status for EffectiveHorizontalPodAutoscaler, you may see this error: - lastTransitionTime : \"2022-05-15T14:05:43Z\" message : 'the HPA was unable to compute the replica count: unable to get metric crane_pod_cpu_usage: unable to fetch metrics from custom metrics API: TimeSeriesPrediction is not ready. ' reason : FailedGetPodsMetric status : \"False\" type : ScalingActive reason: Not all workload's cpu metric are predictable, if predict your workload failed, it will show above errors. solution: Just waiting. the Prediction algorithm need more time, you can see DSP section to know more about this algorithm. EffectiveHorizontalPodAutoscaler have a protection mechanism when prediction failed, it will use the actual cpu utilization to do autoscaling.","title":"Effective HPA"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#effectivehorizontalpodautoscaler","text":"EffectiveHorizontalPodAutoscaler helps you manage application scaling in an easy way. It is compatible with HorizontalPodAutoscaler but extends more features. EffectiveHorizontalPodAutoscaler supports prediction-driven autoscaling. With this capability, user can forecast the incoming peak flow and scale up their application ahead, also user can know when the peak flow will end and scale down their application gracefully. Besides that, EffectiveHorizontalPodAutoscaler also defines several scale strategies to support different scaling scenarios.","title":"EffectiveHorizontalPodAutoscaler"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#features","text":"A EffectiveHorizontalPodAutoscaler sample yaml looks like below: apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache spec : scaleTargetRef : #(1) apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 #(2) maxReplicas : 10 #(3) scaleStrategy : Auto #(4) metrics : #(5) - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 prediction : #(6) predictionWindowSeconds : 3600 #(7) predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" ScaleTargetRef is the reference to the workload that should be scaled. MinReplicas is the lower limit replicas to the scale target which the autoscaler can scale down to. MaxReplicas is the upper limit replicas to the scale target which the autoscaler can scale up to. ScaleStrategy indicates the strategy to scaling target, value can be \"Auto\" and \"Preview\". Metrics contains the specifications for which to use to calculate the desired replica count. Prediction defines configurations for predict resources.If unspecified, defaults don't enable prediction. PredictionWindowSeconds is the time window to predict metrics in the future.","title":"Features"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#prediction-driven-autoscaling","text":"Most of online applications follow regular pattern. We can predict future trend of hours or days. DSP is a time series prediction algorithm that applicable for application metrics prediction. The following shows a sample EffectiveHorizontalPodAutoscaler yaml with prediction enabled. apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : prediction : predictionWindowSeconds : 3600 predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\"","title":"Prediction-driven autoscaling"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#metric-conversion","text":"When user defines spec.metrics in EffectiveHorizontalPodAutoscaler and prediction configuration is enabled, EffectiveHPAController will convert it to a new metric and configure the background HorizontalPodAutoscaler. This is a source EffectiveHorizontalPodAutoscaler yaml for metric definition. apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 It's converted to underlying HorizontalPodAutoscaler metrics yaml. apiVersion : autoscaling/v2beta1 kind : HorizontalPodAutoscaler spec : metrics : - pods : metric : name : crane_pod_cpu_usage selector : matchLabels : autoscaling.crane.io/effective-hpa-uid : f9b92249-eab9-4671-afe0-17925e5987b8 target : type : AverageValue averageValue : 100m type : Pods - resource : name : cpu target : type : Utilization averageUtilization : 50 type : Resource In this sample, the resource metric defined by user is converted into two metrics: prediction metric and origin metric. prediction metric is custom metrics that provided by component MetricAdapter. Since custom metric doesn't support targetAverageUtilization , it's converted to targetAverageValue based on target pod cpu request. origin metric is equivalent to user defined metrics in EffectiveHorizontalPodAutoscaler, to fall back to baseline user defined in case of some unexpected situation e.g. business traffic sudden growth. HorizontalPodAutoscaler will calculate on each metric, and propose new replicas based on that. The largest one will be picked as the new scale.","title":"Metric conversion"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#horizontal-scaling-process","text":"There are six steps of prediction and scaling process: EffectiveHPAController create HorizontalPodAutoscaler and TimeSeriesPrediction instance PredictionCore get historic metric from prometheus and persist into TimeSeriesPrediction HPAController read metrics from KubeApiServer KubeApiServer forward requests to MetricAdapter and MetricServer HPAController calculate all metric results and propose a new scale replicas for target HPAController scale target with Scale Api Below is the process flow.","title":"Horizontal scaling process"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#use-case","text":"Let's take one use case that using EffectiveHorizontalPodAutoscaler in production cluster. We did a profiling on the load history of one application in production and replayed it in staging environment. With the same application, we leverage both EffectiveHorizontalPodAutoscaler and HorizontalPodAutoscaler to manage the scale and compare the result. From the red line in below chart, we can see its actual total cpu usage is high at ~8am, ~12pm, ~8pm and low in midnight. The green line shows the prediction cpu usage trend. Below is the comparison result between EffectiveHorizontalPodAutoscaler and HorizontalPodAutoscaler. The red line is the replica number generated by HorizontalPodAutoscaler and the green line is the result from EffectiveHorizontalPodAutoscaler. We can see significant improvement with EffectiveHorizontalPodAutoscaler: Scale up in advance before peek flow Scale down gracefully after peek flow Fewer replicas changes than HorizontalPodAutoscaler","title":"Use case"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#scalestrategy","text":"EffectiveHorizontalPodAutoscaler provides two strategies for scaling: Auto and Preview . User can change the strategy at runtime, and it will take effect on the fly.","title":"ScaleStrategy"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#auto","text":"Auto strategy achieves automatic scaling based on metrics. It is the default strategy. With this strategy, EffectiveHorizontalPodAutoscaler will create and control a HorizontalPodAutoscaler instance in backend. We don't recommend explicit configuration on the underlying HorizontalPodAutoscaler because it will be overridden by EffectiveHPAController. If user delete EffectiveHorizontalPodAutoscaler, HorizontalPodAutoscaler will be cleaned up too.","title":"Auto"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#preview","text":"Preview strategy means EffectiveHorizontalPodAutoscaler won't change target's replicas automatically, so you can preview the calculated replicas and control target's replicas by themselves. User can switch from default strategy to this one by applying spec.scaleStrategy to Preview . It will take effect immediately, During the switch, EffectiveHPAController will disable HorizontalPodAutoscaler if exists and scale the target to the value spec.specificReplicas , if user not set spec.specificReplicas , when ScaleStrategy is change to Preview, it will just stop scaling. A sample preview configuration looks like following: apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : scaleStrategy : Preview # ScaleStrategy indicate the strategy to scaling target, value can be \"Auto\" and \"Preview\". specificReplicas : 5 # SpecificReplicas specify the target replicas. status : expectReplicas : 4 # expectReplicas is the calculated replicas that based on prediction metrics or spec.specificReplicas. currentReplicas : 4 # currentReplicas is actual replicas from target","title":"Preview"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#horizontalpodautoscaler-compatible","text":"EffectiveHorizontalPodAutoscaler is designed to be compatible with k8s native HorizontalPodAutoscaler, because we don't reinvent the autoscaling part but take advantage of the extension from HorizontalPodAutoscaler and build a high level autoscaling CRD. EffectiveHorizontalPodAutoscaler support all abilities from HorizontalPodAutoscaler like metricSpec and behavior. EffectiveHorizontalPodAutoscaler will continue support incoming new feature from HorizontalPodAutoscaler.","title":"HorizontalPodAutoscaler compatible"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#effectivehorizontalpodautoscaler-status","text":"This is a yaml from EffectiveHorizontalPodAutoscaler.Status apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler status : conditions : - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : the HPA controller was able to get the target's current scale reason : SucceededGetScale status : \"True\" type : AbleToScale - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : Effective HPA is ready reason : EffectiveHorizontalPodAutoscalerReady status : \"True\" type : Ready currentReplicas : 1 expectReplicas : 0","title":"EffectiveHorizontalPodAutoscaler status"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#cron-based-autoscaling","text":"EffectiveHorizontalPodAutoscaler supports cron based autoscaling. Besides based on monitoring metrics, sometimes there are differences between holiday and weekdays in workload traffic, and a simple prediction algorithm may not work relatively well. Then you can make up for the lack of prediction by setting the weekend cron to have a larger number of replicas. For some non-web traffic applications, for example, some applications do not need to work on weekends, and then want to reduce the workload replicas to 1, you can also configure cron to reduce the cost for your service. Following are cron main fields in the ehpa spec: CronSpec: You can set multiple cron autoscaling configurations, cron cycle can set the start time and end time of the cycle, and the number of replicas of the workload can be continuously guaranteed to the set target value within the time range. Name: cron identifier TargetReplicas: the target number of replicas of the workload in this cron time range. Start: The start time of the cron, in the standard linux crontab format End: the end time of the cron, in the standard linux crontab format Current cron autoscaling capabilities from some manufacturers and communities have some shortcomings. The cron capability is provided separately, has no global view of autoscaling, poor compatibility with HPA, and conflicts with other scale trigger. The semantics and behavior of cron do not match very well, and are even very difficult to understand when used, which can easily mislead users and lead to autoscaling failures. The following figure shows the comparison between the current EHPA cron autoscaling implementation and other cron capabilities. To address the above issues, the cron autoscaling implemented by EHPA is designed on the basis of compatibility with HPA, and cron, as an indicator of HPA, acts on the workload object together with other indicators. In addition, the setting of cron is also very simple. When cron is configured separately, the default scaling of the workload will not be performed when it is not in the active time range.","title":"Cron-based autoscaling"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#cron-working-without-other-metrics","text":"You can just configure cron itself to work, assume you have no other metrics configured. apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache-local spec : # ScaleTargetRef is the reference to the workload that should be scaled. scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 # MinReplicas is the lower limit replicas to the scale target which the autoscaler can scale down to. maxReplicas : 100 # MaxReplicas is the upper limit replicas to the scale target which the autoscaler can scale up to. scaleStrategy : Auto # ScaleStrategy indicate the strategy to scaling target, value can be \"Auto\" and \"Manual\". # Better to setting cron to fill the one complete time period such as one day, one week # Below is one day cron scheduling, it #(targetReplicas) #80 -------- --------- ---------- # | | | | | | #10 ------------ ----- -------- ---------- #(time) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Local timezone means you use the server's(or maybe is a container's) timezone which the craned running in. for example, if your craned started as utc timezone, then it is utc. if it started as Asia/Shanghai, then it is Asia/Shanghai. crons : - name : \"cron1\" timezone : \"Local\" description : \"scale down\" start : \"0 0 ? * *\" end : \"0 6 ? * *\" targetReplicas : 10 - name : \"cron2\" timezone : \"Local\" description : \"scale up\" start : \"0 6 ? * *\" end : \"0 9 ? * *\" targetReplicas : 80 - name : \"cron3\" timezone : \"Local\" description : \"scale down\" start : \"00 9 ? * *\" end : \"00 11 ? * *\" targetReplicas : 10 - name : \"cron4\" timezone : \"Local\" description : \"scale up\" start : \"00 11 ? * *\" end : \"00 14 ? * *\" targetReplicas : 80 - name : \"cron5\" timezone : \"Local\" description : \"scale down\" start : \"00 14 ? * *\" end : \"00 17 ? * *\" targetReplicas : 10 - name : \"cron6\" timezone : \"Local\" description : \"scale up\" start : \"00 17 ? * *\" end : \"00 20 ? * *\" targetReplicas : 80 - name : \"cron7\" timezone : \"Local\" description : \"scale down\" start : \"00 20 ? * *\" end : \"00 00 ? * *\" targetReplicas : 10 CronSpec has following fields. name defines the name of the cron, cron name must be unique in the same ehpa description defines the details description of the cron. it can be empty. timezone defines the timezone of the cron which the crane to schedule in. If unspecified, default use UTC timezone. you can set it to Local which means you use timezone of the container of crane service running in. Also, America/Los_Angeles is ok. start defines the cron start time schedule, which is crontab format. see https://en.wikipedia.org/wiki/Cron end defines the cron end time schedule, which is crontab format. see https://en.wikipedia.org/wiki/Cron targetReplicas defines the target replicas the workload to scale when the cron is active, which means current time is between start and end. Above means each day, the workload needs to keep the replicas hourly. #80 -------- --------- ---------- # | | | | | | #1 ------------ ----- -------- ---------- #(time) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Remember not to set start time is after end . For example, when you set following: crons: - name: \"cron2\" timezone: \"Local\" description: \"scale up\" start: \"0 9 ? * *\" end: \"0 6 ? * *\" targetReplicas: 80 Above is not valid because the start will be always later than end. The hpa controller will always get the workload's desired replica to scale, which means keep the original replicas.","title":"Cron working without other metrics"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#horizontal-scaling-process_1","text":"There are six steps of cron-driven and scaling process: EffectiveHPAController creates HorizontalPodAutoscaler which is injected to external cron metrics in spec. HPAController reads cron external metrics from KubeApiServer KubeApiServer forwards requests to MetricAdapter and MetricServer The MetricAdapter finds the cron scaler for target hpa, and detect if the cron scaler is active, which means the current time is between the cron start and end schedule time. It will return the TargetReplicas specified in the CronSpec . HPAController calculates all metric results and propose a new scale replicas for target by selecting the largest one. HPAController scales target with Scale Api When use ehpa, users can configure only cron metric, let the ehpa to be used as cron hpa. Multiple crons of one ehpa will be transformed to one external metric. HPA will fetch this external cron metric and calculates target replicas when reconcile. HPA will select the largest proposal replicas to scale the workload from multiple metrics.","title":"Horizontal scaling process"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#cron-working-with-other-metrics-together","text":"EffectiveHorizontalPodAutoscaler is compatible with HorizontalPodAutoscaler(Which is kubernetes built in). So if you configured metrics for HPA such as cpu or memory, then the HPA will scale by the real time metric it observed. With EHPA, users can configure CronMetric\u3001PredictionMetric\u3001OriginalMetric at the same time. We highly recomend you configure metrics of all dimensions. They are represtenting the cron replicas, prior predicted replicas, posterior observed replicas. This is a powerful feature. Because HPA always pick the largest replicas calculated by all dimensional metrics to scale. Which will gurantee your workload's QoS, when you configure three types of autoscaling at the same time, the replicas caculated by real metric observed is largest, then it will use the max one. Although the replicas caculated by prediction metric is smaller for some unexpected reason. So you don't be worried about the QoS.","title":"Cron working with other metrics together"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#mechanism","text":"When metrics adapter deal with the external cron metric requests, metrics adapter will do following steps. graph LR A[Start] --> B{Active Cron?}; B -->|Yes| C(largest targetReplicas) --> F; B -->|No| D{Work together with other metrics?}; D -->|Yes| G(minimum replicas) --> F; D -->|No| H(current replicas) --> F; F[Result workload replicas]; No active cron now, there are two cases: no other hpa metrics work with cron together, then return current workload replicas to keep the original desired replicas other hpa metrics work with cron together, then return min value to remove the cron impact for other metrics. when cron is working with other metrics together, it should not return workload's original desired replicas, because there maybe other metrics want to trigger the workload to scale in. hpa controller select max replicas computed by all metrics(this is hpa default policy in hard code), cron will impact the hpa. so we should remove the cron effect when cron is not active, it should return min value. Has active ones. we use the largest targetReplicas specified in cron spec. Basically, there should not be more then one active cron at the same time period, it is not a best practice. HPA will get the cron external metric value, then it will compute the replicas by itself.","title":"Mechanism"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#use-case_1","text":"When you need to keep the workload replicas to minimum at midnight, you configured cron. And you need the HPA to get the real metric observed by metrics server to do scale based on real time observed metric. At last you configure a prediction-driven metric to do scale up early and scale down lately by predicting way. apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache-multi-dimensions spec : # ScaleTargetRef is the reference to the workload that should be scaled. scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 # MinReplicas is the lower limit replicas to the scale target which the autoscaler can scale down to. maxReplicas : 100 # MaxReplicas is the upper limit replicas to the scale target which the autoscaler can scale up to. scaleStrategy : Auto # ScaleStrategy indicate the strategy to scaling target, value can be \"Auto\" and \"Manual\". # Metrics contains the specifications for which to use to calculate the desired replica count. metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 # Prediction defines configurations for predict resources. # If unspecified, defaults don't enable prediction. prediction : predictionWindowSeconds : 3600 # PredictionWindowSeconds is the time window to predict metrics in the future. predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" crons : - name : \"cron1\" description : \"scale up\" start : \"0 0 ? * 6\" end : \"00 23 ? * 0\" targetReplicas : 100","title":"Use Case"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#faq","text":"","title":"FAQ"},{"location":"tutorials/using-effective-hpa-to-scaling-with-effectiveness/#error-unable-to-get-metric-crane_pod_cpu_usage","text":"When checking the status for EffectiveHorizontalPodAutoscaler, you may see this error: - lastTransitionTime : \"2022-05-15T14:05:43Z\" message : 'the HPA was unable to compute the replica count: unable to get metric crane_pod_cpu_usage: unable to fetch metrics from custom metrics API: TimeSeriesPrediction is not ready. ' reason : FailedGetPodsMetric status : \"False\" type : ScalingActive reason: Not all workload's cpu metric are predictable, if predict your workload failed, it will show above errors. solution: Just waiting. the Prediction algorithm need more time, you can see DSP section to know more about this algorithm. EffectiveHorizontalPodAutoscaler have a protection mechanism when prediction failed, it will use the actual cpu utilization to do autoscaling.","title":"error: unable to get metric crane_pod_cpu_usage"},{"location":"tutorials/using-qos-ensurance/","text":"Qos Ensurance \u00b6 QoS ensurance guarantees the stability of the pods running on Kubernetes. Disable schedule, throttle, evict will be applied to low priority pods when the higher priority pods is impacted by resource competition. Qos Ensurance Architecture \u00b6 Qos ensurance's architecture is shown as below. It contains three modules. state collector: collect metrics periodically anomaly analyzer: analyze the node triggered anomaly used collected metrics action executor: execute avoidance actions, include disable scheduling, throttle and eviction. The main process: State collector synchronizes policies from kube-apiserver. If the policies are changed, the state collector updates the collectors. State collector collects metrics periodically. State collector transmits metrics to anomaly analyzer. Anomaly analyzer ranges all rules to analyze the avoidance threshold or the restored threshold reached. Anomaly analyzer merges the analyzed results and notices the avoidance actions. Action executor executes actions based on the analyzed results. Disable Scheduling \u00b6 The following AvoidanceAction and NodeQOSEnsurancePolicy can be defined. As a result, when the node CPU usage triggers the threshold, disable schedule action for the node will be executed. The sample YAML looks like below: apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : labels : app : system name : disablescheduling spec : description : disable schedule new pods to the node coolDownSeconds : 300 # The minimum wait time of the node from scheduling disable status to normal status apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline1\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 #(1) restoreThreshold : 2 #(2) actionName : \"disablescheduling\" #(3) strategy : \"None\" #(4) metricRule : name : \"cpu_total_usage\" #(5) value : 4000 #(6) We consider the rule is triggered, when the threshold reached continued so many times We consider the rule is restored, when the threshold not reached continued so many times Name of AvoidanceAction which be associated Strategy for the action, you can set it \"Preview\" to not perform actually Name of metric Threshold of metric Please check the video to learn more about the scheduling disable actions. Throttle \u00b6 The following AvoidanceAction and NodeQOSEnsurancePolicy can be defined. As a result, when the node CPU usage triggers the threshold, throttle action for the node will be executed. The sample YAML looks like below: apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : throttle labels : app : system spec : coolDownSeconds : 300 throttle : cpuThrottle : minCPURatio : 10 #(1) stepCPURatio : 10 #(2) description : \"throttle low priority pods\" The minimal ratio of the CPU quota, if the pod is throttled lower than this ratio, it will be set to this. The step for throttle action. It will reduce this percentage of CPU quota in each avoidance triggered.It will increase this percentage of CPU quota in each restored. apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline2\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoredThreshold : 2 actionName : \"throttle\" strategy : \"None\" metricRule : name : \"cpu_total_usage\" value : 6000 Eviction \u00b6 The following YAML is another case, low priority pods on the node will be evicted, when the node CPU usage trigger the threshold. apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : eviction labels : app : system spec : coolDownSeconds : 300 eviction : terminationGracePeriodSeconds : 30 #(1) description : \"evict low priority pods\" Duration in seconds the pod needs to terminate gracefully. apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline3\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoreThreshold : 2 actionName : \"evict\" strategy : \"Preview\" #(1) metricRule : name : \"cpu_total_usage\" value : 6000 Strategy for the action, \"Preview\" to not perform actually Supported Metrics \u00b6 Name Description cpu_total_usage node cpu usage cpu_total_utilization node cpu utilization","title":"Qos Ensurance"},{"location":"tutorials/using-qos-ensurance/#qos-ensurance","text":"QoS ensurance guarantees the stability of the pods running on Kubernetes. Disable schedule, throttle, evict will be applied to low priority pods when the higher priority pods is impacted by resource competition.","title":"Qos Ensurance"},{"location":"tutorials/using-qos-ensurance/#qos-ensurance-architecture","text":"Qos ensurance's architecture is shown as below. It contains three modules. state collector: collect metrics periodically anomaly analyzer: analyze the node triggered anomaly used collected metrics action executor: execute avoidance actions, include disable scheduling, throttle and eviction. The main process: State collector synchronizes policies from kube-apiserver. If the policies are changed, the state collector updates the collectors. State collector collects metrics periodically. State collector transmits metrics to anomaly analyzer. Anomaly analyzer ranges all rules to analyze the avoidance threshold or the restored threshold reached. Anomaly analyzer merges the analyzed results and notices the avoidance actions. Action executor executes actions based on the analyzed results.","title":"Qos Ensurance Architecture"},{"location":"tutorials/using-qos-ensurance/#disable-scheduling","text":"The following AvoidanceAction and NodeQOSEnsurancePolicy can be defined. As a result, when the node CPU usage triggers the threshold, disable schedule action for the node will be executed. The sample YAML looks like below: apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : labels : app : system name : disablescheduling spec : description : disable schedule new pods to the node coolDownSeconds : 300 # The minimum wait time of the node from scheduling disable status to normal status apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline1\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 #(1) restoreThreshold : 2 #(2) actionName : \"disablescheduling\" #(3) strategy : \"None\" #(4) metricRule : name : \"cpu_total_usage\" #(5) value : 4000 #(6) We consider the rule is triggered, when the threshold reached continued so many times We consider the rule is restored, when the threshold not reached continued so many times Name of AvoidanceAction which be associated Strategy for the action, you can set it \"Preview\" to not perform actually Name of metric Threshold of metric Please check the video to learn more about the scheduling disable actions.","title":"Disable Scheduling"},{"location":"tutorials/using-qos-ensurance/#throttle","text":"The following AvoidanceAction and NodeQOSEnsurancePolicy can be defined. As a result, when the node CPU usage triggers the threshold, throttle action for the node will be executed. The sample YAML looks like below: apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : throttle labels : app : system spec : coolDownSeconds : 300 throttle : cpuThrottle : minCPURatio : 10 #(1) stepCPURatio : 10 #(2) description : \"throttle low priority pods\" The minimal ratio of the CPU quota, if the pod is throttled lower than this ratio, it will be set to this. The step for throttle action. It will reduce this percentage of CPU quota in each avoidance triggered.It will increase this percentage of CPU quota in each restored. apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline2\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoredThreshold : 2 actionName : \"throttle\" strategy : \"None\" metricRule : name : \"cpu_total_usage\" value : 6000","title":"Throttle"},{"location":"tutorials/using-qos-ensurance/#eviction","text":"The following YAML is another case, low priority pods on the node will be evicted, when the node CPU usage trigger the threshold. apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : eviction labels : app : system spec : coolDownSeconds : 300 eviction : terminationGracePeriodSeconds : 30 #(1) description : \"evict low priority pods\" Duration in seconds the pod needs to terminate gracefully. apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline3\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoreThreshold : 2 actionName : \"evict\" strategy : \"Preview\" #(1) metricRule : name : \"cpu_total_usage\" value : 6000 Strategy for the action, \"Preview\" to not perform actually","title":"Eviction"},{"location":"tutorials/using-qos-ensurance/#supported-metrics","text":"Name Description cpu_total_usage node cpu usage cpu_total_utilization node cpu utilization","title":"Supported Metrics"},{"location":"tutorials/using-time-series-prediction/","text":"TimeSeriesPrediction \u00b6 Knowing the future makes things easier for us. Many businesses are naturally cyclical in time series, especially for those that directly or indirectly serve \"people\". This periodicity is determined by the regularity of people\u2019s daily activities. For example, people are accustomed to ordering take-out at noon and in the evenings; there are always traffic peaks in the morning and evening; even for services that don't have such obvious patterns, such as searching, the amount of requests at night is much lower than that during business hours. For applications related to this kind of business, it is a natural idea to infer the next day's metrics from the historical data of the past few days, or to infer the coming Monday's access traffic from the data of last Monday. With predicted metrics or traffic patterns in the next 24 hours, we can better manage our application instances, stabilize our system, and meanwhile, reduce the cost. TimeSeriesPrediction is used to forecast the kubernetes object metric. It is based on PredictionCore to do forecast. Features \u00b6 A TimeSeriesPrediction sample yaml looks like below: apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : targetRef : kind : Node name : 192.168.56.166 predictionWindowSeconds : 600 predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" - resourceIdentifier : node-mem type : ResourceQuery resourceQuery : memory algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"1000000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" spec.targetRef defines the reference to the kubernetes object including Node or other workload such as Deployment. spec.predictionMetrics defines the metrics about the spec.targetRef. spec.predictionWindowSeconds is a prediction time series duration. the TimeSeriesPredictionController will rotate the predicted data in spec.Status for consumer to consume the predicted time series data. PredictionMetrics \u00b6 apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" MetricType \u00b6 There are three types of the metric query: ResourceQuery is a kubernetes built-in resource metric such as cpu or memory. crane supports only cpu and memory now. RawQuery is a query by DSL, such as prometheus query language. now support prometheus. ExpressionQuery is a query by Expression selector. Now we only support prometheus as data source. We define the MetricType to orthogonal with the datasource. but now maybe some datasources do not support the metricType. Algorithm \u00b6 Algorithm define the algorithm type and params to do predict for the metric. Now there are two kinds of algorithms: dsp is an algorithm to forcasting a time series, it is based on FFT(Fast Fourier Transform), it is good at predicting some time series with seasonality and periods. percentile is an algorithm to estimate a time series, and find a recommended value to represent the past time series, it is based on exponentially-decaying weights historgram statistics. it is used to estimate a time series, it is not good at to predict a time sequences, although the percentile can output a time series predicted data, but it is all the same value. so if you want to predict a time sequences, dsp is a better choice. dsp params \u00b6 percentile params \u00b6","title":"Time Series Prediction"},{"location":"tutorials/using-time-series-prediction/#timeseriesprediction","text":"Knowing the future makes things easier for us. Many businesses are naturally cyclical in time series, especially for those that directly or indirectly serve \"people\". This periodicity is determined by the regularity of people\u2019s daily activities. For example, people are accustomed to ordering take-out at noon and in the evenings; there are always traffic peaks in the morning and evening; even for services that don't have such obvious patterns, such as searching, the amount of requests at night is much lower than that during business hours. For applications related to this kind of business, it is a natural idea to infer the next day's metrics from the historical data of the past few days, or to infer the coming Monday's access traffic from the data of last Monday. With predicted metrics or traffic patterns in the next 24 hours, we can better manage our application instances, stabilize our system, and meanwhile, reduce the cost. TimeSeriesPrediction is used to forecast the kubernetes object metric. It is based on PredictionCore to do forecast.","title":"TimeSeriesPrediction"},{"location":"tutorials/using-time-series-prediction/#features","text":"A TimeSeriesPrediction sample yaml looks like below: apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : targetRef : kind : Node name : 192.168.56.166 predictionWindowSeconds : 600 predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" - resourceIdentifier : node-mem type : ResourceQuery resourceQuery : memory algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"1000000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" spec.targetRef defines the reference to the kubernetes object including Node or other workload such as Deployment. spec.predictionMetrics defines the metrics about the spec.targetRef. spec.predictionWindowSeconds is a prediction time series duration. the TimeSeriesPredictionController will rotate the predicted data in spec.Status for consumer to consume the predicted time series data.","title":"Features"},{"location":"tutorials/using-time-series-prediction/#predictionmetrics","text":"apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\"","title":"PredictionMetrics"},{"location":"tutorials/using-time-series-prediction/#metrictype","text":"There are three types of the metric query: ResourceQuery is a kubernetes built-in resource metric such as cpu or memory. crane supports only cpu and memory now. RawQuery is a query by DSL, such as prometheus query language. now support prometheus. ExpressionQuery is a query by Expression selector. Now we only support prometheus as data source. We define the MetricType to orthogonal with the datasource. but now maybe some datasources do not support the metricType.","title":"MetricType"},{"location":"tutorials/using-time-series-prediction/#algorithm","text":"Algorithm define the algorithm type and params to do predict for the metric. Now there are two kinds of algorithms: dsp is an algorithm to forcasting a time series, it is based on FFT(Fast Fourier Transform), it is good at predicting some time series with seasonality and periods. percentile is an algorithm to estimate a time series, and find a recommended value to represent the past time series, it is based on exponentially-decaying weights historgram statistics. it is used to estimate a time series, it is not good at to predict a time sequences, although the percentile can output a time series predicted data, but it is all the same value. so if you want to predict a time sequences, dsp is a better choice.","title":"Algorithm"},{"location":"tutorials/using-time-series-prediction/#dsp-params","text":"","title":"dsp params"},{"location":"tutorials/using-time-series-prediction/#percentile-params","text":"","title":"percentile params"},{"location":"zh/","text":"\u4ecb\u7ecd \u00b6 The goal of Crane is to provide a one-stop-shop project to help Kubernetes users to save cloud resource usage with a rich set of functionalities: Time Series Prediction based on monitoring data Usage and Cost visibility Usage & Cost Optimization including: R2 (Resource Re-allocation) R3 (Request & Replicas Recommendation) Effective Pod Autoscaling (Effective Horizontal & Vertical Pod Autoscaling) Cost Optimization Enhanced QoS based on Pod PriorityClass Load-aware Scheduling Features \u00b6 Time Series Prediction \u00b6 TimeSeriesPrediction defines metric spec to predict kubernetes resources like Pod or Node. The prediction module is the core component that other crane components relied on, like EHPA and Analytics . Please see this document to learn more. Effective HorizontalPodAutoscaler \u00b6 EffectiveHorizontalPodAutoscaler helps you manage application scaling in an easy way. It is compatible with native HorizontalPodAutoscaler but extends more features like prediction-driven autoscaling. Please see this document to learn more. Analytics \u00b6 \u667a\u80fd\u63a8\u8350\u80fd\u591f\u5e2e\u52a9\u7528\u6237\u81ea\u52a8\u5206\u6790\u96c6\u7fa4\u5e76\u7ed9\u51fa\u4f18\u5316\u5efa\u8bae\u3002\u5c31\u50cf\u624b\u673a\u52a9\u624b\u4e00\u6837\uff0c\u667a\u80fd\u63a8\u8350\u4f1a\u5b9a\u671f\u7684\u626b\u63cf\u3001\u5206\u6790\u4f60\u7684\u96c6\u7fa4\u5e76\u7ed9\u51fa\u63a8\u8350\u5efa\u8bae\u3002\u76ee\u524d\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e24\u79cd\u4f18\u5316\u80fd\u529b\uff1a \u8d44\u6e90\u63a8\u8350 : \u901a\u8fc7\u8d44\u6e90\u63a8\u8350\u7684\u7b97\u6cd5\u5206\u6790\u5e94\u7528\u7684\u771f\u5b9e\u7528\u91cf\u63a8\u8350\u66f4\u5408\u9002\u7684\u8d44\u6e90\u914d\u7f6e\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003\u5e76\u91c7\u7eb3\u5b83\u63d0\u5347\u96c6\u7fa4\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002 \u526f\u672c\u6570\u63a8\u8350 : \u901a\u8fc7\u526f\u672c\u6570\u63a8\u8350\u7684\u7b97\u6cd5\u5206\u6790\u5e94\u7528\u7684\u771f\u5b9e\u7528\u91cf\u63a8\u8350\u66f4\u5408\u9002\u7684\u526f\u672c\u548c EHPA \u914d\u7f6e\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003\u5e76\u91c7\u7eb3\u5b83\u63d0\u5347\u96c6\u7fa4\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002 Please see this document to learn more. QoS Ensurance \u00b6 Kubernetes is capable of starting multiple pods on same node, and as a result, some of the user applications may be impacted when there are resources(e.g. cpu) consumption competition. To mitigate this, Crane allows users defining PrioirtyClass for the pods and QoSEnsurancePolicy, and then detects disruption and ensure the high priority pods not being impacted by resource competition. Avoidance Actions: Disable Schedule : disable scheduling by setting node taint and condition Throttle : throttle the low priority pods by squeezing cgroup settings Evict : evict low priority pods Please see this document to learn more. \u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6 \u00b6 \u539f\u751f\u7684 Kubernetes \u8c03\u5ea6\u5668\u53ea\u80fd\u57fa\u4e8e\u8d44\u6e90\u7684 Request \u8fdb\u884c\u8c03\u5ea6\u4e1a\u52a1\uff0c\u8fd9\u5f88\u5bb9\u6613\u5bfc\u81f4\u96c6\u7fa4\u8d1f\u8f7d\u4e0d\u5747\u7684\u95ee\u9898\u3002\u4e0e\u4e4b\u5bf9\u6bd4\u7684\u662f\uff0c Crane-scheudler \u53ef\u4ee5\u76f4\u63a5\u4ece Prometheus \u83b7\u53d6\u8282\u70b9\u7684\u771f\u5b9e\u8d1f\u8f7d\u60c5\u51b5\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u8c03\u5ea6\u3002 \u66f4\u591a\u8bf7\u53c2\u89c1 \u6587\u6863 \u3002 Repositories \u00b6 Crane is composed of the following components: craned - main crane control plane. Predictor - Predicts resources metrics trends based on historical data. AnalyticsController - Analyzes resources and generate related recommendations. RecommendationController - Recommend Pod resource requests and autoscaler. ClusterNodePredictionController - Create Predictor for nodes. EffectiveHPAController - Effective HPA for horizontal scaling. EffectiveVPAController - Effective VPA for vertical scaling. metric-adaptor - Metric server for driving the scaling. crane-agent - Ensure critical workloads SLO based on abnormally detection. gocrane/api - This repository defines component-level APIs for the Crane platform. gocrane/fadvisor - Financial advisor which collect resource prices from cloud API. gocrane/crane-scheduler - \u4e00\u4e2a\u53ef\u4ee5\u57fa\u4e8e\u771f\u5b9e\u8d1f\u8f7d\u5bf9\u4e1a\u52a1\u8fdb\u884c\u8c03\u5ea6\u7684 Kubernestes \u8c03\u5ea6\u5668\u3002","title":"\u4ecb\u7ecd"},{"location":"zh/#_1","text":"The goal of Crane is to provide a one-stop-shop project to help Kubernetes users to save cloud resource usage with a rich set of functionalities: Time Series Prediction based on monitoring data Usage and Cost visibility Usage & Cost Optimization including: R2 (Resource Re-allocation) R3 (Request & Replicas Recommendation) Effective Pod Autoscaling (Effective Horizontal & Vertical Pod Autoscaling) Cost Optimization Enhanced QoS based on Pod PriorityClass Load-aware Scheduling","title":"\u4ecb\u7ecd"},{"location":"zh/#features","text":"","title":"Features"},{"location":"zh/#time-series-prediction","text":"TimeSeriesPrediction defines metric spec to predict kubernetes resources like Pod or Node. The prediction module is the core component that other crane components relied on, like EHPA and Analytics . Please see this document to learn more.","title":"Time Series Prediction"},{"location":"zh/#effective-horizontalpodautoscaler","text":"EffectiveHorizontalPodAutoscaler helps you manage application scaling in an easy way. It is compatible with native HorizontalPodAutoscaler but extends more features like prediction-driven autoscaling. Please see this document to learn more.","title":"Effective HorizontalPodAutoscaler"},{"location":"zh/#analytics","text":"\u667a\u80fd\u63a8\u8350\u80fd\u591f\u5e2e\u52a9\u7528\u6237\u81ea\u52a8\u5206\u6790\u96c6\u7fa4\u5e76\u7ed9\u51fa\u4f18\u5316\u5efa\u8bae\u3002\u5c31\u50cf\u624b\u673a\u52a9\u624b\u4e00\u6837\uff0c\u667a\u80fd\u63a8\u8350\u4f1a\u5b9a\u671f\u7684\u626b\u63cf\u3001\u5206\u6790\u4f60\u7684\u96c6\u7fa4\u5e76\u7ed9\u51fa\u63a8\u8350\u5efa\u8bae\u3002\u76ee\u524d\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e24\u79cd\u4f18\u5316\u80fd\u529b\uff1a \u8d44\u6e90\u63a8\u8350 : \u901a\u8fc7\u8d44\u6e90\u63a8\u8350\u7684\u7b97\u6cd5\u5206\u6790\u5e94\u7528\u7684\u771f\u5b9e\u7528\u91cf\u63a8\u8350\u66f4\u5408\u9002\u7684\u8d44\u6e90\u914d\u7f6e\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003\u5e76\u91c7\u7eb3\u5b83\u63d0\u5347\u96c6\u7fa4\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002 \u526f\u672c\u6570\u63a8\u8350 : \u901a\u8fc7\u526f\u672c\u6570\u63a8\u8350\u7684\u7b97\u6cd5\u5206\u6790\u5e94\u7528\u7684\u771f\u5b9e\u7528\u91cf\u63a8\u8350\u66f4\u5408\u9002\u7684\u526f\u672c\u548c EHPA \u914d\u7f6e\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003\u5e76\u91c7\u7eb3\u5b83\u63d0\u5347\u96c6\u7fa4\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002 Please see this document to learn more.","title":"Analytics"},{"location":"zh/#qos-ensurance","text":"Kubernetes is capable of starting multiple pods on same node, and as a result, some of the user applications may be impacted when there are resources(e.g. cpu) consumption competition. To mitigate this, Crane allows users defining PrioirtyClass for the pods and QoSEnsurancePolicy, and then detects disruption and ensure the high priority pods not being impacted by resource competition. Avoidance Actions: Disable Schedule : disable scheduling by setting node taint and condition Throttle : throttle the low priority pods by squeezing cgroup settings Evict : evict low priority pods Please see this document to learn more.","title":"QoS Ensurance"},{"location":"zh/#_2","text":"\u539f\u751f\u7684 Kubernetes \u8c03\u5ea6\u5668\u53ea\u80fd\u57fa\u4e8e\u8d44\u6e90\u7684 Request \u8fdb\u884c\u8c03\u5ea6\u4e1a\u52a1\uff0c\u8fd9\u5f88\u5bb9\u6613\u5bfc\u81f4\u96c6\u7fa4\u8d1f\u8f7d\u4e0d\u5747\u7684\u95ee\u9898\u3002\u4e0e\u4e4b\u5bf9\u6bd4\u7684\u662f\uff0c Crane-scheudler \u53ef\u4ee5\u76f4\u63a5\u4ece Prometheus \u83b7\u53d6\u8282\u70b9\u7684\u771f\u5b9e\u8d1f\u8f7d\u60c5\u51b5\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u8c03\u5ea6\u3002 \u66f4\u591a\u8bf7\u53c2\u89c1 \u6587\u6863 \u3002","title":"\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6"},{"location":"zh/#repositories","text":"Crane is composed of the following components: craned - main crane control plane. Predictor - Predicts resources metrics trends based on historical data. AnalyticsController - Analyzes resources and generate related recommendations. RecommendationController - Recommend Pod resource requests and autoscaler. ClusterNodePredictionController - Create Predictor for nodes. EffectiveHPAController - Effective HPA for horizontal scaling. EffectiveVPAController - Effective VPA for vertical scaling. metric-adaptor - Metric server for driving the scaling. crane-agent - Ensure critical workloads SLO based on abnormally detection. gocrane/api - This repository defines component-level APIs for the Crane platform. gocrane/fadvisor - Financial advisor which collect resource prices from cloud API. gocrane/crane-scheduler - \u4e00\u4e2a\u53ef\u4ee5\u57fa\u4e8e\u771f\u5b9e\u8d1f\u8f7d\u5bf9\u4e1a\u52a1\u8fdb\u884c\u8c03\u5ea6\u7684 Kubernestes \u8c03\u5ea6\u5668\u3002","title":"Repositories"},{"location":"zh/CONTRIBUTING/","text":"Contributing to Crane \u00b6 Welcome to Crane! This document is a guideline about how to contribute to Crane. Become a contributor \u00b6 You can contribute to Crane in several ways. Here are some examples: Contribute to the Crane codebase. Report bugs. Suggest enhancements. Write technical documentation and blog posts, for users and contributors. Organize meetups and user groups in your local area. Help others by answering questions about Crane. For more ways to contribute, check out the Open Source Guides . Report bugs \u00b6 Before submitting a new issue, try to make sure someone hasn't already reported the problem. Look through the existing issues for similar issues. Report a bug by submitting a bug report . Make sure that you provide as much information as possible on how to reproduce the bug. Suggest enhancements \u00b6 If you have an idea to improve Crane, submit an feature request .","title":"\u8d21\u732e"},{"location":"zh/CONTRIBUTING/#contributing-to-crane","text":"Welcome to Crane! This document is a guideline about how to contribute to Crane.","title":"Contributing to Crane"},{"location":"zh/CONTRIBUTING/#become-a-contributor","text":"You can contribute to Crane in several ways. Here are some examples: Contribute to the Crane codebase. Report bugs. Suggest enhancements. Write technical documentation and blog posts, for users and contributors. Organize meetups and user groups in your local area. Help others by answering questions about Crane. For more ways to contribute, check out the Open Source Guides .","title":"Become a contributor"},{"location":"zh/CONTRIBUTING/#report-bugs","text":"Before submitting a new issue, try to make sure someone hasn't already reported the problem. Look through the existing issues for similar issues. Report a bug by submitting a bug report . Make sure that you provide as much information as possible on how to reproduce the bug.","title":"Report bugs"},{"location":"zh/CONTRIBUTING/#suggest-enhancements","text":"If you have an idea to improve Crane, submit an feature request .","title":"Suggest enhancements"},{"location":"zh/code-standards/","text":"Code standards \u00b6 This doc describes the code standards and suggestion for crane project, mainly for new contributor of the project import need to be organized \u00b6 import should be categorized with blank line as system imports, community imports and crane apis and crane imports, like the following example import ( \"reflect\" \"sync\" \"time\" vpa \"k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/util\" \"github.com/gocrane/api/prediction/v1alpha1\" \"github.com/gocrane/crane/pkg/utils\" \"github.com/gocrane/crane/pkg/prediction/config\" ) logs standard \u00b6 logs are required for troubleshooting purpose log message should always start with capital letter log message should be a complete sentence that contains enough context, for example: object key, action, parameters, status, error message by default, you don't need to set log level set 4 for debug level. set 6 for more detail debug level. set 10 for massive data log level. can use klog.KObj() to contain object key to let we know which object the message is printed for klog . Infof ( \"Failed to setup webhook %s\" , \"value\" ) klog . V ( 4 ). Infof ( \"Debug info %s\" , \"value\" ) klog . Errorf ( \"Failed to get scale, ehpa %s error %v\" , klog . KObj ( ehpa ), err ) klog . Error ( error ) klog . ErrorDepth ( 5 , fmt . Errorf ( \"failed to get ehpa %s: %v\" , klog . KObj ( ehpa ), err )) event is needed for critical reconcile loop \u00b6 event is to let user know what happens on serverside, only print info we want user to know consider failure paths and success paths event do not need the object key c . Recorder . Event ( ehpa , v1 . EventTypeNormal , \"FailedGetSubstitute\" , err . Error ()) comment \u00b6 every interface should have comments to clarify comment should be a complete sentence // Interface is a source of monitoring metric that provides metrics that can be used for // prediction, such as 'cpu usage', 'memory footprint', 'request per second (qps)', etc. type Interface interface { // GetTimeSeries returns the metric time series that meet the given // conditions from the specified time range. GetTimeSeries ( metricName string , Conditions [] common . QueryCondition , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // GetLatestTimeSeries returns the latest metric values that meet the given conditions. GetLatestTimeSeries ( metricName string , Conditions [] common . QueryCondition ) ([] * common . TimeSeries , error ) // QueryTimeSeries returns the time series based on a promql like query string. QueryTimeSeries ( queryExpr string , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // QueryLatestTimeSeries returns the latest metric values that meet the given query. QueryLatestTimeSeries ( queryExpr string ) ([] * common . TimeSeries , error ) } functions \u00b6 function name should clarify what do this function do, for example: verb + noun similar functions should be refactored, merge or divide them common functions should move to common folder like utils variable \u00b6 variable name should clarify what do this variable does, better not use too short name and too simple name better to use more meaningful variable name for tmp variable, for example: foo loop folder and file \u00b6 folder name should be letter with lower case and number file name should be letter and number and _ unit test \u00b6 Test-driven developing Complex function that include condition decide should add unit test for it don't forget to run make fmt before you submit code \u00b6","title":"\u4ee3\u7801\u6807\u51c6"},{"location":"zh/code-standards/#code-standards","text":"This doc describes the code standards and suggestion for crane project, mainly for new contributor of the project","title":"Code standards"},{"location":"zh/code-standards/#import-need-to-be-organized","text":"import should be categorized with blank line as system imports, community imports and crane apis and crane imports, like the following example import ( \"reflect\" \"sync\" \"time\" vpa \"k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/util\" \"github.com/gocrane/api/prediction/v1alpha1\" \"github.com/gocrane/crane/pkg/utils\" \"github.com/gocrane/crane/pkg/prediction/config\" )","title":"import need to be organized"},{"location":"zh/code-standards/#logs-standard","text":"logs are required for troubleshooting purpose log message should always start with capital letter log message should be a complete sentence that contains enough context, for example: object key, action, parameters, status, error message by default, you don't need to set log level set 4 for debug level. set 6 for more detail debug level. set 10 for massive data log level. can use klog.KObj() to contain object key to let we know which object the message is printed for klog . Infof ( \"Failed to setup webhook %s\" , \"value\" ) klog . V ( 4 ). Infof ( \"Debug info %s\" , \"value\" ) klog . Errorf ( \"Failed to get scale, ehpa %s error %v\" , klog . KObj ( ehpa ), err ) klog . Error ( error ) klog . ErrorDepth ( 5 , fmt . Errorf ( \"failed to get ehpa %s: %v\" , klog . KObj ( ehpa ), err ))","title":"logs standard"},{"location":"zh/code-standards/#event-is-needed-for-critical-reconcile-loop","text":"event is to let user know what happens on serverside, only print info we want user to know consider failure paths and success paths event do not need the object key c . Recorder . Event ( ehpa , v1 . EventTypeNormal , \"FailedGetSubstitute\" , err . Error ())","title":"event is needed for critical reconcile loop"},{"location":"zh/code-standards/#comment","text":"every interface should have comments to clarify comment should be a complete sentence // Interface is a source of monitoring metric that provides metrics that can be used for // prediction, such as 'cpu usage', 'memory footprint', 'request per second (qps)', etc. type Interface interface { // GetTimeSeries returns the metric time series that meet the given // conditions from the specified time range. GetTimeSeries ( metricName string , Conditions [] common . QueryCondition , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // GetLatestTimeSeries returns the latest metric values that meet the given conditions. GetLatestTimeSeries ( metricName string , Conditions [] common . QueryCondition ) ([] * common . TimeSeries , error ) // QueryTimeSeries returns the time series based on a promql like query string. QueryTimeSeries ( queryExpr string , startTime time . Time , endTime time . Time , step time . Duration ) ([] * common . TimeSeries , error ) // QueryLatestTimeSeries returns the latest metric values that meet the given query. QueryLatestTimeSeries ( queryExpr string ) ([] * common . TimeSeries , error ) }","title":"comment"},{"location":"zh/code-standards/#functions","text":"function name should clarify what do this function do, for example: verb + noun similar functions should be refactored, merge or divide them common functions should move to common folder like utils","title":"functions"},{"location":"zh/code-standards/#variable","text":"variable name should clarify what do this variable does, better not use too short name and too simple name better to use more meaningful variable name for tmp variable, for example: foo loop","title":"variable"},{"location":"zh/code-standards/#folder-and-file","text":"folder name should be letter with lower case and number file name should be letter and number and _","title":"folder and file"},{"location":"zh/code-standards/#unit-test","text":"Test-driven developing Complex function that include condition decide should add unit test for it","title":"unit test"},{"location":"zh/code-standards/#dont-forget-to-run-make-fmt-before-you-submit-code","text":"","title":"don't forget to run make fmt before you submit code"},{"location":"zh/installation/","text":"\u4ea7\u54c1\u90e8\u7f72\u6307\u5357 \u00b6 \u4e3a\u4e86\u8ba9\u60a8\u66f4\u5feb\u7684\u90e8\u7f72 Crane \uff0c\u672c\u6587\u6863\u63d0\u4f9b\u6e05\u6670\u7684\uff1a \u90e8\u7f72\u73af\u5883\u8981\u6c42 \u5177\u4f53\u5b89\u88c5\u6b65\u9aa4 Crane \u5b89\u88c5\u65f6\u95f4\u572810\u5206\u949f\u5de6\u53f3\uff0c\u5177\u4f53\u65f6\u95f4\u4e5f\u4f9d\u8d56\u96c6\u7fa4\u89c4\u6a21\u4ee5\u53ca\u786c\u4ef6\u80fd\u529b\u3002\u76ee\u524d\u5b89\u88c5\u5df2\u7ecf\u975e\u5e38\u6210\u719f\uff0c\u5982\u679c\u60a8\u5b89\u88c5\u4e2d\u9047\u5230\u4efb\u4f55\u95ee\u9898\uff0c\u53ef\u4ee5\u91c7\u53d6\u5982\u4e0b\u51e0\u79cd\u65b9\u5f0f\uff1a \u8bf7\u9996\u5148\u68c0\u67e5\u540e\u6587\u7684 F&Q \u53ef\u4ee5\u63d0\u51fa\u4e00\u4e2a Issue \uff0c\u6211\u4eec\u4f1a\u8ba4\u771f\u5bf9\u5f85\u6bcf\u4e00\u4e2a Issue \u90e8\u7f72\u73af\u5883\u8981\u6c42 \u00b6 Kubernetes 1.18+ Helm 3.1.0 \u5b89\u88c5\u6d41\u7a0b \u00b6 \u5b89\u88c5 Helm \u00b6 \u5efa\u8bae\u53c2\u8003 Helm \u5b98\u7f51 \u5b89\u88c5\u6587\u6863 \u3002 \u5b89\u88c5 Prometheus \u548c Grafana \u00b6 \u4f7f\u7528 Helm \u5b89\u88c5 Prometheus \u548c Grafana\u3002 \u6ce8\u610f \u5982\u679c\u60a8\u5df2\u7ecf\u5728\u73af\u5883\u4e2d\u90e8\u7f72\u4e86 Prometheus \u548c Grafana\uff0c\u53ef\u4ee5\u8df3\u8fc7\u8be5\u6b65\u9aa4\u3002 \u7f51\u7edc\u95ee\u9898 \u5982\u679c\u4f60\u7684\u7f51\u7edc\u65e0\u6cd5\u8bbf\u95eeGitHub\u8d44\u6e90(GitHub Release, GitHub Raw Content raw.githubusercontent.com )\u3002 \u90a3\u4e48\u4f60\u53ef\u4ee5\u5c1d\u8bd5\u955c\u50cf\u4ed3\u5e93\u3002\u4f46\u955c\u50cf\u4ed3\u5e93\u5177\u6709\u4e00\u5b9a\u7684 \u65f6\u5ef6 \u3002 \u955c\u50cf\u4ed3\u5e93 Crane \u4f7f\u7528 Prometheus \u6293\u53d6\u96c6\u7fa4\u5de5\u4f5c\u8d1f\u8f7d\u5bf9\u8d44\u6e90\u7684\u4f7f\u7528\u60c5\u51b5\u3002\u5b89\u88c5 Prometheus\uff1a Main Mirror helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/prometheus/override_values.yaml \\ --create-namespace prometheus-community/prometheus helm repo add prometheus-community https://finops-helm.pkg.coding.net/gocrane/prometheus-community helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/prometheus/override_values.yaml?download = false \\ --create-namespace prometheus-community/prometheus Crane \u7684 Fadvisor \u4f7f\u7528 Grafana \u5c55\u793a\u6210\u672c\u9884\u4f30\u3002\u5b89\u88c5 Grafana\uff1a Main Mirror helm repo add grafana https://grafana.github.io/helm-charts helm install grafana \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/grafana/override_values.yaml \\ -n crane-system \\ --create-namespace grafana/grafana helm repo add grafana https://finops-helm.pkg.coding.net/gocrane/grafana helm install grafana \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false \\ -n crane-system \\ --create-namespace grafana/grafana \u5b89\u88c5 Crane \u548c Fadvisor \u00b6 Main Mirror helm repo add crane https://gocrane.github.io/helm-charts helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor helm repo add crane https://finops-helm.pkg.coding.net/gocrane/gocrane helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor \u5b89\u88c5 Crane-scheduler\uff08\u53ef\u9009\uff09 \u00b6 helm install scheduler -n crane-system --create-namespace crane/scheduler \u9a8c\u8bc1\u5b89\u88c5\u662f\u5426\u6210\u529f \u00b6 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u68c0\u67e5\u5b89\u88c5\u7684 Deployment \u662f\u5426\u6b63\u5e38\uff1a kubectl get deploy -n crane-system \u7ed3\u679c\u7c7b\u4f3c\u5982\u4e0b\uff1a NAME READY UP-TO-DATE AVAILABLE AGE craned 1 /1 1 1 31m fadvisor 1 /1 1 1 41m grafana 1 /1 1 1 42m metric-adapter 1 /1 1 1 31m prometheus-kube-state-metrics 1 /1 1 1 43m prometheus-server 1 /1 1 1 43m \u53ef\u4ee5\u67e5\u770b\u672c\u7bc7 \u6587\u6863 \u83b7\u53d6\u66f4\u591a\u6709\u5173 Crane Helm Chart \u7684\u4fe1\u606f\u3002 \u6210\u672c\u5c55\u793a \u00b6 \u6253\u5f00 Crane \u63a7\u5236\u53f0 \u00b6 \u6ce8\u610f\uff1aCrane \u7684\u63a7\u5236\u53f0\u5730\u5740\u5c31\u662f Crane \u7684 URL \u5730\u5740\uff0c\u53ef\u4ee5\u5c06\u5176\u6dfb\u52a0\u5230\u7edf\u4e00\u7684\u63a7\u5236\u53f0\u67e5\u770b\u591a\u4e2a\u90e8\u7f72 Crane \u7684\u96c6\u7fa4\u7684\u4fe1\u606f\u3002 \u5229\u7528 Port forwarding \u547d\u4ee4\uff0c\u53ef\u4ee5\u5728\u672c\u5730\u8ba1\u7b97\u673a\u7684\u6d4f\u89c8\u5668\u6253\u5f00 Crane \u63a7\u5236\u53f0\uff1a kubectl port-forward -n crane-system svc/craned 9090 \u6267\u884c\u4e0a\u8ff0\u547d\u4ee4\u540e\uff0c\u4e0d\u8981\u5173\u95ed\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u5728\u672c\u5730\u8ba1\u7b97\u673a\u7684\u6d4f\u89c8\u5668\u5730\u5740\u91cc\u8f93\u5165 localhost:9090 \u5373\u53ef\u6253\u5f00 Crane \u7684\u63a7\u5236\u53f0\uff1a \u6dfb\u52a0\u5b89\u88c5\u4e86 Crane \u7684\u96c6\u7fa4 \u00b6 \u60a8\u53ef\u4ee5\u70b9\u51fb\u4e0a\u56fe\u4e2d\u7684\u201c\u6dfb\u52a0\u96c6\u7fa4\u201d\u7684\u84dd\u8272\u6309\u94ae\uff0c\u5c06 Crane \u63a7\u5236\u53f0\u7684\u5730\u5740 http://localhost:9090 \u4f5c\u4e3a Crane \u7684 URL\uff0c\u4f5c\u4e3a\u7b2c\u4e00\u4e2a\u96c6\u7fa4\u6dfb\u52a0\u5230 Crane \u63a7\u5236\u53f0\u3002 \u82e5\u60a8\u60f3\u6dfb\u52a0\u5176\u5b83\u96c6\u7fa4\uff0c\u5b9e\u73b0\u591a\u96c6\u7fa4\u7684\u8d44\u6e90\u4f7f\u7528\u548c\u6210\u672c\u5206\u6790\u3002\u53ef\u4ee5\u5728\u522b\u7684\u96c6\u7fa4\u4e2d\u4e5f\u5b89\u88c5\u5b8c Crane \u4e4b\u540e\uff0c\u5c06 Crane \u7684 URL \u6dfb\u52a0\u8fdb\u6765\u3002 \u81ea\u5b9a\u4e49\u5b89\u88c5 \u00b6 \u901a\u8fc7 YAML \u5b89\u88c5 Crane \u3002 Main Mirror git clone https://github.com/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter git clone https://e.coding.net/finops/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter \u5982\u679c\u60a8\u60f3\u81ea\u5b9a\u4e49 Crane \u91cc\u914d\u7f6e Prometheus \u7684 HTTP \u5730\u5740\uff0c\u8bf7\u53c2\u8003\u4ee5\u4e0b\u7684\u547d\u4ee4\u3002\u5982\u679c\u60a8\u5728\u96c6\u7fa4\u91cc\u5df2\u5b58\u5728\u4e00\u4e2a Prometheus\uff0c\u8bf7\u5c06 Server \u5730\u5740\u586b\u4e8e CUSTOMIZE_PROMETHEUS \u3002 export CUSTOMIZE_PROMETHEUS= if [ $CUSTOMIZE_PROMETHEUS ]; then sed -i '' \"s/http:\\/\\/prometheus-server.crane-system.svc.cluster.local:8080/${CUSTOMIZE_PROMETHEUS}/\" deploy/craned/deployment.yaml ; fi \u5b89\u88c5\u5e38\u89c1\u95ee\u9898 \u00b6 \u5b89\u88c5 Crane \u62a5\u9519 \u00b6 \u5f53\u60a8\u6267\u884c helm install crane -n crane-system --create-namespace crane/crane \u547d\u4ee4\u65f6\uff0c\u53ef\u80fd\u4f1a\u9047\u5230\u5982\u4e0b\u9519\u8bef\uff1a Error: rendered manifests contain a resource that already exists. Unable to continue with install: APIService \"v1beta1.custom.metrics.k8s.io\" in namespace \"\" exists and cannot be imported into the current release: invalid ownership metadata ; label validation error: missing key \"app.kubernetes.io/managed-by\" : must be set to \"Helm\" ; annotation validation error: missing key \"meta.helm.sh/release-name\" : must be set to \"crane\" ; annotation validation error: missing key \"meta.helm.sh/release-namespace\" : must be set to \"crane-system\" \u539f\u56e0\uff1a\u96c6\u7fa4\u5b89\u88c5\u8fc7 custom metric \u7684 APIService\uff0c\u6240\u4ee5\u62a5\u9519\u3002\u53ef\u4ee5\u628a\u4e4b\u524d\u7684\u5220\u9664\u518d\u91cd\u65b0\u6267\u884c\u5b89\u88c5 Crane \u7684\u547d\u4ee4\uff0c\u5220\u9664\u65b9\u5f0f\uff1a kubectl delete apiservice v1beta1.custom.metrics.k8s.io \u3002 \u83b7\u53d6 Crane URL \u7684\u5176\u5b83\u65b9\u5f0f \u00b6 NodePort \u65b9\u5f0f \u00b6 \u60a8\u53ef\u4ee5\u5c06 Crane \u7684 Service \u7684\u7c7b\u578b\u6362\u6210 NodePort \u7c7b\u578b\uff0c\u8fd9\u6837\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7\u96c6\u7fa4\u4efb\u610f\u8282\u70b9 IP + \u8be5\u670d\u52a1\u91ccdashboard- service \u7aef\u53e3\u53f7\u7684\u65b9\u5f0f\uff0c\u6253\u5f00\u63a7\u5236\u53f0\u3002 \u5177\u4f53\u64cd\u4f5c\uff1a\u4fee\u6539 crane-system \u547d\u540d\u7a7a\u95f4\u4e0b\u540d\u4e3a craned \u7684 Service\uff0c\u5c06\u5176\u8bbf\u95ee\u65b9\u5f0f\u8be5\u4e3a NodePort \u7684\u65b9\u5f0f\uff0c\u7136\u540e\u83b7\u53d6\u67d0\u4e00\u96c6\u7fa4\u7684\u8282\u70b9 IP\uff0c\u4ee5\u53ca\u76f8\u5e94\u7684\u7aef\u53e3\u53f7\uff0c\u7aef\u53e3\u53f7\u5982\u4e0b\u6240\u793a\uff1a \u6ce8\u610f\uff1a\u82e5\u60a8\u7684\u96c6\u7fa4\u8282\u70b9\u53ea\u6709\u5185\u7f51 IP\uff0c\u5219\u8bbf\u95ee\u8be5 IP \u7684\u8ba1\u7b97\u673a\u9700\u8981\u5728\u540c\u4e00\u5185\u7f51\u3002\u82e5\u96c6\u7fa4\u8282\u70b9\u62e5\u6709\u5916\u7f51 IP\uff0c\u5219\u6ca1\u6709\u76f8\u5173\u95ee\u9898\u3002 LoadBalance \u65b9\u5f0f \u00b6 \u82e5\u60a8\u4f7f\u7528\u7684\u662f\u516c\u6709\u4e91\u5382\u5546\u7684\u670d\u52a1\uff0c\u60a8\u53ef\u4ee5\u5c06 Crane \u7684 Service \u7684\u7c7b\u578b\u6362\u6210\u516c\u7f51 LB \u7c7b\u578b\uff0c\u8fd9\u6837\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7 LB IP + 9090 \u7aef\u53e3\u53f7\u7684\u65b9\u5f0f\uff0c\u6253\u5f00\u63a7\u5236\u53f0\u3002 \u5177\u4f53\u64cd\u4f5c\uff1a\u4fee\u6539 crane-system \u547d\u540d\u7a7a\u95f4\u4e0b\u540d\u4e3a craned \u7684 Service\uff0c\u5c06\u5176\u8bbf\u95ee\u65b9\u5f0f\u8be5\u4e3a\u516c\u7f51 LB \u7684\u65b9\u5f0f\u3002","title":"\u5b89\u88c5"},{"location":"zh/installation/#_1","text":"\u4e3a\u4e86\u8ba9\u60a8\u66f4\u5feb\u7684\u90e8\u7f72 Crane \uff0c\u672c\u6587\u6863\u63d0\u4f9b\u6e05\u6670\u7684\uff1a \u90e8\u7f72\u73af\u5883\u8981\u6c42 \u5177\u4f53\u5b89\u88c5\u6b65\u9aa4 Crane \u5b89\u88c5\u65f6\u95f4\u572810\u5206\u949f\u5de6\u53f3\uff0c\u5177\u4f53\u65f6\u95f4\u4e5f\u4f9d\u8d56\u96c6\u7fa4\u89c4\u6a21\u4ee5\u53ca\u786c\u4ef6\u80fd\u529b\u3002\u76ee\u524d\u5b89\u88c5\u5df2\u7ecf\u975e\u5e38\u6210\u719f\uff0c\u5982\u679c\u60a8\u5b89\u88c5\u4e2d\u9047\u5230\u4efb\u4f55\u95ee\u9898\uff0c\u53ef\u4ee5\u91c7\u53d6\u5982\u4e0b\u51e0\u79cd\u65b9\u5f0f\uff1a \u8bf7\u9996\u5148\u68c0\u67e5\u540e\u6587\u7684 F&Q \u53ef\u4ee5\u63d0\u51fa\u4e00\u4e2a Issue \uff0c\u6211\u4eec\u4f1a\u8ba4\u771f\u5bf9\u5f85\u6bcf\u4e00\u4e2a Issue","title":"\u4ea7\u54c1\u90e8\u7f72\u6307\u5357"},{"location":"zh/installation/#_2","text":"Kubernetes 1.18+ Helm 3.1.0","title":"\u90e8\u7f72\u73af\u5883\u8981\u6c42"},{"location":"zh/installation/#_3","text":"","title":"\u5b89\u88c5\u6d41\u7a0b"},{"location":"zh/installation/#helm","text":"\u5efa\u8bae\u53c2\u8003 Helm \u5b98\u7f51 \u5b89\u88c5\u6587\u6863 \u3002","title":"\u5b89\u88c5 Helm"},{"location":"zh/installation/#prometheus-grafana","text":"\u4f7f\u7528 Helm \u5b89\u88c5 Prometheus \u548c Grafana\u3002 \u6ce8\u610f \u5982\u679c\u60a8\u5df2\u7ecf\u5728\u73af\u5883\u4e2d\u90e8\u7f72\u4e86 Prometheus \u548c Grafana\uff0c\u53ef\u4ee5\u8df3\u8fc7\u8be5\u6b65\u9aa4\u3002 \u7f51\u7edc\u95ee\u9898 \u5982\u679c\u4f60\u7684\u7f51\u7edc\u65e0\u6cd5\u8bbf\u95eeGitHub\u8d44\u6e90(GitHub Release, GitHub Raw Content raw.githubusercontent.com )\u3002 \u90a3\u4e48\u4f60\u53ef\u4ee5\u5c1d\u8bd5\u955c\u50cf\u4ed3\u5e93\u3002\u4f46\u955c\u50cf\u4ed3\u5e93\u5177\u6709\u4e00\u5b9a\u7684 \u65f6\u5ef6 \u3002 \u955c\u50cf\u4ed3\u5e93 Crane \u4f7f\u7528 Prometheus \u6293\u53d6\u96c6\u7fa4\u5de5\u4f5c\u8d1f\u8f7d\u5bf9\u8d44\u6e90\u7684\u4f7f\u7528\u60c5\u51b5\u3002\u5b89\u88c5 Prometheus\uff1a Main Mirror helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/prometheus/override_values.yaml \\ --create-namespace prometheus-community/prometheus helm repo add prometheus-community https://finops-helm.pkg.coding.net/gocrane/prometheus-community helm install prometheus -n crane-system \\ --set pushgateway.enabled = false \\ --set alertmanager.enabled = false \\ --set server.persistentVolume.enabled = false \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/prometheus/override_values.yaml?download = false \\ --create-namespace prometheus-community/prometheus Crane \u7684 Fadvisor \u4f7f\u7528 Grafana \u5c55\u793a\u6210\u672c\u9884\u4f30\u3002\u5b89\u88c5 Grafana\uff1a Main Mirror helm repo add grafana https://grafana.github.io/helm-charts helm install grafana \\ -f https://raw.githubusercontent.com/gocrane/helm-charts/main/integration/grafana/override_values.yaml \\ -n crane-system \\ --create-namespace grafana/grafana helm repo add grafana https://finops-helm.pkg.coding.net/gocrane/grafana helm install grafana \\ -f https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false \\ -n crane-system \\ --create-namespace grafana/grafana","title":"\u5b89\u88c5 Prometheus \u548c Grafana"},{"location":"zh/installation/#crane-fadvisor","text":"Main Mirror helm repo add crane https://gocrane.github.io/helm-charts helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor helm repo add crane https://finops-helm.pkg.coding.net/gocrane/gocrane helm install crane -n crane-system --create-namespace crane/crane helm install fadvisor -n crane-system --create-namespace crane/fadvisor","title":"\u5b89\u88c5 Crane \u548c Fadvisor"},{"location":"zh/installation/#crane-scheduler","text":"helm install scheduler -n crane-system --create-namespace crane/scheduler","title":"\u5b89\u88c5 Crane-scheduler\uff08\u53ef\u9009\uff09"},{"location":"zh/installation/#_4","text":"\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u68c0\u67e5\u5b89\u88c5\u7684 Deployment \u662f\u5426\u6b63\u5e38\uff1a kubectl get deploy -n crane-system \u7ed3\u679c\u7c7b\u4f3c\u5982\u4e0b\uff1a NAME READY UP-TO-DATE AVAILABLE AGE craned 1 /1 1 1 31m fadvisor 1 /1 1 1 41m grafana 1 /1 1 1 42m metric-adapter 1 /1 1 1 31m prometheus-kube-state-metrics 1 /1 1 1 43m prometheus-server 1 /1 1 1 43m \u53ef\u4ee5\u67e5\u770b\u672c\u7bc7 \u6587\u6863 \u83b7\u53d6\u66f4\u591a\u6709\u5173 Crane Helm Chart \u7684\u4fe1\u606f\u3002","title":"\u9a8c\u8bc1\u5b89\u88c5\u662f\u5426\u6210\u529f"},{"location":"zh/installation/#_5","text":"","title":"\u6210\u672c\u5c55\u793a"},{"location":"zh/installation/#crane","text":"\u6ce8\u610f\uff1aCrane \u7684\u63a7\u5236\u53f0\u5730\u5740\u5c31\u662f Crane \u7684 URL \u5730\u5740\uff0c\u53ef\u4ee5\u5c06\u5176\u6dfb\u52a0\u5230\u7edf\u4e00\u7684\u63a7\u5236\u53f0\u67e5\u770b\u591a\u4e2a\u90e8\u7f72 Crane \u7684\u96c6\u7fa4\u7684\u4fe1\u606f\u3002 \u5229\u7528 Port forwarding \u547d\u4ee4\uff0c\u53ef\u4ee5\u5728\u672c\u5730\u8ba1\u7b97\u673a\u7684\u6d4f\u89c8\u5668\u6253\u5f00 Crane \u63a7\u5236\u53f0\uff1a kubectl port-forward -n crane-system svc/craned 9090 \u6267\u884c\u4e0a\u8ff0\u547d\u4ee4\u540e\uff0c\u4e0d\u8981\u5173\u95ed\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u5728\u672c\u5730\u8ba1\u7b97\u673a\u7684\u6d4f\u89c8\u5668\u5730\u5740\u91cc\u8f93\u5165 localhost:9090 \u5373\u53ef\u6253\u5f00 Crane \u7684\u63a7\u5236\u53f0\uff1a","title":"\u6253\u5f00 Crane \u63a7\u5236\u53f0"},{"location":"zh/installation/#crane_1","text":"\u60a8\u53ef\u4ee5\u70b9\u51fb\u4e0a\u56fe\u4e2d\u7684\u201c\u6dfb\u52a0\u96c6\u7fa4\u201d\u7684\u84dd\u8272\u6309\u94ae\uff0c\u5c06 Crane \u63a7\u5236\u53f0\u7684\u5730\u5740 http://localhost:9090 \u4f5c\u4e3a Crane \u7684 URL\uff0c\u4f5c\u4e3a\u7b2c\u4e00\u4e2a\u96c6\u7fa4\u6dfb\u52a0\u5230 Crane \u63a7\u5236\u53f0\u3002 \u82e5\u60a8\u60f3\u6dfb\u52a0\u5176\u5b83\u96c6\u7fa4\uff0c\u5b9e\u73b0\u591a\u96c6\u7fa4\u7684\u8d44\u6e90\u4f7f\u7528\u548c\u6210\u672c\u5206\u6790\u3002\u53ef\u4ee5\u5728\u522b\u7684\u96c6\u7fa4\u4e2d\u4e5f\u5b89\u88c5\u5b8c Crane \u4e4b\u540e\uff0c\u5c06 Crane \u7684 URL \u6dfb\u52a0\u8fdb\u6765\u3002","title":"\u6dfb\u52a0\u5b89\u88c5\u4e86 Crane \u7684\u96c6\u7fa4"},{"location":"zh/installation/#_6","text":"\u901a\u8fc7 YAML \u5b89\u88c5 Crane \u3002 Main Mirror git clone https://github.com/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter git clone https://e.coding.net/finops/gocrane/crane.git CRANE_LATEST_VERSION = $( curl -s https://api.github.com/repos/gocrane/crane/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")' ) git checkout $CRANE_LATEST_VERSION kubectl apply -f deploy/manifests kubectl apply -f deploy/craned kubectl apply -f deploy/metric-adapter \u5982\u679c\u60a8\u60f3\u81ea\u5b9a\u4e49 Crane \u91cc\u914d\u7f6e Prometheus \u7684 HTTP \u5730\u5740\uff0c\u8bf7\u53c2\u8003\u4ee5\u4e0b\u7684\u547d\u4ee4\u3002\u5982\u679c\u60a8\u5728\u96c6\u7fa4\u91cc\u5df2\u5b58\u5728\u4e00\u4e2a Prometheus\uff0c\u8bf7\u5c06 Server \u5730\u5740\u586b\u4e8e CUSTOMIZE_PROMETHEUS \u3002 export CUSTOMIZE_PROMETHEUS= if [ $CUSTOMIZE_PROMETHEUS ]; then sed -i '' \"s/http:\\/\\/prometheus-server.crane-system.svc.cluster.local:8080/${CUSTOMIZE_PROMETHEUS}/\" deploy/craned/deployment.yaml ; fi","title":"\u81ea\u5b9a\u4e49\u5b89\u88c5"},{"location":"zh/installation/#_7","text":"","title":"\u5b89\u88c5\u5e38\u89c1\u95ee\u9898"},{"location":"zh/installation/#crane_2","text":"\u5f53\u60a8\u6267\u884c helm install crane -n crane-system --create-namespace crane/crane \u547d\u4ee4\u65f6\uff0c\u53ef\u80fd\u4f1a\u9047\u5230\u5982\u4e0b\u9519\u8bef\uff1a Error: rendered manifests contain a resource that already exists. Unable to continue with install: APIService \"v1beta1.custom.metrics.k8s.io\" in namespace \"\" exists and cannot be imported into the current release: invalid ownership metadata ; label validation error: missing key \"app.kubernetes.io/managed-by\" : must be set to \"Helm\" ; annotation validation error: missing key \"meta.helm.sh/release-name\" : must be set to \"crane\" ; annotation validation error: missing key \"meta.helm.sh/release-namespace\" : must be set to \"crane-system\" \u539f\u56e0\uff1a\u96c6\u7fa4\u5b89\u88c5\u8fc7 custom metric \u7684 APIService\uff0c\u6240\u4ee5\u62a5\u9519\u3002\u53ef\u4ee5\u628a\u4e4b\u524d\u7684\u5220\u9664\u518d\u91cd\u65b0\u6267\u884c\u5b89\u88c5 Crane \u7684\u547d\u4ee4\uff0c\u5220\u9664\u65b9\u5f0f\uff1a kubectl delete apiservice v1beta1.custom.metrics.k8s.io \u3002","title":"\u5b89\u88c5 Crane \u62a5\u9519"},{"location":"zh/installation/#crane-url","text":"","title":"\u83b7\u53d6 Crane URL \u7684\u5176\u5b83\u65b9\u5f0f"},{"location":"zh/installation/#nodeport","text":"\u60a8\u53ef\u4ee5\u5c06 Crane \u7684 Service \u7684\u7c7b\u578b\u6362\u6210 NodePort \u7c7b\u578b\uff0c\u8fd9\u6837\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7\u96c6\u7fa4\u4efb\u610f\u8282\u70b9 IP + \u8be5\u670d\u52a1\u91ccdashboard- service \u7aef\u53e3\u53f7\u7684\u65b9\u5f0f\uff0c\u6253\u5f00\u63a7\u5236\u53f0\u3002 \u5177\u4f53\u64cd\u4f5c\uff1a\u4fee\u6539 crane-system \u547d\u540d\u7a7a\u95f4\u4e0b\u540d\u4e3a craned \u7684 Service\uff0c\u5c06\u5176\u8bbf\u95ee\u65b9\u5f0f\u8be5\u4e3a NodePort \u7684\u65b9\u5f0f\uff0c\u7136\u540e\u83b7\u53d6\u67d0\u4e00\u96c6\u7fa4\u7684\u8282\u70b9 IP\uff0c\u4ee5\u53ca\u76f8\u5e94\u7684\u7aef\u53e3\u53f7\uff0c\u7aef\u53e3\u53f7\u5982\u4e0b\u6240\u793a\uff1a \u6ce8\u610f\uff1a\u82e5\u60a8\u7684\u96c6\u7fa4\u8282\u70b9\u53ea\u6709\u5185\u7f51 IP\uff0c\u5219\u8bbf\u95ee\u8be5 IP \u7684\u8ba1\u7b97\u673a\u9700\u8981\u5728\u540c\u4e00\u5185\u7f51\u3002\u82e5\u96c6\u7fa4\u8282\u70b9\u62e5\u6709\u5916\u7f51 IP\uff0c\u5219\u6ca1\u6709\u76f8\u5173\u95ee\u9898\u3002","title":"NodePort \u65b9\u5f0f"},{"location":"zh/installation/#loadbalance","text":"\u82e5\u60a8\u4f7f\u7528\u7684\u662f\u516c\u6709\u4e91\u5382\u5546\u7684\u670d\u52a1\uff0c\u60a8\u53ef\u4ee5\u5c06 Crane \u7684 Service \u7684\u7c7b\u578b\u6362\u6210\u516c\u7f51 LB \u7c7b\u578b\uff0c\u8fd9\u6837\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7 LB IP + 9090 \u7aef\u53e3\u53f7\u7684\u65b9\u5f0f\uff0c\u6253\u5f00\u63a7\u5236\u53f0\u3002 \u5177\u4f53\u64cd\u4f5c\uff1a\u4fee\u6539 crane-system \u547d\u540d\u7a7a\u95f4\u4e0b\u540d\u4e3a craned \u7684 Service\uff0c\u5c06\u5176\u8bbf\u95ee\u65b9\u5f0f\u8be5\u4e3a\u516c\u7f51 LB \u7684\u65b9\u5f0f\u3002","title":"LoadBalance \u65b9\u5f0f"},{"location":"zh/mirror/","text":"\u955c\u50cf\u4ed3\u5e93 \u00b6 \u5173\u4e8e\u955c\u50cf\u4ed3\u5e93 \u00b6 \u56e0\u4e3a\u5404\u79cd\u7f51\u7edc\u95ee\u9898\uff0c\u5bfc\u81f4\u90e8\u5206\u5730\u57df\u96be\u4ee5\u8bbf\u95eeGitHub \u8d44\u6e90\uff0c\u5982GitHub Repo, GitHub Release, GitHub Raw Content raw.githubusercontent.com \u3002 \u4e3a\u4e86\u66f4\u597d\u7684\u4f7f\u7528\u4f53\u9a8c\uff0cGoCrane \u4e3a\u60a8\u989d\u5916\u63d0\u4f9b\u4e86\u591a\u4e2a\u955c\u50cf\u4ed3\u5e93\uff0c\u4f46\u5177\u6709\u4e00\u5b9a\u7684\u65f6\u5ef6\u3002 Helm Resources \u00b6 Tips \u6bcf\u516d\u5c0f\u65f6\u540c\u6b65\u4e00\u6b21\u4e0a\u6e38\u7684\u6700\u65b0\u7248\u672c Origin Mirror Type Public https://gocrane.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/gocrane Helm Public https://prometheus-community.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/prometheus-community Helm Public https://grafana.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/grafana Helm Public Git Resources \u00b6 Tips \u6bcf\u5929\u540c\u6b65\u4e00\u6b21\u4e0a\u6e38\u4ed3\u5e93 Origin Mirror Type Public https://github.com/gocrane/crane.git https://e.coding.net/finops/gocrane/crane.git Git Public https://github.com/gocrane/helm-charts.git https://e.coding.net/finops/gocrane/helm-charts.git Git Public https://github.com/gocrane/api.git https://e.coding.net/finops/gocrane/api.git Git Public https://github.com/gocrane/crane-scheduler.git https://e.coding.net/finops/gocrane/crane-scheduler.git Git Public https://github.com/gocrane/fadvisor.git https://e.coding.net/finops/gocrane/fadvisor.git Git Public \u83b7\u53d6 Coding Git \u4ed3\u5e93\u6e90\u6587\u4ef6\u5185\u5bb9 \u00b6 \u5728\u8fd9\u91cc\u5c06\u4e3a\u60a8\u4ecb\u7ecd\uff0c\u5982\u4f55\u901a\u8fc7HTTP\u8bf7\u6c42\u76f4\u63a5\u83b7\u53d6 Coding Git \u4ed3\u5e93\u4e2d\u7684\u6e90\u6587\u4ef6\u5185\u5bb9\u3002 Coding Git \u4ed3\u5e93\u7684\u5173\u952e\u53c2\u6570 \u00b6 \u4e0e\u5e38\u89c4\u7684API\u8bf7\u6c42\u7c7b\u4f3c\uff0cCoding Git\u4ed3\u5e93\u63d0\u4f9b\u4e86\u5bf9\u5e94\u7684API\u63a5\u53e3\u3002 \u4e0b\u9762\u4e3a\u60a8\u4ecb\u7ecd\u76f8\u5173\u7684\u53c2\u6570\u3002 Example \u4ee5 https:// finops .coding.net/public/ gocrane / helm-charts /git/files /main/integration/grafana/override_values.yaml \u4f5c\u4e3a\u4f8b\u5b50\u3002 \u70b9\u51fb\u8bbf\u95ee \u53c2\u6570 \u8bf4\u660e \u4f8b\u5b50 team \u56e2\u961f\u540d\u79f0 finops project \u9879\u76ee\u540d\u79f0 gocrane repo Git \u4ed3\u5e93\u540d\u79f0 helm-charts branch \u5206\u652f\u540d\u79f0 main file path \u9879\u76ee\u4e2d\u7684\u6587\u4ef6\u8def\u5f84 /integration/grafana/override_values.yaml \u6784\u9020HTTP\u8bf7\u6c42 \u00b6 \u6839\u636e\u4e0a\u9762\u6240\u63d0\u5230\u7684\u5c5e\u6027\uff0c\u6309\u7167\u4e0b\u9762\u7684URL\u6784\u9020\u89c4\u5219\u4f9d\u6b21\u586b\u5165\uff0c\u5373\u53ef\u83b7\u5f97\u4e00\u4e2a\u53ef\u4ee5\u76f4\u63a5\u83b7\u53d6\u6e90\u6587\u4ef6\u5185\u5bb9\u7684URL\u3002 https://<team>.coding.net/p/<project>/d/<repo>/git/raw/<branch>/<file path>?download = false https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false Tips \u5c1d\u8bd5\u4ee5\u4e0b\u7684\u547d\u4ee4 curl https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false","title":"\u955c\u50cf\u4ed3\u5e93"},{"location":"zh/mirror/#_1","text":"","title":"\u955c\u50cf\u4ed3\u5e93"},{"location":"zh/mirror/#_2","text":"\u56e0\u4e3a\u5404\u79cd\u7f51\u7edc\u95ee\u9898\uff0c\u5bfc\u81f4\u90e8\u5206\u5730\u57df\u96be\u4ee5\u8bbf\u95eeGitHub \u8d44\u6e90\uff0c\u5982GitHub Repo, GitHub Release, GitHub Raw Content raw.githubusercontent.com \u3002 \u4e3a\u4e86\u66f4\u597d\u7684\u4f7f\u7528\u4f53\u9a8c\uff0cGoCrane \u4e3a\u60a8\u989d\u5916\u63d0\u4f9b\u4e86\u591a\u4e2a\u955c\u50cf\u4ed3\u5e93\uff0c\u4f46\u5177\u6709\u4e00\u5b9a\u7684\u65f6\u5ef6\u3002","title":"\u5173\u4e8e\u955c\u50cf\u4ed3\u5e93"},{"location":"zh/mirror/#helm-resources","text":"Tips \u6bcf\u516d\u5c0f\u65f6\u540c\u6b65\u4e00\u6b21\u4e0a\u6e38\u7684\u6700\u65b0\u7248\u672c Origin Mirror Type Public https://gocrane.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/gocrane Helm Public https://prometheus-community.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/prometheus-community Helm Public https://grafana.github.io/helm-charts https://finops-helm.pkg.coding.net/gocrane/grafana Helm Public","title":"Helm Resources"},{"location":"zh/mirror/#git-resources","text":"Tips \u6bcf\u5929\u540c\u6b65\u4e00\u6b21\u4e0a\u6e38\u4ed3\u5e93 Origin Mirror Type Public https://github.com/gocrane/crane.git https://e.coding.net/finops/gocrane/crane.git Git Public https://github.com/gocrane/helm-charts.git https://e.coding.net/finops/gocrane/helm-charts.git Git Public https://github.com/gocrane/api.git https://e.coding.net/finops/gocrane/api.git Git Public https://github.com/gocrane/crane-scheduler.git https://e.coding.net/finops/gocrane/crane-scheduler.git Git Public https://github.com/gocrane/fadvisor.git https://e.coding.net/finops/gocrane/fadvisor.git Git Public","title":"Git Resources"},{"location":"zh/mirror/#coding-git","text":"\u5728\u8fd9\u91cc\u5c06\u4e3a\u60a8\u4ecb\u7ecd\uff0c\u5982\u4f55\u901a\u8fc7HTTP\u8bf7\u6c42\u76f4\u63a5\u83b7\u53d6 Coding Git \u4ed3\u5e93\u4e2d\u7684\u6e90\u6587\u4ef6\u5185\u5bb9\u3002","title":"\u83b7\u53d6 Coding Git \u4ed3\u5e93\u6e90\u6587\u4ef6\u5185\u5bb9"},{"location":"zh/mirror/#coding-git_1","text":"\u4e0e\u5e38\u89c4\u7684API\u8bf7\u6c42\u7c7b\u4f3c\uff0cCoding Git\u4ed3\u5e93\u63d0\u4f9b\u4e86\u5bf9\u5e94\u7684API\u63a5\u53e3\u3002 \u4e0b\u9762\u4e3a\u60a8\u4ecb\u7ecd\u76f8\u5173\u7684\u53c2\u6570\u3002 Example \u4ee5 https:// finops .coding.net/public/ gocrane / helm-charts /git/files /main/integration/grafana/override_values.yaml \u4f5c\u4e3a\u4f8b\u5b50\u3002 \u70b9\u51fb\u8bbf\u95ee \u53c2\u6570 \u8bf4\u660e \u4f8b\u5b50 team \u56e2\u961f\u540d\u79f0 finops project \u9879\u76ee\u540d\u79f0 gocrane repo Git \u4ed3\u5e93\u540d\u79f0 helm-charts branch \u5206\u652f\u540d\u79f0 main file path \u9879\u76ee\u4e2d\u7684\u6587\u4ef6\u8def\u5f84 /integration/grafana/override_values.yaml","title":"Coding Git \u4ed3\u5e93\u7684\u5173\u952e\u53c2\u6570"},{"location":"zh/mirror/#http","text":"\u6839\u636e\u4e0a\u9762\u6240\u63d0\u5230\u7684\u5c5e\u6027\uff0c\u6309\u7167\u4e0b\u9762\u7684URL\u6784\u9020\u89c4\u5219\u4f9d\u6b21\u586b\u5165\uff0c\u5373\u53ef\u83b7\u5f97\u4e00\u4e2a\u53ef\u4ee5\u76f4\u63a5\u83b7\u53d6\u6e90\u6587\u4ef6\u5185\u5bb9\u7684URL\u3002 https://<team>.coding.net/p/<project>/d/<repo>/git/raw/<branch>/<file path>?download = false https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false Tips \u5c1d\u8bd5\u4ee5\u4e0b\u7684\u547d\u4ee4 curl https://finops.coding.net/p/gocrane/d/helm-charts/git/raw/main/integration/grafana/override_values.yaml?download = false","title":"\u6784\u9020HTTP\u8bf7\u6c42"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/","text":"Advanced CPUSet Manager \u00b6 Static CPU manager is supported by kubelet, when a guaranteed Pod is running on a node, kubelet allocate specific cpu cores to the processes exclusively, which generally keeps the cpu utilization of the node low. This proposal provides a new mechanism to manage cpusets, which allows sharing cpu cores with other processes while binds cpuset.It also allows to revise cpuset when pod is running and relaxes restrictions of binding cpus in kubelet. Table of Contents \u00b6 Advanced CPUSet Manager Table of Contents Motivation Goals Non-Goals/Future Work Proposal Relax restrictions of cpuset allocation Add new annotation to describe the requirement of cpuset contorl manger Advanced CPU Manager component User Stories Story 1 Story 2 Risks and Mitigations Motivation \u00b6 Some latency-sensitive applications have lower lantency and cpu usage when running with specific cores, which results in fewer context switchs and higer cache affinity. But kubelet will always exclude assigned cores in shared cores, which may waste resources.Offline and other online pods can running on the cores actually. In our experiment, for the most part, it is barely noticeable for performance of service. Goals \u00b6 Provide a new mechanism to manage cpuset bypass Provide a new cpuset manager method \"shared\" Allow revise cpuset when pod running Relax restrictions of binding cpus Non-Goals/Future Work \u00b6 Solve the conflicts with kubelet static cpuset manager, you need to set kubelet cpuset manager to \"none\" Numa manager will support in future, CCX/CCD manager also be considered Proposal \u00b6 Relax restrictions of cpuset allocation \u00b6 Kubelet allocate cpus for containers should meet the conditions: requests and limits are specified for all the containers and they are equal the container's resource limit for the limit of CPU is an integer greater than or equal to one and equal to request request of CPU. In Crane, only need to meet condition No.2 Add new annotation to describe the requirement of cpuset contorl manger \u00b6 apiVersion : v1 kind : Pod metadata : annotations : qos.gocrane.io/cpu-manager : none/exclusive/share Provide three polices for cpuset manager: - none: containers of this pod shares a set of cpus which not allocated to exclusive containers - exclusive: containers of this pod monopolize the allocated CPUs , other containers not allowed to use. - share: containers of this pod runs in theallocated CPUs , but other containers can also use. Advanced CPU Manager component \u00b6 Crane-agent use podLister informs to sense the creation of pod. Crane-agent allocate cpus when pod is binded, and loop in cycle to addContainer(change cpuset) until the containers are created Update/Delete pod will handle in reconcile state. state.State referenced from kubelet and topology_cpu_assignment copied from kubelet User Stories \u00b6 Users can update pod annotaion to control cpuset policy flexibly Story 1 \u00b6 make pod from none to share without recreating pod Story 2 \u00b6 make pod from exclusive to share, so offline process can use these CPUs Risks and Mitigations \u00b6 kubelet cpu manger policy need to be set to none, otherwise will be conflicted with crane-agent if crane-agent can not allocate CPUs for pods, it will not refuse to start pod as kubelet","title":"Advanced CpuSet Manager"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#advanced-cpuset-manager","text":"Static CPU manager is supported by kubelet, when a guaranteed Pod is running on a node, kubelet allocate specific cpu cores to the processes exclusively, which generally keeps the cpu utilization of the node low. This proposal provides a new mechanism to manage cpusets, which allows sharing cpu cores with other processes while binds cpuset.It also allows to revise cpuset when pod is running and relaxes restrictions of binding cpus in kubelet.","title":"Advanced CPUSet Manager"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#table-of-contents","text":"Advanced CPUSet Manager Table of Contents Motivation Goals Non-Goals/Future Work Proposal Relax restrictions of cpuset allocation Add new annotation to describe the requirement of cpuset contorl manger Advanced CPU Manager component User Stories Story 1 Story 2 Risks and Mitigations","title":"Table of Contents"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#motivation","text":"Some latency-sensitive applications have lower lantency and cpu usage when running with specific cores, which results in fewer context switchs and higer cache affinity. But kubelet will always exclude assigned cores in shared cores, which may waste resources.Offline and other online pods can running on the cores actually. In our experiment, for the most part, it is barely noticeable for performance of service.","title":"Motivation"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#goals","text":"Provide a new mechanism to manage cpuset bypass Provide a new cpuset manager method \"shared\" Allow revise cpuset when pod running Relax restrictions of binding cpus","title":"Goals"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#non-goalsfuture-work","text":"Solve the conflicts with kubelet static cpuset manager, you need to set kubelet cpuset manager to \"none\" Numa manager will support in future, CCX/CCD manager also be considered","title":"Non-Goals/Future Work"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#proposal","text":"","title":"Proposal"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#relax-restrictions-of-cpuset-allocation","text":"Kubelet allocate cpus for containers should meet the conditions: requests and limits are specified for all the containers and they are equal the container's resource limit for the limit of CPU is an integer greater than or equal to one and equal to request request of CPU. In Crane, only need to meet condition No.2","title":"Relax restrictions of cpuset allocation"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#add-new-annotation-to-describe-the-requirement-of-cpuset-contorl-manger","text":"apiVersion : v1 kind : Pod metadata : annotations : qos.gocrane.io/cpu-manager : none/exclusive/share Provide three polices for cpuset manager: - none: containers of this pod shares a set of cpus which not allocated to exclusive containers - exclusive: containers of this pod monopolize the allocated CPUs , other containers not allowed to use. - share: containers of this pod runs in theallocated CPUs , but other containers can also use.","title":"Add new annotation to describe the  requirement of cpuset contorl manger"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#advanced-cpu-manager-component","text":"Crane-agent use podLister informs to sense the creation of pod. Crane-agent allocate cpus when pod is binded, and loop in cycle to addContainer(change cpuset) until the containers are created Update/Delete pod will handle in reconcile state. state.State referenced from kubelet and topology_cpu_assignment copied from kubelet","title":"Advanced CPU Manager component"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#user-stories","text":"Users can update pod annotaion to control cpuset policy flexibly","title":"User Stories"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#story-1","text":"make pod from none to share without recreating pod","title":"Story 1"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#story-2","text":"make pod from exclusive to share, so offline process can use these CPUs","title":"Story 2"},{"location":"zh/proposals/20220228-advanced-cpuset-manger/#risks-and-mitigations","text":"kubelet cpu manger policy need to be set to none, otherwise will be conflicted with crane-agent if crane-agent can not allocate CPUs for pods, it will not refuse to start pod as kubelet","title":"Risks and Mitigations"},{"location":"zh/roadmaps/roadmap-1h-2022/","text":"Crane Roadmap for H1 2022 \u00b6 Please refer the following sections for Crane release plan of H1 2022, new release will be cut on monthly basis. Please let us know if you have urgent needs which are not presented in the plan. 0.1.0 [released] \u00b6 Predictor to support Moving Windows and DSP algorithms Resource Request Recommendation and Effective Horizontal Pod Autoscaler Grafana Dashboard to view resource utilization and cost trends fadvisor to support billing 0.2.0\uff1a[released] \u00b6 Multiple Metric Adaptor support Node QoS Ensurance for CPU Operation Metrics about R3 and EPA applied ratio 0.3.0 [released] \u00b6 UI with cost visibility and usage optimizations. Request Recommendation adapts with Virtual Kubelet Multiple Triggers for EPA Node QoS Ensurance for Mem Prediction with CPU, Memory, and Business Metrics Scalability to support 1K TSP and 1K EPA 0.4.0 [released] \u00b6 UI to support EPA. 0.5.0 [May] \u00b6 Resource and Replicas Recommendation Load-aware Scheduler 0.6.0 [June] \u00b6 Scalability to support 3k TSP and 3k EPA Algorithm and QoS Documentation EHPA grafana dashboard 0.7.0 [July] \u00b6 Support apiservice router for multiple metric adapters Prediction with Business Metrics 0.8.0 [August] \u00b6 Algorithm estimate notebook","title":"1H 2022"},{"location":"zh/roadmaps/roadmap-1h-2022/#crane-roadmap-for-h1-2022","text":"Please refer the following sections for Crane release plan of H1 2022, new release will be cut on monthly basis. Please let us know if you have urgent needs which are not presented in the plan.","title":"Crane Roadmap for H1 2022"},{"location":"zh/roadmaps/roadmap-1h-2022/#010-released","text":"Predictor to support Moving Windows and DSP algorithms Resource Request Recommendation and Effective Horizontal Pod Autoscaler Grafana Dashboard to view resource utilization and cost trends fadvisor to support billing","title":"0.1.0 [released]"},{"location":"zh/roadmaps/roadmap-1h-2022/#020released","text":"Multiple Metric Adaptor support Node QoS Ensurance for CPU Operation Metrics about R3 and EPA applied ratio","title":"0.2.0\uff1a[released]"},{"location":"zh/roadmaps/roadmap-1h-2022/#030-released","text":"UI with cost visibility and usage optimizations. Request Recommendation adapts with Virtual Kubelet Multiple Triggers for EPA Node QoS Ensurance for Mem Prediction with CPU, Memory, and Business Metrics Scalability to support 1K TSP and 1K EPA","title":"0.3.0 [released]"},{"location":"zh/roadmaps/roadmap-1h-2022/#040-released","text":"UI to support EPA.","title":"0.4.0 [released]"},{"location":"zh/roadmaps/roadmap-1h-2022/#050-may","text":"Resource and Replicas Recommendation Load-aware Scheduler","title":"0.5.0 [May]"},{"location":"zh/roadmaps/roadmap-1h-2022/#060-june","text":"Scalability to support 3k TSP and 3k EPA Algorithm and QoS Documentation EHPA grafana dashboard","title":"0.6.0 [June]"},{"location":"zh/roadmaps/roadmap-1h-2022/#070-july","text":"Support apiservice router for multiple metric adapters Prediction with Business Metrics","title":"0.7.0 [July]"},{"location":"zh/roadmaps/roadmap-1h-2022/#080-august","text":"Algorithm estimate notebook","title":"0.8.0 [August]"},{"location":"zh/tutorials/analytics-and-recommendation/","text":"\u667a\u80fd\u63a8\u8350 \u00b6 \u667a\u80fd\u63a8\u8350\u80fd\u591f\u5e2e\u52a9\u7528\u6237\u81ea\u52a8\u5206\u6790\u96c6\u7fa4\u5e76\u7ed9\u51fa\u4f18\u5316\u5efa\u8bae\u3002\u5c31\u50cf\u624b\u673a\u52a9\u624b\u4e00\u6837\uff0c\u667a\u80fd\u63a8\u8350\u4f1a\u5b9a\u671f\u7684\u626b\u63cf\u3001\u5206\u6790\u4f60\u7684\u96c6\u7fa4\u5e76\u7ed9\u51fa\u63a8\u8350\u5efa\u8bae\u3002\u76ee\u524d\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e24\u79cd\u4f18\u5316\u80fd\u529b\uff1a \u8d44\u6e90\u63a8\u8350 : \u901a\u8fc7\u8d44\u6e90\u63a8\u8350\u7684\u7b97\u6cd5\u5206\u6790\u5e94\u7528\u7684\u771f\u5b9e\u7528\u91cf\u63a8\u8350\u66f4\u5408\u9002\u7684\u8d44\u6e90\u914d\u7f6e\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003\u5e76\u91c7\u7eb3\u5b83\u63d0\u5347\u96c6\u7fa4\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002 \u526f\u672c\u6570\u63a8\u8350 : \u901a\u8fc7\u526f\u672c\u6570\u63a8\u8350\u7684\u7b97\u6cd5\u5206\u6790\u5e94\u7528\u7684\u771f\u5b9e\u7528\u91cf\u63a8\u8350\u66f4\u5408\u9002\u7684\u526f\u672c\u548c EHPA \u914d\u7f6e\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003\u5e76\u91c7\u7eb3\u5b83\u63d0\u5347\u96c6\u7fa4\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002 \u5e94\u7528\u53ef\u4ee5\u6839\u636e\u8d44\u6e90\u63a8\u8350\u8c03\u6574 request \u4e5f\u53ef\u4ee5\u6839\u636e\u526f\u672c\u6570\u63a8\u8350\u8c03\u6574\u526f\u672c\u6570\uff0c\u8fd9\u4e24\u79cd\u4f18\u5316\u90fd\u80fd\u5e2e\u52a9\u60a8\u964d\u4f4e\u6210\u672c\uff0c\u60a8\u53ef\u4ee5\u6839\u636e\u60a8\u7684\u9700\u6c42\u9009\u62e9\u91c7\u7528\u76f8\u5e94\u7684\u4f18\u5316\u5efa\u8bae\u3002 \u67b6\u6784 \u00b6 \u4e00\u6b21\u5206\u6790\u7684\u8fc7\u7a0b \u00b6 \u7528\u6237\u521b\u5efa Analytics \u5bf9\u8c61\uff0c\u901a\u8fc7 ResourceSelector \u9009\u62e9\u9700\u8981\u5206\u6790\u7684\u8d44\u6e90\uff0c\u652f\u6301\u9009\u62e9\u591a\u7c7b\u578b\uff08\u57fa\u4e8eGroup,Kind,Version\uff09\u7684\u6279\u91cf\u9009\u62e9 \u5e76\u884c\u5206\u6790\u6bcf\u4e2a\u9009\u62e9\u7684\u8d44\u6e90\uff0c\u5c1d\u8bd5\u8fdb\u884c\u5206\u6790\u63a8\u8350\uff0c\u6bcf\u6b21\u5206\u6790\u8fc7\u7a0b\u5206\u6210\u7b5b\u9009\u548c\u63a8\u8350\u4e24\u4e2a\u9636\u6bb5\uff1a \u7b5b\u9009\uff1a\u6392\u9664\u4e0d\u6ee1\u8db3\u63a8\u8350\u6761\u4ef6\u7684\u8d44\u6e90\u3002\u6bd4\u5982\u5bf9\u4e8e\u5f39\u6027\u63a8\u8350\uff0c\u6392\u9664\u6ca1\u6709 running pod \u7684 workload \u63a8\u8350\uff1a\u901a\u8fc7\u7b97\u6cd5\u8ba1\u7b97\u5206\u6790\uff0c\u7ed9\u51fa\u63a8\u8350\u7ed3\u679c \u5982\u679c\u901a\u8fc7\u7b5b\u9009\uff0c\u521b\u5efa Recommendation \u5bf9\u8c61\uff0c\u5c06\u63a8\u8350\u7ed3\u679c\u5c55\u793a\u5728 Recommendation.Status \u672a\u901a\u8fc7\u7b5b\u9009\u7684\u539f\u56e0\u548c\u72b6\u6001\u5c55\u793a\u5728 Analytics.Status \u6839\u636e\u8fd0\u884c\u95f4\u9694\u7b49\u5f85\u4e0b\u6b21\u5206\u6790 \u540d\u8bcd\u89e3\u91ca \u00b6 \u5206\u6790 \u00b6 \u5206\u6790\u5b9a\u4e49\u4e86\u4e00\u4e2a\u626b\u63cf\u5206\u6790\u4efb\u52a1\u3002\u652f\u6301\u4e24\u79cd\u4efb\u52a1\u7c7b\u578b\uff1a\u8d44\u6e90\u63a8\u8350\u548c\u5f39\u6027\u63a8\u8350\u3002Crane \u5b9a\u671f\u8fd0\u884c\u5206\u6790\u4efb\u52a1\uff0c\u5e76\u4ea7\u751f\u63a8\u8350\u7ed3\u679c\u3002 \u63a8\u8350 \u00b6 \u63a8\u8350\u5c55\u793a\u4e86\u4e00\u4e2a\u4f18\u5316\u63a8\u8350\u7684\u7ed3\u679c\u3002\u63a8\u8350\u7684\u7ed3\u679c\u662f\u4e00\u6bb5 YAML \u914d\u7f6e\uff0c\u6839\u636e\u7ed3\u679c\u7528\u6237\u53ef\u4ee5\u8fdb\u884c\u76f8\u5e94\u7684\u4f18\u5316\u52a8\u4f5c\uff0c\u6bd4\u5982\u8c03\u6574\u5e94\u7528\u7684\u8d44\u6e90\u914d\u7f6e\u3002 \u53c2\u6570\u914d\u7f6e \u00b6 \u4e0d\u540c\u7684\u5206\u6790\u91c7\u7528\u4e0d\u540c\u7684\u8ba1\u7b97\u6a21\u578b\uff0cCrane \u63d0\u4f9b\u4e86\u4e00\u5957\u9ed8\u8ba4\u7684\u8ba1\u7b97\u6a21\u578b\u4ee5\u53ca\u4e00\u5957\u914d\u5957\u7684\u914d\u7f6e\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539\u914d\u7f6e\u6765\u5b9a\u5236\u63a8\u8350\u7684\u6548\u679c\u3002\u652f\u6301\u4fee\u6539\u5168\u5c40\u7684\u9ed8\u8ba4\u914d\u7f6e\u548c\u4fee\u6539\u5355\u4e2a\u5206\u6790\u4efb\u52a1\u7684\u914d\u7f6e\u3002","title":"\u63a8\u8350\u603b\u4f53\u4ecb\u7ecd"},{"location":"zh/tutorials/analytics-and-recommendation/#_1","text":"\u667a\u80fd\u63a8\u8350\u80fd\u591f\u5e2e\u52a9\u7528\u6237\u81ea\u52a8\u5206\u6790\u96c6\u7fa4\u5e76\u7ed9\u51fa\u4f18\u5316\u5efa\u8bae\u3002\u5c31\u50cf\u624b\u673a\u52a9\u624b\u4e00\u6837\uff0c\u667a\u80fd\u63a8\u8350\u4f1a\u5b9a\u671f\u7684\u626b\u63cf\u3001\u5206\u6790\u4f60\u7684\u96c6\u7fa4\u5e76\u7ed9\u51fa\u63a8\u8350\u5efa\u8bae\u3002\u76ee\u524d\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e24\u79cd\u4f18\u5316\u80fd\u529b\uff1a \u8d44\u6e90\u63a8\u8350 : \u901a\u8fc7\u8d44\u6e90\u63a8\u8350\u7684\u7b97\u6cd5\u5206\u6790\u5e94\u7528\u7684\u771f\u5b9e\u7528\u91cf\u63a8\u8350\u66f4\u5408\u9002\u7684\u8d44\u6e90\u914d\u7f6e\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003\u5e76\u91c7\u7eb3\u5b83\u63d0\u5347\u96c6\u7fa4\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002 \u526f\u672c\u6570\u63a8\u8350 : \u901a\u8fc7\u526f\u672c\u6570\u63a8\u8350\u7684\u7b97\u6cd5\u5206\u6790\u5e94\u7528\u7684\u771f\u5b9e\u7528\u91cf\u63a8\u8350\u66f4\u5408\u9002\u7684\u526f\u672c\u548c EHPA \u914d\u7f6e\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003\u5e76\u91c7\u7eb3\u5b83\u63d0\u5347\u96c6\u7fa4\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002 \u5e94\u7528\u53ef\u4ee5\u6839\u636e\u8d44\u6e90\u63a8\u8350\u8c03\u6574 request \u4e5f\u53ef\u4ee5\u6839\u636e\u526f\u672c\u6570\u63a8\u8350\u8c03\u6574\u526f\u672c\u6570\uff0c\u8fd9\u4e24\u79cd\u4f18\u5316\u90fd\u80fd\u5e2e\u52a9\u60a8\u964d\u4f4e\u6210\u672c\uff0c\u60a8\u53ef\u4ee5\u6839\u636e\u60a8\u7684\u9700\u6c42\u9009\u62e9\u91c7\u7528\u76f8\u5e94\u7684\u4f18\u5316\u5efa\u8bae\u3002","title":"\u667a\u80fd\u63a8\u8350"},{"location":"zh/tutorials/analytics-and-recommendation/#_2","text":"","title":"\u67b6\u6784"},{"location":"zh/tutorials/analytics-and-recommendation/#_3","text":"\u7528\u6237\u521b\u5efa Analytics \u5bf9\u8c61\uff0c\u901a\u8fc7 ResourceSelector \u9009\u62e9\u9700\u8981\u5206\u6790\u7684\u8d44\u6e90\uff0c\u652f\u6301\u9009\u62e9\u591a\u7c7b\u578b\uff08\u57fa\u4e8eGroup,Kind,Version\uff09\u7684\u6279\u91cf\u9009\u62e9 \u5e76\u884c\u5206\u6790\u6bcf\u4e2a\u9009\u62e9\u7684\u8d44\u6e90\uff0c\u5c1d\u8bd5\u8fdb\u884c\u5206\u6790\u63a8\u8350\uff0c\u6bcf\u6b21\u5206\u6790\u8fc7\u7a0b\u5206\u6210\u7b5b\u9009\u548c\u63a8\u8350\u4e24\u4e2a\u9636\u6bb5\uff1a \u7b5b\u9009\uff1a\u6392\u9664\u4e0d\u6ee1\u8db3\u63a8\u8350\u6761\u4ef6\u7684\u8d44\u6e90\u3002\u6bd4\u5982\u5bf9\u4e8e\u5f39\u6027\u63a8\u8350\uff0c\u6392\u9664\u6ca1\u6709 running pod \u7684 workload \u63a8\u8350\uff1a\u901a\u8fc7\u7b97\u6cd5\u8ba1\u7b97\u5206\u6790\uff0c\u7ed9\u51fa\u63a8\u8350\u7ed3\u679c \u5982\u679c\u901a\u8fc7\u7b5b\u9009\uff0c\u521b\u5efa Recommendation \u5bf9\u8c61\uff0c\u5c06\u63a8\u8350\u7ed3\u679c\u5c55\u793a\u5728 Recommendation.Status \u672a\u901a\u8fc7\u7b5b\u9009\u7684\u539f\u56e0\u548c\u72b6\u6001\u5c55\u793a\u5728 Analytics.Status \u6839\u636e\u8fd0\u884c\u95f4\u9694\u7b49\u5f85\u4e0b\u6b21\u5206\u6790","title":"\u4e00\u6b21\u5206\u6790\u7684\u8fc7\u7a0b"},{"location":"zh/tutorials/analytics-and-recommendation/#_4","text":"","title":"\u540d\u8bcd\u89e3\u91ca"},{"location":"zh/tutorials/analytics-and-recommendation/#_5","text":"\u5206\u6790\u5b9a\u4e49\u4e86\u4e00\u4e2a\u626b\u63cf\u5206\u6790\u4efb\u52a1\u3002\u652f\u6301\u4e24\u79cd\u4efb\u52a1\u7c7b\u578b\uff1a\u8d44\u6e90\u63a8\u8350\u548c\u5f39\u6027\u63a8\u8350\u3002Crane \u5b9a\u671f\u8fd0\u884c\u5206\u6790\u4efb\u52a1\uff0c\u5e76\u4ea7\u751f\u63a8\u8350\u7ed3\u679c\u3002","title":"\u5206\u6790"},{"location":"zh/tutorials/analytics-and-recommendation/#_6","text":"\u63a8\u8350\u5c55\u793a\u4e86\u4e00\u4e2a\u4f18\u5316\u63a8\u8350\u7684\u7ed3\u679c\u3002\u63a8\u8350\u7684\u7ed3\u679c\u662f\u4e00\u6bb5 YAML \u914d\u7f6e\uff0c\u6839\u636e\u7ed3\u679c\u7528\u6237\u53ef\u4ee5\u8fdb\u884c\u76f8\u5e94\u7684\u4f18\u5316\u52a8\u4f5c\uff0c\u6bd4\u5982\u8c03\u6574\u5e94\u7528\u7684\u8d44\u6e90\u914d\u7f6e\u3002","title":"\u63a8\u8350"},{"location":"zh/tutorials/analytics-and-recommendation/#_7","text":"\u4e0d\u540c\u7684\u5206\u6790\u91c7\u7528\u4e0d\u540c\u7684\u8ba1\u7b97\u6a21\u578b\uff0cCrane \u63d0\u4f9b\u4e86\u4e00\u5957\u9ed8\u8ba4\u7684\u8ba1\u7b97\u6a21\u578b\u4ee5\u53ca\u4e00\u5957\u914d\u5957\u7684\u914d\u7f6e\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539\u914d\u7f6e\u6765\u5b9a\u5236\u63a8\u8350\u7684\u6548\u679c\u3002\u652f\u6301\u4fee\u6539\u5168\u5c40\u7684\u9ed8\u8ba4\u914d\u7f6e\u548c\u4fee\u6539\u5355\u4e2a\u5206\u6790\u4efb\u52a1\u7684\u914d\u7f6e\u3002","title":"\u53c2\u6570\u914d\u7f6e"},{"location":"zh/tutorials/dynamic-scheduler-plugin/","text":"Dynamic Scheduler\uff1a\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u5668\u63d2\u4ef6 \u00b6 \u4ecb\u7ecd \u00b6 kubernetes \u7684\u539f\u751f\u8c03\u5ea6\u5668\u53ea\u80fd\u901a\u8fc7\u8d44\u6e90\u8bf7\u6c42\u6765\u8c03\u5ea6 pod\uff0c\u8fd9\u5f88\u5bb9\u6613\u9020\u6210\u4e00\u7cfb\u5217\u8d1f\u8f7d\u4e0d\u5747\u7684\u95ee\u9898\uff1a \u5bf9\u4e8e\u67d0\u4e9b\u8282\u70b9\uff0c\u5b9e\u9645\u8d1f\u8f7d\u4e0e\u8d44\u6e90\u8bf7\u6c42\u76f8\u5dee\u4e0d\u5927\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5f88\u5927\u6982\u7387\u51fa\u73b0\u7a33\u5b9a\u6027\u95ee\u9898\u3002 \u5bf9\u4e8e\u5176\u4ed6\u8282\u70b9\u6765\u8bf4\uff0c\u5b9e\u9645\u8d1f\u8f7d\u8fdc\u5c0f\u4e8e\u8d44\u6e90\u8bf7\u6c42\uff0c\u8fd9\u5c06\u5bfc\u81f4\u8d44\u6e90\u7684\u5de8\u5927\u6d6a\u8d39\u3002 \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u52a8\u6001\u8c03\u5ea6\u5668\u6839\u636e\u5b9e\u9645\u7684\u8282\u70b9\u5229\u7528\u7387\u6784\u5efa\u4e86\u4e00\u4e2a\u7b80\u5355\u4f46\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u5e76\u8fc7\u6ee4\u6389\u90a3\u4e9b\u8d1f\u8f7d\u9ad8\u7684\u8282\u70b9\u6765\u5e73\u8861\u96c6\u7fa4\u3002 \u8bbe\u8ba1\u7ec6\u8282 \u00b6 \u67b6\u6784 \u00b6 \u5982\u4e0a\u56fe\uff0c\u52a8\u6001\u8c03\u5ea6\u5668\u4f9d\u8d56\u4e8e Prometheus \u548c Node-exporter \u6536\u96c6\u548c\u6c47\u603b\u6307\u6807\u6570\u636e\uff0c\u5b83\u7531\u4e24\u4e2a\u7ec4\u4ef6\u7ec4\u6210\uff1a Note Node-annotator \u76ee\u524d\u662f Crane-scheduler-controller \u7684\u4e00\u4e2a\u6a21\u5757. Node-annotator \u5b9a\u671f\u4ece Prometheus \u62c9\u53d6\u6570\u636e\uff0c\u5e76\u4ee5\u6ce8\u91ca\u7684\u5f62\u5f0f\u5728\u8282\u70b9\u4e0a\u7528\u65f6\u95f4\u6233\u6807\u8bb0\u5b83\u4eec\u3002 Dynamic plugin \u76f4\u63a5\u4ece\u8282\u70b9\u7684\u6ce8\u91ca\u4e2d\u8bfb\u53d6\u8d1f\u8f7d\u6570\u636e\uff0c\u8fc7\u6ee4\u5e76\u57fa\u4e8e\u7b80\u5355\u7684\u7b97\u6cd5\u5bf9\u5019\u9009\u8282\u70b9\u8fdb\u884c\u8bc4\u5206\u3002 \u8c03\u5ea6\u7b56\u7565 \u00b6 \u52a8\u6001\u8c03\u5ea6\u5668\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ed8\u8ba4\u503c \u8c03\u5ea6\u7b56\u7565 \u5e76\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u7b56\u7565\u3002\u9ed8\u8ba4\u7b56\u7565\u4f9d\u8d56\u4e8e\u4ee5\u4e0b\u6307\u6807\uff1a cpu_usage_avg_5m cpu_usage_max_avg_1h cpu_usage_max_avg_1d mem_usage_avg_5m mem_usage_max_avg_1h mem_usage_max_avg_1d \u5728\u8c03\u5ea6\u7684 Filter \u9636\u6bb5\uff0c\u5982\u679c\u8be5\u8282\u70b9\u7684\u5b9e\u9645\u4f7f\u7528\u7387\u5927\u4e8e\u4e0a\u8ff0\u4efb\u4e00\u6307\u6807\u7684\u9608\u503c\uff0c\u5219\u8be5\u8282\u70b9\u5c06\u88ab\u8fc7\u6ee4\u3002\u800c\u5728 Score \u9636\u6bb5\uff0c\u6700\u7ec8\u5f97\u5206\u662f\u8fd9\u4e9b\u6307\u6807\u503c\u7684\u52a0\u6743\u548c\u3002 Hot Value \u00b6 \u5728\u751f\u4ea7\u96c6\u7fa4\u4e2d\uff0c\u53ef\u80fd\u4f1a\u9891\u7e41\u51fa\u73b0\u8c03\u5ea6\u70ed\u70b9\uff0c\u56e0\u4e3a\u521b\u5efa Pod \u540e\u8282\u70b9\u7684\u8d1f\u8f7d\u4e0d\u80fd\u7acb\u5373\u589e\u52a0\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u989d\u5916\u7684\u6307\u6807\uff0c\u540d\u4e3a Hot Value \uff0c\u8868\u793a\u8282\u70b9\u6700\u8fd1\u51e0\u6b21\u7684\u8c03\u5ea6\u9891\u7387\u3002\u5e76\u4e14\u8282\u70b9\u7684\u6700\u7ec8\u4f18\u5148\u7ea7\u662f\u6700\u7ec8\u5f97\u5206\u51cf\u53bb Hot Value \u3002","title":"Dynamic Scheduler\uff1a\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u5668\u63d2\u4ef6"},{"location":"zh/tutorials/dynamic-scheduler-plugin/#dynamic-scheduler","text":"","title":"Dynamic Scheduler\uff1a\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u5668\u63d2\u4ef6"},{"location":"zh/tutorials/dynamic-scheduler-plugin/#_1","text":"kubernetes \u7684\u539f\u751f\u8c03\u5ea6\u5668\u53ea\u80fd\u901a\u8fc7\u8d44\u6e90\u8bf7\u6c42\u6765\u8c03\u5ea6 pod\uff0c\u8fd9\u5f88\u5bb9\u6613\u9020\u6210\u4e00\u7cfb\u5217\u8d1f\u8f7d\u4e0d\u5747\u7684\u95ee\u9898\uff1a \u5bf9\u4e8e\u67d0\u4e9b\u8282\u70b9\uff0c\u5b9e\u9645\u8d1f\u8f7d\u4e0e\u8d44\u6e90\u8bf7\u6c42\u76f8\u5dee\u4e0d\u5927\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5f88\u5927\u6982\u7387\u51fa\u73b0\u7a33\u5b9a\u6027\u95ee\u9898\u3002 \u5bf9\u4e8e\u5176\u4ed6\u8282\u70b9\u6765\u8bf4\uff0c\u5b9e\u9645\u8d1f\u8f7d\u8fdc\u5c0f\u4e8e\u8d44\u6e90\u8bf7\u6c42\uff0c\u8fd9\u5c06\u5bfc\u81f4\u8d44\u6e90\u7684\u5de8\u5927\u6d6a\u8d39\u3002 \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u52a8\u6001\u8c03\u5ea6\u5668\u6839\u636e\u5b9e\u9645\u7684\u8282\u70b9\u5229\u7528\u7387\u6784\u5efa\u4e86\u4e00\u4e2a\u7b80\u5355\u4f46\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u5e76\u8fc7\u6ee4\u6389\u90a3\u4e9b\u8d1f\u8f7d\u9ad8\u7684\u8282\u70b9\u6765\u5e73\u8861\u96c6\u7fa4\u3002","title":"\u4ecb\u7ecd"},{"location":"zh/tutorials/dynamic-scheduler-plugin/#_2","text":"","title":"\u8bbe\u8ba1\u7ec6\u8282"},{"location":"zh/tutorials/dynamic-scheduler-plugin/#_3","text":"\u5982\u4e0a\u56fe\uff0c\u52a8\u6001\u8c03\u5ea6\u5668\u4f9d\u8d56\u4e8e Prometheus \u548c Node-exporter \u6536\u96c6\u548c\u6c47\u603b\u6307\u6807\u6570\u636e\uff0c\u5b83\u7531\u4e24\u4e2a\u7ec4\u4ef6\u7ec4\u6210\uff1a Note Node-annotator \u76ee\u524d\u662f Crane-scheduler-controller \u7684\u4e00\u4e2a\u6a21\u5757. Node-annotator \u5b9a\u671f\u4ece Prometheus \u62c9\u53d6\u6570\u636e\uff0c\u5e76\u4ee5\u6ce8\u91ca\u7684\u5f62\u5f0f\u5728\u8282\u70b9\u4e0a\u7528\u65f6\u95f4\u6233\u6807\u8bb0\u5b83\u4eec\u3002 Dynamic plugin \u76f4\u63a5\u4ece\u8282\u70b9\u7684\u6ce8\u91ca\u4e2d\u8bfb\u53d6\u8d1f\u8f7d\u6570\u636e\uff0c\u8fc7\u6ee4\u5e76\u57fa\u4e8e\u7b80\u5355\u7684\u7b97\u6cd5\u5bf9\u5019\u9009\u8282\u70b9\u8fdb\u884c\u8bc4\u5206\u3002","title":"\u67b6\u6784"},{"location":"zh/tutorials/dynamic-scheduler-plugin/#_4","text":"\u52a8\u6001\u8c03\u5ea6\u5668\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ed8\u8ba4\u503c \u8c03\u5ea6\u7b56\u7565 \u5e76\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u7b56\u7565\u3002\u9ed8\u8ba4\u7b56\u7565\u4f9d\u8d56\u4e8e\u4ee5\u4e0b\u6307\u6807\uff1a cpu_usage_avg_5m cpu_usage_max_avg_1h cpu_usage_max_avg_1d mem_usage_avg_5m mem_usage_max_avg_1h mem_usage_max_avg_1d \u5728\u8c03\u5ea6\u7684 Filter \u9636\u6bb5\uff0c\u5982\u679c\u8be5\u8282\u70b9\u7684\u5b9e\u9645\u4f7f\u7528\u7387\u5927\u4e8e\u4e0a\u8ff0\u4efb\u4e00\u6307\u6807\u7684\u9608\u503c\uff0c\u5219\u8be5\u8282\u70b9\u5c06\u88ab\u8fc7\u6ee4\u3002\u800c\u5728 Score \u9636\u6bb5\uff0c\u6700\u7ec8\u5f97\u5206\u662f\u8fd9\u4e9b\u6307\u6807\u503c\u7684\u52a0\u6743\u548c\u3002","title":"\u8c03\u5ea6\u7b56\u7565"},{"location":"zh/tutorials/dynamic-scheduler-plugin/#hot-value","text":"\u5728\u751f\u4ea7\u96c6\u7fa4\u4e2d\uff0c\u53ef\u80fd\u4f1a\u9891\u7e41\u51fa\u73b0\u8c03\u5ea6\u70ed\u70b9\uff0c\u56e0\u4e3a\u521b\u5efa Pod \u540e\u8282\u70b9\u7684\u8d1f\u8f7d\u4e0d\u80fd\u7acb\u5373\u589e\u52a0\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u989d\u5916\u7684\u6307\u6807\uff0c\u540d\u4e3a Hot Value \uff0c\u8868\u793a\u8282\u70b9\u6700\u8fd1\u51e0\u6b21\u7684\u8c03\u5ea6\u9891\u7387\u3002\u5e76\u4e14\u8282\u70b9\u7684\u6700\u7ec8\u4f18\u5148\u7ea7\u662f\u6700\u7ec8\u5f97\u5206\u51cf\u53bb Hot Value \u3002","title":"Hot Value"},{"location":"zh/tutorials/replicas-recommendation/","text":"\u526f\u672c\u6570\u63a8\u8350 \u00b6 Kubernetes \u7528\u6237\u5728\u521b\u5efa\u5e94\u7528\u8d44\u6e90\u65f6\u5e38\u5e38\u662f\u57fa\u4e8e\u7ecf\u9a8c\u503c\u6765\u8bbe\u7f6e\u526f\u672c\u6570\u6216\u8005 EHPA \u914d\u7f6e\u3002\u901a\u8fc7\u526f\u672c\u6570\u63a8\u8350\u7684\u7b97\u6cd5\u5206\u6790\u5e94\u7528\u7684\u771f\u5b9e\u7528\u91cf\u63a8\u8350\u66f4\u5408\u9002\u7684\u526f\u672c\u914d\u7f6e\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003\u5e76\u91c7\u7eb3\u5b83\u63d0\u5347\u96c6\u7fa4\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002 \u4ea7\u54c1\u529f\u80fd \u00b6 \u7b97\u6cd5\uff1a\u8ba1\u7b97\u526f\u672c\u6570\u7684\u7b97\u6cd5\u53c2\u8003\u4e86 HPA \u7684\u8ba1\u7b97\u516c\u5f0f\uff0c\u5e76\u4e14\u652f\u6301\u81ea\u5b9a\u4e49\u7b97\u6cd5\u7684\u5173\u952e\u914d\u7f6e HPA \u63a8\u8350\uff1a\u526f\u672c\u6570\u63a8\u8350\u4f1a\u626b\u63cf\u51fa\u9002\u5408\u914d\u7f6e\u6c34\u5e73\u5f39\u6027\uff08EHPA\uff09\u7684\u5e94\u7528\uff0c\u5e76\u7ed9\u51fa EHPA \u7684\u914d\u7f6e, EHPA \u662f Crane \u63d0\u4f9b\u4e86\u667a\u80fd\u6c34\u5e73\u5f39\u6027\u4ea7\u54c1 \u652f\u6301\u6279\u91cf\u5206\u6790\uff1a\u901a\u8fc7 Analytics \u7684 ResourceSelector\uff0c\u7528\u6237\u53ef\u4ee5\u6279\u91cf\u5206\u6790\u591a\u4e2a\u5de5\u4f5c\u8d1f\u8f7d \u521b\u5efa\u5f39\u6027\u5206\u6790 \u00b6 \u521b\u5efa\u4e00\u4e2a \u5f39\u6027\u5206\u6790 Analytics \uff0c\u8fd9\u91cc\u6211\u4eec\u901a\u8fc7\u5b9e\u4f8b deployment: nginx \u4f5c\u4e3a\u4e00\u4e2a\u4f8b\u5b50 Main Mirror kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/nginx-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-hpa.yaml kubectl get analytics kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/nginx-deployment.yaml?download = false kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/analytics-hpa.yaml?download = false kubectl get analytics analytics-hpa.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-replicas spec : type : Replicas # This can only be \"Resource\" or \"Replicas\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 600 # analytics selected resources every 10 minutes resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : nginx-deployment config : # defines all the configuration for this analytics replicas.workload-min-replicas : \"1\" replicas.fluctuation-threshold : \"0\" replicas.min-cpu-usage-threshold : \"0\" \u7ed3\u679c\u5982\u4e0b: NAME AGE nginx-replicas 16m \u67e5\u770b Analytics \u8be6\u60c5: kubectl get analytics nginx-replicas -o yaml \u7ed3\u679c\u5982\u4e0b: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-replicas namespace : default spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 600 config : replicas.fluctuation-threshold : \"0\" replicas.min-cpu-usage-threshold : \"0\" replicas.workload-min-replicas : \"1\" resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : nginx-deployment type : Replicas status : conditions : - lastTransitionTime : \"2022-06-17T06:56:07Z\" message : Analytics is ready reason : AnalyticsReady status : \"True\" type : Ready lastUpdateTime : \"2022-06-17T06:56:06Z\" recommendations : - lastStartTime : \"2022-06-17T06:56:06Z\" message : Success name : nginx-replicas-replicas-wq6wm namespace : default targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default uid : 59f3eb3c-f786-4b15-b37e-774e5784c2db \u67e5\u770b\u5206\u6790\u7ed3\u679c \u00b6 \u67e5\u770b Recommendation \u7ed3\u679c\uff1a kubectl get recommend -l analysis.crane.io/analytics-name = nginx-replicas -o yaml \u5206\u6790\u7ed3\u679c\u5982\u4e0b\uff1a apiVersion : v1 items : - apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : creationTimestamp : \"2022-06-17T06:56:06Z\" generateName : nginx-replicas-replicas- generation : 2 labels : analysis.crane.io/analytics-name : nginx-replicas analysis.crane.io/analytics-type : Replicas analysis.crane.io/analytics-uid : 795f245b-1e1f-4f7b-a02b-885d7a495e5b app : nginx name : nginx-replicas-replicas-wq6wm namespace : default ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : nginx-replicas uid : 795f245b-1e1f-4f7b-a02b-885d7a495e5b resourceVersion : \"2182455668\" selfLink : /apis/analysis.crane.io/v1alpha1/namespaces/default/recommendations/nginx-replicas-replicas-wq6wm uid : 59f3eb3c-f786-4b15-b37e-774e5784c2db spec : adoptionType : StatusAndAnnotation completionStrategy : completionStrategyType : Once targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default type : Replicas status : conditions : - lastTransitionTime : \"2022-06-17T06:56:07Z\" message : Recommendation is ready reason : RecommendationReady status : \"True\" type : Ready lastUpdateTime : \"2022-06-17T06:56:07Z\" recommendedValue : | effectiveHPA: maxReplicas: 3 metrics: - resource: name: cpu target: averageUtilization: 75 type: Utilization type: Resource minReplicas: 3 replicasRecommendation: replicas: 3 kind : List metadata : resourceVersion : \"\" selfLink : \"\" \u6279\u91cf\u63a8\u8350 \u00b6 \u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u4f8b\u5b50\u6765\u6f14\u793a\u5982\u4f55\u4f7f\u7528 Analytics \u63a8\u8350\u96c6\u7fa4\u4e2d\u6240\u6709\u7684 Deployment \u548c StatefulSet\uff1a apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : workload-replicas namespace : crane-system # The Analytics in Crane-system will select all resource across all namespaces. spec : type : Replicas # This can only be \"Resource\" or \"Replicas\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 - kind : StatefulSet apiVersion : apps/v1 \u5f53 namespace \u7b49\u4e8e crane-system \u65f6\uff0c Analytics \u9009\u62e9\u7684\u8d44\u6e90\u662f\u96c6\u7fa4\u4e2d\u6240\u6709\u7684 namespace\uff0c\u5f53 namespace \u4e0d\u7b49\u4e8e crane-system \u65f6\uff0c Analytics \u9009\u62e9 Analytics namespace \u4e0b\u7684\u8d44\u6e90 resourceSelectors \u901a\u8fc7\u6570\u7ec4\u914d\u7f6e\u9700\u8981\u5206\u6790\u7684\u8d44\u6e90\uff0ckind \u548c apiVersion \u662f\u5fc5\u586b\u5b57\u6bb5\uff0cname \u9009\u586b resourceSelectors \u652f\u6301\u914d\u7f6e\u4efb\u610f\u652f\u6301 Scale Subresource \u7684\u8d44\u6e90 \u5f39\u6027\u63a8\u8350\u8ba1\u7b97\u6a21\u578b \u00b6 \u7b5b\u9009\u9636\u6bb5 \u00b6 \u4f4e\u526f\u672c\u6570\u7684\u5de5\u4f5c\u8d1f\u8f7d: \u8fc7\u4f4e\u7684\u526f\u672c\u6570\u53ef\u80fd\u5f39\u6027\u9700\u6c42\u4e0d\u9ad8\uff0c\u5173\u8054\u914d\u7f6e: ehpa.deployment-min-replicas | ehpa.statefulset-min-replicas | ehpa.workload-min-replicas \u5b58\u5728\u4e00\u5b9a\u6bd4\u4f8b\u975e Running Pod \u7684\u5de5\u4f5c\u8d1f\u8f7d: \u5982\u679c\u5de5\u4f5c\u8d1f\u8f7d\u7684 Pod \u5927\u591a\u4e0d\u80fd\u6b63\u5e38\u8fd0\u884c\uff0c\u53ef\u80fd\u4e0d\u9002\u5408\u5f39\u6027\uff0c\u5173\u8054\u914d\u7f6e: ehpa.pod-min-ready-seconds | ehpa.pod-available-ratio \u4f4e CPU \u4f7f\u7528\u91cf\u7684\u5de5\u4f5c\u8d1f\u8f7d: \u8fc7\u4f4e\u4f7f\u7528\u91cf\u7684\u5de5\u4f5c\u8d1f\u8f7d\u610f\u5473\u7740\u6ca1\u6709\u4e1a\u52a1\u538b\u529b\uff0c\u6b64\u65f6\u901a\u8fc7\u4f7f\u7528\u7387\u63a8\u8350\u5f39\u6027\u4e0d\u51c6\uff0c\u5173\u8054\u914d\u7f6e: ehpa.min-cpu-usage-threshold CPU \u4f7f\u7528\u91cf\u7684\u6ce2\u52a8\u7387\u8fc7\u4f4e: \u4f7f\u7528\u91cf\u7684\u6700\u5927\u503c\u548c\u6700\u5c0f\u503c\u7684\u500d\u6570\u5b9a\u4e49\u4e3a\u6ce2\u52a8\u7387\uff0c\u6ce2\u52a8\u7387\u8fc7\u4f4e\u7684\u5de5\u4f5c\u8d1f\u8f7d\u901a\u8fc7\u5f39\u6027\u964d\u672c\u7684\u6536\u76ca\u4e0d\u5927\uff0c\u5173\u8054\u914d\u7f6e: ehpa.fluctuation-threshold \u63a8\u8350 \u00b6 \u63a8\u8350\u9636\u6bb5\u901a\u8fc7\u4ee5\u4e0b\u6a21\u578b\u63a8\u8350\u4e00\u4e2a EffectiveHPA \u7684 Spec\u3002\u6bcf\u4e2a\u5b57\u6bb5\u7684\u63a8\u8350\u903b\u8f91\u5982\u4e0b\uff1a \u63a8\u8350 TargetUtilization \u539f\u7406: \u4f7f\u7528 Pod P99 \u8d44\u6e90\u5229\u7528\u7387\u63a8\u8350\u5f39\u6027\u7684\u76ee\u6807\u3002\u56e0\u4e3a\u5982\u679c\u5e94\u7528\u53ef\u4ee5\u5728 P99 \u65f6\u95f4\u5185\u63a5\u53d7\u8fd9\u4e2a\u5229\u7528\u7387\uff0c\u53ef\u4ee5\u63a8\u65ad\u51fa\u53ef\u4f5c\u4e3a\u5f39\u6027\u7684\u76ee\u6807\u3002 \u901a\u8fc7 Percentile \u7b97\u6cd5\u5f97\u5230 Pod \u8fc7\u53bb\u4e03\u5929 \u7684 P99 \u4f7f\u7528\u91cf: \\(pod\\_cpu\\_usage\\_p99\\) \u5bf9\u5e94\u7684\u5229\u7528\u7387: \\(target\\_pod\\_CPU\\_utilization = \\frac{pod\\_cpu\\_usage\\_p99}{pod\\_cpu\\_request}\\) \u4e3a\u4e86\u9632\u6b62\u5229\u7528\u7387\u8fc7\u5927\u6216\u8fc7\u5c0f\uff0ctarget_pod_cpu_utilization \u9700\u8981\u5c0f\u4e8e ehpa.min-cpu-target-utilization \u548c\u5927\u4e8e ehpa.max-cpu-target-utilization \\(ehpa.max\\mbox{-}cpu\\mbox{-}target\\mbox{-}utilization < target\\_pod\\_cpu\\_utilization < ehpa.min\\mbox{-}cpu\\mbox{-}target\\mbox{-}utilization\\) \u63a8\u8350 minReplicas \u539f\u7406: \u4f7f\u7528 workload \u8fc7\u53bb\u4e03\u5929\u5185\u6bcf\u5c0f\u65f6\u8d1f\u8f7d\u6700\u4f4e\u7684\u5229\u7528\u7387\u63a8\u8350 minReplicas\u3002 \u8ba1\u7b97\u8fc7\u53bb7\u5929 workload \u6bcf\u5c0f\u65f6\u4f7f\u7528\u91cf\u4e2d\u4f4d\u6570\u7684\u6700\u4f4e\u503c: \\(workload\\_cpu\\_usage\\_medium\\_min\\) \u5bf9\u5e94\u7684\u6700\u4f4e\u5229\u7528\u7387\u5bf9\u5e94\u7684\u526f\u672c\u6570: \\(minReplicas = \\frac{\\mathrm{workload\\_cpu\\_usage\\_medium\\_min} }{pod\\_cpu\\_request \\times ehpa.max-cpu-target-utilization}\\) \u4e3a\u4e86\u9632\u6b62 minReplicas \u8fc7\u5c0f\uff0cminReplicas \u9700\u8981\u5927\u4e8e\u7b49\u4e8e ehpa.default-min-replicas \\(minReplicas \\geq ehpa.default\\mbox{-}min\\mbox{-}replicas\\) \u63a8\u8350 maxReplicas \u539f\u7406: \u4f7f\u7528 workload \u8fc7\u53bb\u548c\u672a\u6765\u4e03\u5929\u7684\u8d1f\u8f7d\u63a8\u8350\u6700\u5927\u526f\u672c\u6570\u3002 \u8ba1\u7b97\u8fc7\u53bb\u4e03\u5929\u548c\u672a\u6765\u4e03\u5929 workload cpu \u4f7f\u7528\u91cf\u7684 P95: \\(workload\\_cpu\\_usage\\_p95\\) \u5bf9\u5e94\u7684\u526f\u672c\u6570: \\(max\\_replicas\\_origin = \\frac{\\mathrm{workload\\_cpu\\_usage\\_p95} }{pod\\_cpu\\_request \\times target\\_cpu\\_utilization}\\) \u4e3a\u4e86\u5e94\u5bf9\u6d41\u91cf\u6d2a\u5cf0\uff0c\u653e\u5927\u4e00\u5b9a\u500d\u6570: \\(max\\_replicas = max\\_replicas\\_origin \\times ehpa.max\\mbox{-}replicas\\mbox{-}factor\\) \u63a8\u8350CPU\u4ee5\u5916 MetricSpec \u5982\u679c workload \u914d\u7f6e\u4e86 HPA\uff0c\u7ee7\u627f\u76f8\u5e94\u9664 CpuUtilization \u4ee5\u5916\u7684\u5176\u4ed6 MetricSpec \u63a8\u8350 Behavior \u5982\u679c workload \u914d\u7f6e\u4e86 HPA\uff0c\u7ee7\u627f\u76f8\u5e94\u7684 Behavior \u914d\u7f6e \u9884\u6d4b \u5c1d\u8bd5\u9884\u6d4b\u5de5\u4f5c\u8d1f\u8f7d\u672a\u6765\u4e03\u5929\u7684 CPU \u4f7f\u7528\u91cf\uff0c\u7b97\u6cd5\u662f DSP \u5982\u679c\u9884\u6d4b\u6210\u529f\u5219\u6dfb\u52a0\u9884\u6d4b\u914d\u7f6e \u5982\u679c\u4e0d\u53ef\u9884\u6d4b\u5219\u4e0d\u6dfb\u52a0\u9884\u6d4b\u914d\u7f6e\uff0c\u9000\u5316\u6210\u4e0d\u5177\u6709\u9884\u6d4b\u529f\u80fd\u7684 EffectiveHPA \u5f39\u6027\u5206\u6790\u8ba1\u7b97\u914d\u7f6e \u00b6 \u914d\u7f6e\u9879 \u9ed8\u8ba4\u503c \u63cf\u8ff0 ehpa.deployment-min-replicas 1 \u5c0f\u4e8e\u8be5\u503c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u505a\u5f39\u6027\u63a8\u8350 ehpa.statefulset-min-replicas 1 \u5c0f\u4e8e\u8be5\u503c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u505a\u5f39\u6027\u63a8\u8350 ehpa.workload-min-replicas 1 \u5c0f\u4e8e\u8be5\u503c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u505a\u5f39\u6027\u63a8\u8350 ehpa.pod-min-ready-seconds 30 \u5b9a\u4e49\u4e86 Pod \u662f\u5426 Ready \u7684\u79d2\u6570 ehpa.pod-available-ratio 0.5 Ready Pod \u6bd4\u4f8b\u5c0f\u4e8e\u8be5\u503c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u505a\u5f39\u6027\u63a8\u8350 ehpa.default-min-replicas 2 \u6700\u5c0f minReplicas ehpa.max-replicas-factor 3 \u8ba1\u7b97 maxReplicas \u7684\u500d\u6570 ehpa.min-cpu-usage-threshold 10 \u5c0f\u4e8e\u8be5\u503c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u505a\u5f39\u6027\u63a8\u8350 ehpa.fluctuation-threshold 1.5 \u5c0f\u4e8e\u8be5\u503c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u505a\u5f39\u6027\u63a8\u8350 ehpa.min-cpu-target-utilization 30 ehpa.max-cpu-target-utilization 75 ehpa.reference-hpa true \u7ee7\u627f\u73b0\u6709\u7684 HPA \u914d\u7f6e","title":"\u526f\u672c\u6570\u63a8\u8350"},{"location":"zh/tutorials/replicas-recommendation/#_1","text":"Kubernetes \u7528\u6237\u5728\u521b\u5efa\u5e94\u7528\u8d44\u6e90\u65f6\u5e38\u5e38\u662f\u57fa\u4e8e\u7ecf\u9a8c\u503c\u6765\u8bbe\u7f6e\u526f\u672c\u6570\u6216\u8005 EHPA \u914d\u7f6e\u3002\u901a\u8fc7\u526f\u672c\u6570\u63a8\u8350\u7684\u7b97\u6cd5\u5206\u6790\u5e94\u7528\u7684\u771f\u5b9e\u7528\u91cf\u63a8\u8350\u66f4\u5408\u9002\u7684\u526f\u672c\u914d\u7f6e\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003\u5e76\u91c7\u7eb3\u5b83\u63d0\u5347\u96c6\u7fa4\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002","title":"\u526f\u672c\u6570\u63a8\u8350"},{"location":"zh/tutorials/replicas-recommendation/#_2","text":"\u7b97\u6cd5\uff1a\u8ba1\u7b97\u526f\u672c\u6570\u7684\u7b97\u6cd5\u53c2\u8003\u4e86 HPA \u7684\u8ba1\u7b97\u516c\u5f0f\uff0c\u5e76\u4e14\u652f\u6301\u81ea\u5b9a\u4e49\u7b97\u6cd5\u7684\u5173\u952e\u914d\u7f6e HPA \u63a8\u8350\uff1a\u526f\u672c\u6570\u63a8\u8350\u4f1a\u626b\u63cf\u51fa\u9002\u5408\u914d\u7f6e\u6c34\u5e73\u5f39\u6027\uff08EHPA\uff09\u7684\u5e94\u7528\uff0c\u5e76\u7ed9\u51fa EHPA \u7684\u914d\u7f6e, EHPA \u662f Crane \u63d0\u4f9b\u4e86\u667a\u80fd\u6c34\u5e73\u5f39\u6027\u4ea7\u54c1 \u652f\u6301\u6279\u91cf\u5206\u6790\uff1a\u901a\u8fc7 Analytics \u7684 ResourceSelector\uff0c\u7528\u6237\u53ef\u4ee5\u6279\u91cf\u5206\u6790\u591a\u4e2a\u5de5\u4f5c\u8d1f\u8f7d","title":"\u4ea7\u54c1\u529f\u80fd"},{"location":"zh/tutorials/replicas-recommendation/#_3","text":"\u521b\u5efa\u4e00\u4e2a \u5f39\u6027\u5206\u6790 Analytics \uff0c\u8fd9\u91cc\u6211\u4eec\u901a\u8fc7\u5b9e\u4f8b deployment: nginx \u4f5c\u4e3a\u4e00\u4e2a\u4f8b\u5b50 Main Mirror kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/nginx-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-hpa.yaml kubectl get analytics kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/nginx-deployment.yaml?download = false kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/analytics-hpa.yaml?download = false kubectl get analytics analytics-hpa.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-replicas spec : type : Replicas # This can only be \"Resource\" or \"Replicas\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 600 # analytics selected resources every 10 minutes resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : nginx-deployment config : # defines all the configuration for this analytics replicas.workload-min-replicas : \"1\" replicas.fluctuation-threshold : \"0\" replicas.min-cpu-usage-threshold : \"0\" \u7ed3\u679c\u5982\u4e0b: NAME AGE nginx-replicas 16m \u67e5\u770b Analytics \u8be6\u60c5: kubectl get analytics nginx-replicas -o yaml \u7ed3\u679c\u5982\u4e0b: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-replicas namespace : default spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 600 config : replicas.fluctuation-threshold : \"0\" replicas.min-cpu-usage-threshold : \"0\" replicas.workload-min-replicas : \"1\" resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : nginx-deployment type : Replicas status : conditions : - lastTransitionTime : \"2022-06-17T06:56:07Z\" message : Analytics is ready reason : AnalyticsReady status : \"True\" type : Ready lastUpdateTime : \"2022-06-17T06:56:06Z\" recommendations : - lastStartTime : \"2022-06-17T06:56:06Z\" message : Success name : nginx-replicas-replicas-wq6wm namespace : default targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default uid : 59f3eb3c-f786-4b15-b37e-774e5784c2db","title":"\u521b\u5efa\u5f39\u6027\u5206\u6790"},{"location":"zh/tutorials/replicas-recommendation/#_4","text":"\u67e5\u770b Recommendation \u7ed3\u679c\uff1a kubectl get recommend -l analysis.crane.io/analytics-name = nginx-replicas -o yaml \u5206\u6790\u7ed3\u679c\u5982\u4e0b\uff1a apiVersion : v1 items : - apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : creationTimestamp : \"2022-06-17T06:56:06Z\" generateName : nginx-replicas-replicas- generation : 2 labels : analysis.crane.io/analytics-name : nginx-replicas analysis.crane.io/analytics-type : Replicas analysis.crane.io/analytics-uid : 795f245b-1e1f-4f7b-a02b-885d7a495e5b app : nginx name : nginx-replicas-replicas-wq6wm namespace : default ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : nginx-replicas uid : 795f245b-1e1f-4f7b-a02b-885d7a495e5b resourceVersion : \"2182455668\" selfLink : /apis/analysis.crane.io/v1alpha1/namespaces/default/recommendations/nginx-replicas-replicas-wq6wm uid : 59f3eb3c-f786-4b15-b37e-774e5784c2db spec : adoptionType : StatusAndAnnotation completionStrategy : completionStrategyType : Once targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default type : Replicas status : conditions : - lastTransitionTime : \"2022-06-17T06:56:07Z\" message : Recommendation is ready reason : RecommendationReady status : \"True\" type : Ready lastUpdateTime : \"2022-06-17T06:56:07Z\" recommendedValue : | effectiveHPA: maxReplicas: 3 metrics: - resource: name: cpu target: averageUtilization: 75 type: Utilization type: Resource minReplicas: 3 replicasRecommendation: replicas: 3 kind : List metadata : resourceVersion : \"\" selfLink : \"\"","title":"\u67e5\u770b\u5206\u6790\u7ed3\u679c"},{"location":"zh/tutorials/replicas-recommendation/#_5","text":"\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u4f8b\u5b50\u6765\u6f14\u793a\u5982\u4f55\u4f7f\u7528 Analytics \u63a8\u8350\u96c6\u7fa4\u4e2d\u6240\u6709\u7684 Deployment \u548c StatefulSet\uff1a apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : workload-replicas namespace : crane-system # The Analytics in Crane-system will select all resource across all namespaces. spec : type : Replicas # This can only be \"Resource\" or \"Replicas\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 - kind : StatefulSet apiVersion : apps/v1 \u5f53 namespace \u7b49\u4e8e crane-system \u65f6\uff0c Analytics \u9009\u62e9\u7684\u8d44\u6e90\u662f\u96c6\u7fa4\u4e2d\u6240\u6709\u7684 namespace\uff0c\u5f53 namespace \u4e0d\u7b49\u4e8e crane-system \u65f6\uff0c Analytics \u9009\u62e9 Analytics namespace \u4e0b\u7684\u8d44\u6e90 resourceSelectors \u901a\u8fc7\u6570\u7ec4\u914d\u7f6e\u9700\u8981\u5206\u6790\u7684\u8d44\u6e90\uff0ckind \u548c apiVersion \u662f\u5fc5\u586b\u5b57\u6bb5\uff0cname \u9009\u586b resourceSelectors \u652f\u6301\u914d\u7f6e\u4efb\u610f\u652f\u6301 Scale Subresource \u7684\u8d44\u6e90","title":"\u6279\u91cf\u63a8\u8350"},{"location":"zh/tutorials/replicas-recommendation/#_6","text":"","title":"\u5f39\u6027\u63a8\u8350\u8ba1\u7b97\u6a21\u578b"},{"location":"zh/tutorials/replicas-recommendation/#_7","text":"\u4f4e\u526f\u672c\u6570\u7684\u5de5\u4f5c\u8d1f\u8f7d: \u8fc7\u4f4e\u7684\u526f\u672c\u6570\u53ef\u80fd\u5f39\u6027\u9700\u6c42\u4e0d\u9ad8\uff0c\u5173\u8054\u914d\u7f6e: ehpa.deployment-min-replicas | ehpa.statefulset-min-replicas | ehpa.workload-min-replicas \u5b58\u5728\u4e00\u5b9a\u6bd4\u4f8b\u975e Running Pod \u7684\u5de5\u4f5c\u8d1f\u8f7d: \u5982\u679c\u5de5\u4f5c\u8d1f\u8f7d\u7684 Pod \u5927\u591a\u4e0d\u80fd\u6b63\u5e38\u8fd0\u884c\uff0c\u53ef\u80fd\u4e0d\u9002\u5408\u5f39\u6027\uff0c\u5173\u8054\u914d\u7f6e: ehpa.pod-min-ready-seconds | ehpa.pod-available-ratio \u4f4e CPU \u4f7f\u7528\u91cf\u7684\u5de5\u4f5c\u8d1f\u8f7d: \u8fc7\u4f4e\u4f7f\u7528\u91cf\u7684\u5de5\u4f5c\u8d1f\u8f7d\u610f\u5473\u7740\u6ca1\u6709\u4e1a\u52a1\u538b\u529b\uff0c\u6b64\u65f6\u901a\u8fc7\u4f7f\u7528\u7387\u63a8\u8350\u5f39\u6027\u4e0d\u51c6\uff0c\u5173\u8054\u914d\u7f6e: ehpa.min-cpu-usage-threshold CPU \u4f7f\u7528\u91cf\u7684\u6ce2\u52a8\u7387\u8fc7\u4f4e: \u4f7f\u7528\u91cf\u7684\u6700\u5927\u503c\u548c\u6700\u5c0f\u503c\u7684\u500d\u6570\u5b9a\u4e49\u4e3a\u6ce2\u52a8\u7387\uff0c\u6ce2\u52a8\u7387\u8fc7\u4f4e\u7684\u5de5\u4f5c\u8d1f\u8f7d\u901a\u8fc7\u5f39\u6027\u964d\u672c\u7684\u6536\u76ca\u4e0d\u5927\uff0c\u5173\u8054\u914d\u7f6e: ehpa.fluctuation-threshold","title":"\u7b5b\u9009\u9636\u6bb5"},{"location":"zh/tutorials/replicas-recommendation/#_8","text":"\u63a8\u8350\u9636\u6bb5\u901a\u8fc7\u4ee5\u4e0b\u6a21\u578b\u63a8\u8350\u4e00\u4e2a EffectiveHPA \u7684 Spec\u3002\u6bcf\u4e2a\u5b57\u6bb5\u7684\u63a8\u8350\u903b\u8f91\u5982\u4e0b\uff1a \u63a8\u8350 TargetUtilization \u539f\u7406: \u4f7f\u7528 Pod P99 \u8d44\u6e90\u5229\u7528\u7387\u63a8\u8350\u5f39\u6027\u7684\u76ee\u6807\u3002\u56e0\u4e3a\u5982\u679c\u5e94\u7528\u53ef\u4ee5\u5728 P99 \u65f6\u95f4\u5185\u63a5\u53d7\u8fd9\u4e2a\u5229\u7528\u7387\uff0c\u53ef\u4ee5\u63a8\u65ad\u51fa\u53ef\u4f5c\u4e3a\u5f39\u6027\u7684\u76ee\u6807\u3002 \u901a\u8fc7 Percentile \u7b97\u6cd5\u5f97\u5230 Pod \u8fc7\u53bb\u4e03\u5929 \u7684 P99 \u4f7f\u7528\u91cf: \\(pod\\_cpu\\_usage\\_p99\\) \u5bf9\u5e94\u7684\u5229\u7528\u7387: \\(target\\_pod\\_CPU\\_utilization = \\frac{pod\\_cpu\\_usage\\_p99}{pod\\_cpu\\_request}\\) \u4e3a\u4e86\u9632\u6b62\u5229\u7528\u7387\u8fc7\u5927\u6216\u8fc7\u5c0f\uff0ctarget_pod_cpu_utilization \u9700\u8981\u5c0f\u4e8e ehpa.min-cpu-target-utilization \u548c\u5927\u4e8e ehpa.max-cpu-target-utilization \\(ehpa.max\\mbox{-}cpu\\mbox{-}target\\mbox{-}utilization < target\\_pod\\_cpu\\_utilization < ehpa.min\\mbox{-}cpu\\mbox{-}target\\mbox{-}utilization\\) \u63a8\u8350 minReplicas \u539f\u7406: \u4f7f\u7528 workload \u8fc7\u53bb\u4e03\u5929\u5185\u6bcf\u5c0f\u65f6\u8d1f\u8f7d\u6700\u4f4e\u7684\u5229\u7528\u7387\u63a8\u8350 minReplicas\u3002 \u8ba1\u7b97\u8fc7\u53bb7\u5929 workload \u6bcf\u5c0f\u65f6\u4f7f\u7528\u91cf\u4e2d\u4f4d\u6570\u7684\u6700\u4f4e\u503c: \\(workload\\_cpu\\_usage\\_medium\\_min\\) \u5bf9\u5e94\u7684\u6700\u4f4e\u5229\u7528\u7387\u5bf9\u5e94\u7684\u526f\u672c\u6570: \\(minReplicas = \\frac{\\mathrm{workload\\_cpu\\_usage\\_medium\\_min} }{pod\\_cpu\\_request \\times ehpa.max-cpu-target-utilization}\\) \u4e3a\u4e86\u9632\u6b62 minReplicas \u8fc7\u5c0f\uff0cminReplicas \u9700\u8981\u5927\u4e8e\u7b49\u4e8e ehpa.default-min-replicas \\(minReplicas \\geq ehpa.default\\mbox{-}min\\mbox{-}replicas\\) \u63a8\u8350 maxReplicas \u539f\u7406: \u4f7f\u7528 workload \u8fc7\u53bb\u548c\u672a\u6765\u4e03\u5929\u7684\u8d1f\u8f7d\u63a8\u8350\u6700\u5927\u526f\u672c\u6570\u3002 \u8ba1\u7b97\u8fc7\u53bb\u4e03\u5929\u548c\u672a\u6765\u4e03\u5929 workload cpu \u4f7f\u7528\u91cf\u7684 P95: \\(workload\\_cpu\\_usage\\_p95\\) \u5bf9\u5e94\u7684\u526f\u672c\u6570: \\(max\\_replicas\\_origin = \\frac{\\mathrm{workload\\_cpu\\_usage\\_p95} }{pod\\_cpu\\_request \\times target\\_cpu\\_utilization}\\) \u4e3a\u4e86\u5e94\u5bf9\u6d41\u91cf\u6d2a\u5cf0\uff0c\u653e\u5927\u4e00\u5b9a\u500d\u6570: \\(max\\_replicas = max\\_replicas\\_origin \\times ehpa.max\\mbox{-}replicas\\mbox{-}factor\\) \u63a8\u8350CPU\u4ee5\u5916 MetricSpec \u5982\u679c workload \u914d\u7f6e\u4e86 HPA\uff0c\u7ee7\u627f\u76f8\u5e94\u9664 CpuUtilization \u4ee5\u5916\u7684\u5176\u4ed6 MetricSpec \u63a8\u8350 Behavior \u5982\u679c workload \u914d\u7f6e\u4e86 HPA\uff0c\u7ee7\u627f\u76f8\u5e94\u7684 Behavior \u914d\u7f6e \u9884\u6d4b \u5c1d\u8bd5\u9884\u6d4b\u5de5\u4f5c\u8d1f\u8f7d\u672a\u6765\u4e03\u5929\u7684 CPU \u4f7f\u7528\u91cf\uff0c\u7b97\u6cd5\u662f DSP \u5982\u679c\u9884\u6d4b\u6210\u529f\u5219\u6dfb\u52a0\u9884\u6d4b\u914d\u7f6e \u5982\u679c\u4e0d\u53ef\u9884\u6d4b\u5219\u4e0d\u6dfb\u52a0\u9884\u6d4b\u914d\u7f6e\uff0c\u9000\u5316\u6210\u4e0d\u5177\u6709\u9884\u6d4b\u529f\u80fd\u7684 EffectiveHPA","title":"\u63a8\u8350"},{"location":"zh/tutorials/replicas-recommendation/#_9","text":"\u914d\u7f6e\u9879 \u9ed8\u8ba4\u503c \u63cf\u8ff0 ehpa.deployment-min-replicas 1 \u5c0f\u4e8e\u8be5\u503c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u505a\u5f39\u6027\u63a8\u8350 ehpa.statefulset-min-replicas 1 \u5c0f\u4e8e\u8be5\u503c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u505a\u5f39\u6027\u63a8\u8350 ehpa.workload-min-replicas 1 \u5c0f\u4e8e\u8be5\u503c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u505a\u5f39\u6027\u63a8\u8350 ehpa.pod-min-ready-seconds 30 \u5b9a\u4e49\u4e86 Pod \u662f\u5426 Ready \u7684\u79d2\u6570 ehpa.pod-available-ratio 0.5 Ready Pod \u6bd4\u4f8b\u5c0f\u4e8e\u8be5\u503c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u505a\u5f39\u6027\u63a8\u8350 ehpa.default-min-replicas 2 \u6700\u5c0f minReplicas ehpa.max-replicas-factor 3 \u8ba1\u7b97 maxReplicas \u7684\u500d\u6570 ehpa.min-cpu-usage-threshold 10 \u5c0f\u4e8e\u8be5\u503c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u505a\u5f39\u6027\u63a8\u8350 ehpa.fluctuation-threshold 1.5 \u5c0f\u4e8e\u8be5\u503c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u505a\u5f39\u6027\u63a8\u8350 ehpa.min-cpu-target-utilization 30 ehpa.max-cpu-target-utilization 75 ehpa.reference-hpa true \u7ee7\u627f\u73b0\u6709\u7684 HPA \u914d\u7f6e","title":"\u5f39\u6027\u5206\u6790\u8ba1\u7b97\u914d\u7f6e"},{"location":"zh/tutorials/resource-recommendation/","text":"\u8d44\u6e90\u63a8\u8350 \u00b6 Kubernetes \u7528\u6237\u5728\u521b\u5efa\u5e94\u7528\u8d44\u6e90\u65f6\u5e38\u5e38\u662f\u57fa\u4e8e\u7ecf\u9a8c\u503c\u6765\u8bbe\u7f6e request \u548c limit\u3002\u901a\u8fc7\u8d44\u6e90\u63a8\u8350\u7684\u7b97\u6cd5\u5206\u6790\u5e94\u7528\u7684\u771f\u5b9e\u7528\u91cf\u63a8\u8350\u66f4\u5408\u9002\u7684\u8d44\u6e90\u914d\u7f6e\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003\u5e76\u91c7\u7eb3\u5b83\u63d0\u5347\u96c6\u7fa4\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002 \u4ea7\u54c1\u529f\u80fd \u00b6 \u8d44\u6e90\u63a8\u8350\u662f VPA \u7684\u8f7b\u91cf\u5316\u5b9e\u73b0\uff0c\u4e14\u66f4\u7075\u6d3b\u3002 \u7b97\u6cd5\uff1a\u7b97\u6cd5\u6a21\u578b\u91c7\u7528\u4e86 VPA \u7684\u6ed1\u52a8\u7a97\u53e3\uff08Moving Window\uff09\u7b97\u6cd5\uff0c\u5e76\u4e14\u652f\u6301\u81ea\u5b9a\u4e49\u7b97\u6cd5\u7684\u5173\u952e\u914d\u7f6e\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027 \u652f\u6301\u6279\u91cf\u5206\u6790\uff1a\u901a\u8fc7 Analytics \u7684 ResourceSelector\uff0c\u7528\u6237\u53ef\u4ee5\u6279\u91cf\u5206\u6790\u591a\u4e2a\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u800c\u65e0\u9700\u4e00\u4e2a\u4e00\u4e2a\u7684\u521b\u5efa VPA \u5bf9\u8c61 \u66f4\u8f7b\u4fbf\uff1a\u7531\u4e8e VPA \u7684 Auto \u6a21\u5f0f\u5728\u66f4\u65b0\u5bb9\u5668\u8d44\u6e90\u914d\u7f6e\u65f6\u4f1a\u5bfc\u81f4\u5bb9\u5668\u91cd\u5efa\uff0c\u56e0\u6b64\u5f88\u96be\u5728\u751f\u4ea7\u4e0a\u4f7f\u7528\u81ea\u52a8\u6a21\u5f0f\uff0c\u8d44\u6e90\u63a8\u8350\u7ed9\u7528\u6237\u63d0\u4f9b\u8d44\u6e90\u5efa\u8bae\uff0c\u628a\u53d8\u66f4\u7684\u51b3\u5b9a\u4ea4\u7ed9\u7528\u6237\u51b3\u5b9a \u521b\u5efa\u8d44\u6e90\u5206\u6790 \u00b6 \u6211\u4eec\u901a\u8fc7 deployment: nginx \u548c Analytics \u4f5c\u4e3a\u4e00\u4e2a\u4f8b\u5b50\u6f14\u793a\u5982\u4f55\u5f00\u59cb\u4e00\u6b21\u8d44\u6e90\u63a8\u8350\u4e4b\u65c5\uff1a Main Mirror kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/nginx-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-resource.yaml kubectl get analytics kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/nginx-deployment.yaml?download = false kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/analytics-resource.yaml?download = false kubectl get analytics analytics-resource.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-resource spec : type : Resource # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : nginx-deployment \u7ed3\u679c\u5982\u4e0b: NAME AGE nginx-resource 16m \u67e5\u770b Analytics \u8be6\u60c5: kubectl get analytics nginx-resource -o yaml \u7ed3\u679c\u5982\u4e0b: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-resource namespace : default spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : nginx-deployment type : Resource status : conditions : - lastTransitionTime : \"2022-05-15T14:38:35Z\" message : Analytics is ready reason : AnalyticsReady status : \"True\" type : Ready lastUpdateTime : \"2022-05-15T14:38:35Z\" recommendations : - lastStartTime : \"2022-05-15T14:38:35Z\" message : Success name : nginx-resource-resource-w45nq namespace : default targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default uid : 750cb3bd-0b87-4f87-acbe-57e621af0a1e \u67e5\u770b\u5206\u6790\u7ed3\u679c \u00b6 \u67e5\u770b\u5206\u6790\u7ed3\u679c Recommendation \uff1a kubectl get recommend -l analysis.crane.io/analytics-name = nginx-resource -o yaml \u5206\u6790\u7ed3\u679c\u5982\u4e0b\uff1a apiVersion : v1 items : - apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : creationTimestamp : \"2022-06-15T15:26:25Z\" generateName : nginx-resource-resource- generation : 1 labels : analysis.crane.io/analytics-name : nginx-resource analysis.crane.io/analytics-type : Resource analysis.crane.io/analytics-uid : 9e78964b-f8ae-40de-9740-f9a715d16280 app : nginx name : nginx-resource-resource-t4xpn namespace : default ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : nginx-resource uid : 9e78964b-f8ae-40de-9740-f9a715d16280 resourceVersion : \"2117439429\" selfLink : /apis/analysis.crane.io/v1alpha1/namespaces/default/recommendations/nginx-resource-resource-t4xpn uid : 8005e3e0-8fe9-470b-99cf-5ce9dd407529 spec : adoptionType : StatusAndAnnotation completionStrategy : completionStrategyType : Once targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default type : Resource status : recommendedValue : | resourceRequest: containers: - containerName: nginx target: cpu: 100m memory: 100Mi kind : List metadata : resourceVersion : \"\" selfLink : \"\" \u6279\u91cf\u63a8\u8350 \u00b6 \u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u4f8b\u5b50\u6765\u6f14\u793a\u5982\u4f55\u4f7f\u7528 Analytics \u63a8\u8350\u96c6\u7fa4\u4e2d\u6240\u6709\u7684 Deployment \u548c StatefulSet\uff1a apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : workload-resource namespace : crane-system # The Analytics in Crane-system will select all resource across all namespaces. spec : type : Resource # This can only be \"Resource\" or \"Replicas\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 - kind : StatefulSet apiVersion : apps/v1 \u5f53 namespace \u7b49\u4e8e crane-system \u65f6\uff0c Analytics \u9009\u62e9\u7684\u8d44\u6e90\u662f\u96c6\u7fa4\u4e2d\u6240\u6709\u7684 namespace\uff0c\u5f53 namespace \u4e0d\u7b49\u4e8e crane-system \u65f6\uff0c Analytics \u9009\u62e9 Analytics namespace \u4e0b\u7684\u8d44\u6e90 resourceSelectors \u901a\u8fc7\u6570\u7ec4\u914d\u7f6e\u9700\u8981\u5206\u6790\u7684\u8d44\u6e90\uff0ckind \u548c apiVersion \u662f\u5fc5\u586b\u5b57\u6bb5\uff0cname \u9009\u586b resourceSelectors \u652f\u6301\u914d\u7f6e\u4efb\u610f\u652f\u6301 Scale Subresource \u7684\u8d44\u6e90 \u8d44\u6e90\u63a8\u8350\u8ba1\u7b97\u6a21\u578b \u00b6 \u7b5b\u9009\u9636\u6bb5 \u00b6 \u6ca1\u6709 Pod \u7684\u5de5\u4f5c\u8d1f\u8f7d: \u5982\u679c\u5de5\u4f5c\u8d1f\u8f7d\u6ca1\u6709 Pod\uff0c\u65e0\u6cd5\u8fdb\u884c\u7b97\u6cd5\u5206\u6790\u3002 \u63a8\u8350 \u00b6 \u91c7\u7528 VPA \u7684\u6ed1\u52a8\u7a97\u53e3\uff08Moving Window\uff09\u7b97\u6cd5\u5206\u522b\u8ba1\u7b97\u6bcf\u4e2a\u5bb9\u5668\u7684 CPU \u548c Memory \u5e76\u7ed9\u51fa\u5bf9\u5e94\u7684\u63a8\u8350\u503c \u5e38\u89c1\u95ee\u9898 \u00b6 \u5982\u4f55\u8ba9\u63a8\u8350\u7ed3\u679c\u66f4\u51c6\u786e \u00b6 \u5e94\u7528\u5728\u76d1\u63a7\u7cfb\u7edf\uff08\u6bd4\u5982 Prometheus\uff09\u4e2d\u7684\u5386\u53f2\u6570\u636e\u8d8a\u4e45\uff0c\u63a8\u8350\u7ed3\u679c\u5c31\u8d8a\u51c6\u786e\uff0c\u5efa\u8bae\u751f\u4ea7\u4e0a\u8d85\u8fc7\u4e24\u5468\u65f6\u95f4\u3002\u5bf9\u65b0\u5efa\u5e94\u7528\u7684\u9884\u6d4b\u5f80\u5f80\u4e0d\u51c6\uff0c\u53ef\u4ee5\u901a\u8fc7\u53c2\u6570\u914d\u7f6e\u4fdd\u8bc1\u53ea\u5bf9\u5386\u53f2\u6570\u636e\u957f\u5ea6\u8d85\u8fc7\u4e00\u5b9a\u5929\u6570\u7684\u4e1a\u52a1\u63a8\u8350\u3002","title":"\u8d44\u6e90\u63a8\u8350"},{"location":"zh/tutorials/resource-recommendation/#_1","text":"Kubernetes \u7528\u6237\u5728\u521b\u5efa\u5e94\u7528\u8d44\u6e90\u65f6\u5e38\u5e38\u662f\u57fa\u4e8e\u7ecf\u9a8c\u503c\u6765\u8bbe\u7f6e request \u548c limit\u3002\u901a\u8fc7\u8d44\u6e90\u63a8\u8350\u7684\u7b97\u6cd5\u5206\u6790\u5e94\u7528\u7684\u771f\u5b9e\u7528\u91cf\u63a8\u8350\u66f4\u5408\u9002\u7684\u8d44\u6e90\u914d\u7f6e\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003\u5e76\u91c7\u7eb3\u5b83\u63d0\u5347\u96c6\u7fa4\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002","title":"\u8d44\u6e90\u63a8\u8350"},{"location":"zh/tutorials/resource-recommendation/#_2","text":"\u8d44\u6e90\u63a8\u8350\u662f VPA \u7684\u8f7b\u91cf\u5316\u5b9e\u73b0\uff0c\u4e14\u66f4\u7075\u6d3b\u3002 \u7b97\u6cd5\uff1a\u7b97\u6cd5\u6a21\u578b\u91c7\u7528\u4e86 VPA \u7684\u6ed1\u52a8\u7a97\u53e3\uff08Moving Window\uff09\u7b97\u6cd5\uff0c\u5e76\u4e14\u652f\u6301\u81ea\u5b9a\u4e49\u7b97\u6cd5\u7684\u5173\u952e\u914d\u7f6e\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027 \u652f\u6301\u6279\u91cf\u5206\u6790\uff1a\u901a\u8fc7 Analytics \u7684 ResourceSelector\uff0c\u7528\u6237\u53ef\u4ee5\u6279\u91cf\u5206\u6790\u591a\u4e2a\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u800c\u65e0\u9700\u4e00\u4e2a\u4e00\u4e2a\u7684\u521b\u5efa VPA \u5bf9\u8c61 \u66f4\u8f7b\u4fbf\uff1a\u7531\u4e8e VPA \u7684 Auto \u6a21\u5f0f\u5728\u66f4\u65b0\u5bb9\u5668\u8d44\u6e90\u914d\u7f6e\u65f6\u4f1a\u5bfc\u81f4\u5bb9\u5668\u91cd\u5efa\uff0c\u56e0\u6b64\u5f88\u96be\u5728\u751f\u4ea7\u4e0a\u4f7f\u7528\u81ea\u52a8\u6a21\u5f0f\uff0c\u8d44\u6e90\u63a8\u8350\u7ed9\u7528\u6237\u63d0\u4f9b\u8d44\u6e90\u5efa\u8bae\uff0c\u628a\u53d8\u66f4\u7684\u51b3\u5b9a\u4ea4\u7ed9\u7528\u6237\u51b3\u5b9a","title":"\u4ea7\u54c1\u529f\u80fd"},{"location":"zh/tutorials/resource-recommendation/#_3","text":"\u6211\u4eec\u901a\u8fc7 deployment: nginx \u548c Analytics \u4f5c\u4e3a\u4e00\u4e2a\u4f8b\u5b50\u6f14\u793a\u5982\u4f55\u5f00\u59cb\u4e00\u6b21\u8d44\u6e90\u63a8\u8350\u4e4b\u65c5\uff1a Main Mirror kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/nginx-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/gocrane/crane/main/examples/analytics/analytics-resource.yaml kubectl get analytics kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/nginx-deployment.yaml?download = false kubectl apply -f https://finops.coding.net/p/gocrane/d/crane/git/raw/main/examples/analytics/analytics-resource.yaml?download = false kubectl get analytics analytics-resource.yaml apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-resource spec : type : Resource # This can only be \"Resource\" or \"HPA\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 name : nginx-deployment \u7ed3\u679c\u5982\u4e0b: NAME AGE nginx-resource 16m \u67e5\u770b Analytics \u8be6\u60c5: kubectl get analytics nginx-resource -o yaml \u7ed3\u679c\u5982\u4e0b: apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : nginx-resource namespace : default spec : completionStrategy : completionStrategyType : Periodical periodSeconds : 86400 resourceSelectors : - apiVersion : apps/v1 kind : Deployment labelSelector : {} name : nginx-deployment type : Resource status : conditions : - lastTransitionTime : \"2022-05-15T14:38:35Z\" message : Analytics is ready reason : AnalyticsReady status : \"True\" type : Ready lastUpdateTime : \"2022-05-15T14:38:35Z\" recommendations : - lastStartTime : \"2022-05-15T14:38:35Z\" message : Success name : nginx-resource-resource-w45nq namespace : default targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default uid : 750cb3bd-0b87-4f87-acbe-57e621af0a1e","title":"\u521b\u5efa\u8d44\u6e90\u5206\u6790"},{"location":"zh/tutorials/resource-recommendation/#_4","text":"\u67e5\u770b\u5206\u6790\u7ed3\u679c Recommendation \uff1a kubectl get recommend -l analysis.crane.io/analytics-name = nginx-resource -o yaml \u5206\u6790\u7ed3\u679c\u5982\u4e0b\uff1a apiVersion : v1 items : - apiVersion : analysis.crane.io/v1alpha1 kind : Recommendation metadata : creationTimestamp : \"2022-06-15T15:26:25Z\" generateName : nginx-resource-resource- generation : 1 labels : analysis.crane.io/analytics-name : nginx-resource analysis.crane.io/analytics-type : Resource analysis.crane.io/analytics-uid : 9e78964b-f8ae-40de-9740-f9a715d16280 app : nginx name : nginx-resource-resource-t4xpn namespace : default ownerReferences : - apiVersion : analysis.crane.io/v1alpha1 blockOwnerDeletion : false controller : false kind : Analytics name : nginx-resource uid : 9e78964b-f8ae-40de-9740-f9a715d16280 resourceVersion : \"2117439429\" selfLink : /apis/analysis.crane.io/v1alpha1/namespaces/default/recommendations/nginx-resource-resource-t4xpn uid : 8005e3e0-8fe9-470b-99cf-5ce9dd407529 spec : adoptionType : StatusAndAnnotation completionStrategy : completionStrategyType : Once targetRef : apiVersion : apps/v1 kind : Deployment name : nginx-deployment namespace : default type : Resource status : recommendedValue : | resourceRequest: containers: - containerName: nginx target: cpu: 100m memory: 100Mi kind : List metadata : resourceVersion : \"\" selfLink : \"\"","title":"\u67e5\u770b\u5206\u6790\u7ed3\u679c"},{"location":"zh/tutorials/resource-recommendation/#_5","text":"\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u4f8b\u5b50\u6765\u6f14\u793a\u5982\u4f55\u4f7f\u7528 Analytics \u63a8\u8350\u96c6\u7fa4\u4e2d\u6240\u6709\u7684 Deployment \u548c StatefulSet\uff1a apiVersion : analysis.crane.io/v1alpha1 kind : Analytics metadata : name : workload-resource namespace : crane-system # The Analytics in Crane-system will select all resource across all namespaces. spec : type : Resource # This can only be \"Resource\" or \"Replicas\". completionStrategy : completionStrategyType : Periodical # This can only be \"Once\" or \"Periodical\". periodSeconds : 86400 # analytics selected resources every 1 day resourceSelectors : # defines all the resources to be select with - kind : Deployment apiVersion : apps/v1 - kind : StatefulSet apiVersion : apps/v1 \u5f53 namespace \u7b49\u4e8e crane-system \u65f6\uff0c Analytics \u9009\u62e9\u7684\u8d44\u6e90\u662f\u96c6\u7fa4\u4e2d\u6240\u6709\u7684 namespace\uff0c\u5f53 namespace \u4e0d\u7b49\u4e8e crane-system \u65f6\uff0c Analytics \u9009\u62e9 Analytics namespace \u4e0b\u7684\u8d44\u6e90 resourceSelectors \u901a\u8fc7\u6570\u7ec4\u914d\u7f6e\u9700\u8981\u5206\u6790\u7684\u8d44\u6e90\uff0ckind \u548c apiVersion \u662f\u5fc5\u586b\u5b57\u6bb5\uff0cname \u9009\u586b resourceSelectors \u652f\u6301\u914d\u7f6e\u4efb\u610f\u652f\u6301 Scale Subresource \u7684\u8d44\u6e90","title":"\u6279\u91cf\u63a8\u8350"},{"location":"zh/tutorials/resource-recommendation/#_6","text":"","title":"\u8d44\u6e90\u63a8\u8350\u8ba1\u7b97\u6a21\u578b"},{"location":"zh/tutorials/resource-recommendation/#_7","text":"\u6ca1\u6709 Pod \u7684\u5de5\u4f5c\u8d1f\u8f7d: \u5982\u679c\u5de5\u4f5c\u8d1f\u8f7d\u6ca1\u6709 Pod\uff0c\u65e0\u6cd5\u8fdb\u884c\u7b97\u6cd5\u5206\u6790\u3002","title":"\u7b5b\u9009\u9636\u6bb5"},{"location":"zh/tutorials/resource-recommendation/#_8","text":"\u91c7\u7528 VPA \u7684\u6ed1\u52a8\u7a97\u53e3\uff08Moving Window\uff09\u7b97\u6cd5\u5206\u522b\u8ba1\u7b97\u6bcf\u4e2a\u5bb9\u5668\u7684 CPU \u548c Memory \u5e76\u7ed9\u51fa\u5bf9\u5e94\u7684\u63a8\u8350\u503c","title":"\u63a8\u8350"},{"location":"zh/tutorials/resource-recommendation/#_9","text":"","title":"\u5e38\u89c1\u95ee\u9898"},{"location":"zh/tutorials/resource-recommendation/#_10","text":"\u5e94\u7528\u5728\u76d1\u63a7\u7cfb\u7edf\uff08\u6bd4\u5982 Prometheus\uff09\u4e2d\u7684\u5386\u53f2\u6570\u636e\u8d8a\u4e45\uff0c\u63a8\u8350\u7ed3\u679c\u5c31\u8d8a\u51c6\u786e\uff0c\u5efa\u8bae\u751f\u4ea7\u4e0a\u8d85\u8fc7\u4e24\u5468\u65f6\u95f4\u3002\u5bf9\u65b0\u5efa\u5e94\u7528\u7684\u9884\u6d4b\u5f80\u5f80\u4e0d\u51c6\uff0c\u53ef\u4ee5\u901a\u8fc7\u53c2\u6570\u914d\u7f6e\u4fdd\u8bc1\u53ea\u5bf9\u5386\u53f2\u6570\u636e\u957f\u5ea6\u8d85\u8fc7\u4e00\u5b9a\u5929\u6570\u7684\u4e1a\u52a1\u63a8\u8350\u3002","title":"\u5982\u4f55\u8ba9\u63a8\u8350\u7ed3\u679c\u66f4\u51c6\u786e"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/","text":"Crane-scheduler \u00b6 \u6982\u8ff0 \u00b6 Crane-scheduler \u662f\u4e00\u7ec4\u57fa\u4e8e scheduler framework \u7684\u8c03\u5ea6\u63d2\u4ef6\uff0c \u5305\u542b\uff1a Dynamic scheduler\uff1a\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u5668\u63d2\u4ef6 \u5f00\u59cb \u00b6 \u5b89\u88c5 Prometheus \u00b6 \u786e\u4fdd\u4f60\u7684 Kubernetes \u96c6\u7fa4\u5df2\u5b89\u88c5 Prometheus\u3002\u5982\u679c\u6ca1\u6709\uff0c\u8bf7\u53c2\u8003 Install Prometheus . \u914d\u7f6e Prometheus \u89c4\u5219 \u00b6 \u914d\u7f6e Prometheus \u7684\u89c4\u5219\u4ee5\u83b7\u53d6\u9884\u671f\u7684\u805a\u5408\u6570\u636e\uff1a apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : name : example-record spec : groups : - name : cpu_mem_usage_active interval : 30s rules : - record : cpu_usage_active expr : 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[30s])) * 100) - record : mem_usage_active expr : 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) - name : cpu-usage-5m interval : 5m rules : - record : cpu_usage_max_avg_1h expr : max_over_time(cpu_usage_avg_5m[1h]) - record : cpu_usage_max_avg_1d expr : max_over_time(cpu_usage_avg_5m[1d]) - name : cpu-usage-1m interval : 1m rules : - record : cpu_usage_avg_5m expr : avg_over_time(cpu_usage_active[5m]) - name : mem-usage-5m interval : 5m rules : - record : mem_usage_max_avg_1h expr : max_over_time(mem_usage_avg_5m[1h]) - record : mem_usage_max_avg_1d expr : max_over_time(mem_usage_avg_5m[1d]) - name : mem-usage-1m interval : 1m rules : - record : mem_usage_avg_5m expr : avg_over_time(mem_usage_active[5m]) \ufe0fTroubleshooting Prometheus \u7684\u91c7\u6837\u95f4\u9694\u5fc5\u987b\u5c0f\u4e8e30\u79d2\uff0c\u4e0d\u7136\u53ef\u80fd\u4f1a\u5bfc\u81f4\u89c4\u5219\u65e0\u6cd5\u6b63\u5e38\u751f\u6548\u3002\u5982\uff1a cpu_usage_active \u3002 2. \u66f4\u65b0 Prometheus \u670d\u52a1\u53d1\u73b0\u7684\u914d\u7f6e\uff0c\u786e\u4fdd node_exporters/telegraf \u6b63\u5728\u4f7f\u7528\u8282\u70b9\u540d\u79f0\u4f5c\u4e3a\u5b9e\u4f8b\u540d\u79f0\uff1a - job_name : kubernetes-node-exporter tls_config : ca_file : /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify : true bearer_token_file : /var/run/secrets/kubernetes.io/serviceaccount/token scheme : https kubernetes_sd_configs : ... # Host name - source_labels : [ __meta_kubernetes_node_name ] target_label : instance ... Note \u5982\u679c\u8282\u70b9\u540d\u79f0\u662f\u672c\u673aIP\uff0c\u5219\u53ef\u4ee5\u8df3\u8fc7\u6b64\u6b65\u9aa4\u3002 \u5b89\u88c5 Crane-scheduler \u00b6 \u6709\u4e24\u79cd\u9009\u62e9\uff1a \u5b89\u88c5 Crane-scheduler \u4f5c\u4e3a\u7b2c\u4e8c\u4e2a\u8c03\u5ea6\u5668 \u7528 Crane-scheduler \u66ff\u6362\u539f\u751f Kube-scheduler \u5b89\u88c5 Crane-scheduler \u4f5c\u4e3a\u7b2c\u4e8c\u4e2a\u8c03\u5ea6\u5668 \u00b6 Main Mirror helm repo add crane https://gocrane.github.io/helm-charts helm install scheduler -n crane-system --create-namespace --set global.prometheusAddr = \"REPLACE_ME_WITH_PROMETHEUS_ADDR\" crane/scheduler helm repo add crane https://finops-helm.pkg.coding.net/gocrane/gocrane helm install scheduler -n crane-system --create-namespace --set global.prometheusAddr = \"REPLACE_ME_WITH_PROMETHEUS_ADDR\" crane/scheduler \u7528 Crane-scheduler \u66ff\u6362\u539f\u751f Kube-scheduler \u00b6 \u5907\u4efd /etc/kubernetes/manifests/kube-scheduler.yaml cp /etc/kubernetes/manifests/kube-scheduler.yaml /etc/kubernetes/ \u901a\u8fc7\u4fee\u6539 kube-scheduler \u7684\u914d\u7f6e\u6587\u4ef6\uff08 scheduler-config.yaml ) \u542f\u7528\u52a8\u6001\u8c03\u5ea6\u63d2\u4ef6\u5e76\u914d\u7f6e\u63d2\u4ef6\u53c2\u6570\uff1a scheduler-config.yaml apiVersion : kubescheduler.config.k8s.io/v1beta2 kind : KubeSchedulerConfiguration ... profiles : - schedulerName : default-scheduler plugins : filter : enabled : - name : Dynamic score : enabled : - name : Dynamic weight : 3 pluginConfig : - name : Dynamic args : policyConfigPath : /etc/kubernetes/policy.yaml ... \u65b0\u5efa /etc/kubernetes/policy.yaml \uff0c\u7528\u4f5c\u52a8\u6001\u63d2\u4ef6\u7684\u8c03\u5ea6\u7b56\u7565\uff1a /etc/kubernetes/policy.yaml apiVersion : scheduler.policy.crane.io/v1alpha1 kind : DynamicSchedulerPolicy spec : syncPolicy : ##cpu usage - name : cpu_usage_avg_5m period : 3m - name : cpu_usage_max_avg_1h period : 15m - name : cpu_usage_max_avg_1d period : 3h ##memory usage - name : mem_usage_avg_5m period : 3m - name : mem_usage_max_avg_1h period : 15m - name : mem_usage_max_avg_1d period : 3h predicate : ##cpu usage - name : cpu_usage_avg_5m maxLimitPecent : 0.65 - name : cpu_usage_max_avg_1h maxLimitPecent : 0.75 ##memory usage - name : mem_usage_avg_5m maxLimitPecent : 0.65 - name : mem_usage_max_avg_1h maxLimitPecent : 0.75 priority : ##cpu usage - name : cpu_usage_avg_5m weight : 0.2 - name : cpu_usage_max_avg_1h weight : 0.3 - name : cpu_usage_max_avg_1d weight : 0.5 ##memory usage - name : mem_usage_avg_5m weight : 0.2 - name : mem_usage_max_avg_1h weight : 0.3 - name : mem_usage_max_avg_1d weight : 0.5 hotValue : - timeRange : 5m count : 5 - timeRange : 1m count : 2 \u4fee\u6539 kube-scheduler.yaml \u5e76\u7528 Crane-scheduler\u7684\u955c\u50cf\u66ff\u6362 kube-scheduler \u955c\u50cf\uff1a kube-scheduler.yaml ... image : docker.io/gocrane/crane-scheduler:0.0.23 ... \u5b89\u88c5 crane-scheduler-controller \uff1a Main Mirror kubectl apply -f https://raw.githubusercontent.com/gocrane/crane-scheduler/main/deploy/controller/rbac.yaml kubectl apply -f https://raw.githubusercontent.com/gocrane/crane-scheduler/main/deploy/controller/deployment.yaml kubectl apply -f https://finops.coding.net/p/gocrane/d/crane-scheduler/git/raw/main/deploy/controller/rbac.yaml?download = false kubectl apply -f https://finops.coding.net/p/gocrane/d/crane-scheduler/git/raw/main/deploy/controller/deployment.yaml?download = false \u4f7f\u7528 Crane-scheduler \u8c03\u5ea6 Pod \u00b6 \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u6d4b\u8bd5 Crane-scheduler \uff1a apiVersion : apps/v1 kind : Deployment metadata : name : cpu-stress spec : selector : matchLabels : app : cpu-stress replicas : 1 template : metadata : labels : app : cpu-stress spec : schedulerName : crane-scheduler hostNetwork : true tolerations : - key : node.kubernetes.io/network-unavailable operator : Exists effect : NoSchedule containers : - name : stress image : docker.io/gocrane/stress:latest command : [ \"stress\" , \"-c\" , \"1\" ] resources : requests : memory : \"1Gi\" cpu : \"1\" limits : memory : \"1Gi\" cpu : \"1\" Note \u5982\u679c\u60f3\u5c06 crane-scheduler \u7528\u4f5c\u9ed8\u8ba4\u8c03\u5ea6\u5668\uff0c\u8bf7\u5c06 crane-scheduler \u66f4\u6539\u4e3a default-scheduler \u3002 \u5982\u679c\u6d4b\u8bd5 pod \u8c03\u5ea6\u6210\u529f\uff0c\u5c06\u4f1a\u6709\u4ee5\u4e0b\u4e8b\u4ef6\uff1a Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 28s crane-scheduler Successfully assigned default/cpu-stress-7669499b57-zmrgb to vm-162-247-ubuntu","title":"Load-aware Scheduling"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#crane-scheduler","text":"","title":"Crane-scheduler"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#_1","text":"Crane-scheduler \u662f\u4e00\u7ec4\u57fa\u4e8e scheduler framework \u7684\u8c03\u5ea6\u63d2\u4ef6\uff0c \u5305\u542b\uff1a Dynamic scheduler\uff1a\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u5668\u63d2\u4ef6","title":"\u6982\u8ff0"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#_2","text":"","title":"\u5f00\u59cb"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#prometheus","text":"\u786e\u4fdd\u4f60\u7684 Kubernetes \u96c6\u7fa4\u5df2\u5b89\u88c5 Prometheus\u3002\u5982\u679c\u6ca1\u6709\uff0c\u8bf7\u53c2\u8003 Install Prometheus .","title":"\u5b89\u88c5 Prometheus"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#prometheus_1","text":"\u914d\u7f6e Prometheus \u7684\u89c4\u5219\u4ee5\u83b7\u53d6\u9884\u671f\u7684\u805a\u5408\u6570\u636e\uff1a apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : name : example-record spec : groups : - name : cpu_mem_usage_active interval : 30s rules : - record : cpu_usage_active expr : 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[30s])) * 100) - record : mem_usage_active expr : 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) - name : cpu-usage-5m interval : 5m rules : - record : cpu_usage_max_avg_1h expr : max_over_time(cpu_usage_avg_5m[1h]) - record : cpu_usage_max_avg_1d expr : max_over_time(cpu_usage_avg_5m[1d]) - name : cpu-usage-1m interval : 1m rules : - record : cpu_usage_avg_5m expr : avg_over_time(cpu_usage_active[5m]) - name : mem-usage-5m interval : 5m rules : - record : mem_usage_max_avg_1h expr : max_over_time(mem_usage_avg_5m[1h]) - record : mem_usage_max_avg_1d expr : max_over_time(mem_usage_avg_5m[1d]) - name : mem-usage-1m interval : 1m rules : - record : mem_usage_avg_5m expr : avg_over_time(mem_usage_active[5m]) \ufe0fTroubleshooting Prometheus \u7684\u91c7\u6837\u95f4\u9694\u5fc5\u987b\u5c0f\u4e8e30\u79d2\uff0c\u4e0d\u7136\u53ef\u80fd\u4f1a\u5bfc\u81f4\u89c4\u5219\u65e0\u6cd5\u6b63\u5e38\u751f\u6548\u3002\u5982\uff1a cpu_usage_active \u3002 2. \u66f4\u65b0 Prometheus \u670d\u52a1\u53d1\u73b0\u7684\u914d\u7f6e\uff0c\u786e\u4fdd node_exporters/telegraf \u6b63\u5728\u4f7f\u7528\u8282\u70b9\u540d\u79f0\u4f5c\u4e3a\u5b9e\u4f8b\u540d\u79f0\uff1a - job_name : kubernetes-node-exporter tls_config : ca_file : /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify : true bearer_token_file : /var/run/secrets/kubernetes.io/serviceaccount/token scheme : https kubernetes_sd_configs : ... # Host name - source_labels : [ __meta_kubernetes_node_name ] target_label : instance ... Note \u5982\u679c\u8282\u70b9\u540d\u79f0\u662f\u672c\u673aIP\uff0c\u5219\u53ef\u4ee5\u8df3\u8fc7\u6b64\u6b65\u9aa4\u3002","title":"\u914d\u7f6e Prometheus \u89c4\u5219"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#crane-scheduler_1","text":"\u6709\u4e24\u79cd\u9009\u62e9\uff1a \u5b89\u88c5 Crane-scheduler \u4f5c\u4e3a\u7b2c\u4e8c\u4e2a\u8c03\u5ea6\u5668 \u7528 Crane-scheduler \u66ff\u6362\u539f\u751f Kube-scheduler","title":"\u5b89\u88c5 Crane-scheduler"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#crane-scheduler_2","text":"Main Mirror helm repo add crane https://gocrane.github.io/helm-charts helm install scheduler -n crane-system --create-namespace --set global.prometheusAddr = \"REPLACE_ME_WITH_PROMETHEUS_ADDR\" crane/scheduler helm repo add crane https://finops-helm.pkg.coding.net/gocrane/gocrane helm install scheduler -n crane-system --create-namespace --set global.prometheusAddr = \"REPLACE_ME_WITH_PROMETHEUS_ADDR\" crane/scheduler","title":"\u5b89\u88c5 Crane-scheduler \u4f5c\u4e3a\u7b2c\u4e8c\u4e2a\u8c03\u5ea6\u5668"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#crane-scheduler-kube-scheduler","text":"\u5907\u4efd /etc/kubernetes/manifests/kube-scheduler.yaml cp /etc/kubernetes/manifests/kube-scheduler.yaml /etc/kubernetes/ \u901a\u8fc7\u4fee\u6539 kube-scheduler \u7684\u914d\u7f6e\u6587\u4ef6\uff08 scheduler-config.yaml ) \u542f\u7528\u52a8\u6001\u8c03\u5ea6\u63d2\u4ef6\u5e76\u914d\u7f6e\u63d2\u4ef6\u53c2\u6570\uff1a scheduler-config.yaml apiVersion : kubescheduler.config.k8s.io/v1beta2 kind : KubeSchedulerConfiguration ... profiles : - schedulerName : default-scheduler plugins : filter : enabled : - name : Dynamic score : enabled : - name : Dynamic weight : 3 pluginConfig : - name : Dynamic args : policyConfigPath : /etc/kubernetes/policy.yaml ... \u65b0\u5efa /etc/kubernetes/policy.yaml \uff0c\u7528\u4f5c\u52a8\u6001\u63d2\u4ef6\u7684\u8c03\u5ea6\u7b56\u7565\uff1a /etc/kubernetes/policy.yaml apiVersion : scheduler.policy.crane.io/v1alpha1 kind : DynamicSchedulerPolicy spec : syncPolicy : ##cpu usage - name : cpu_usage_avg_5m period : 3m - name : cpu_usage_max_avg_1h period : 15m - name : cpu_usage_max_avg_1d period : 3h ##memory usage - name : mem_usage_avg_5m period : 3m - name : mem_usage_max_avg_1h period : 15m - name : mem_usage_max_avg_1d period : 3h predicate : ##cpu usage - name : cpu_usage_avg_5m maxLimitPecent : 0.65 - name : cpu_usage_max_avg_1h maxLimitPecent : 0.75 ##memory usage - name : mem_usage_avg_5m maxLimitPecent : 0.65 - name : mem_usage_max_avg_1h maxLimitPecent : 0.75 priority : ##cpu usage - name : cpu_usage_avg_5m weight : 0.2 - name : cpu_usage_max_avg_1h weight : 0.3 - name : cpu_usage_max_avg_1d weight : 0.5 ##memory usage - name : mem_usage_avg_5m weight : 0.2 - name : mem_usage_max_avg_1h weight : 0.3 - name : mem_usage_max_avg_1d weight : 0.5 hotValue : - timeRange : 5m count : 5 - timeRange : 1m count : 2 \u4fee\u6539 kube-scheduler.yaml \u5e76\u7528 Crane-scheduler\u7684\u955c\u50cf\u66ff\u6362 kube-scheduler \u955c\u50cf\uff1a kube-scheduler.yaml ... image : docker.io/gocrane/crane-scheduler:0.0.23 ... \u5b89\u88c5 crane-scheduler-controller \uff1a Main Mirror kubectl apply -f https://raw.githubusercontent.com/gocrane/crane-scheduler/main/deploy/controller/rbac.yaml kubectl apply -f https://raw.githubusercontent.com/gocrane/crane-scheduler/main/deploy/controller/deployment.yaml kubectl apply -f https://finops.coding.net/p/gocrane/d/crane-scheduler/git/raw/main/deploy/controller/rbac.yaml?download = false kubectl apply -f https://finops.coding.net/p/gocrane/d/crane-scheduler/git/raw/main/deploy/controller/deployment.yaml?download = false","title":"\u7528 Crane-scheduler \u66ff\u6362\u539f\u751f Kube-scheduler"},{"location":"zh/tutorials/scheduling-pods-based-on-actual-node-load/#crane-scheduler-pod","text":"\u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u6d4b\u8bd5 Crane-scheduler \uff1a apiVersion : apps/v1 kind : Deployment metadata : name : cpu-stress spec : selector : matchLabels : app : cpu-stress replicas : 1 template : metadata : labels : app : cpu-stress spec : schedulerName : crane-scheduler hostNetwork : true tolerations : - key : node.kubernetes.io/network-unavailable operator : Exists effect : NoSchedule containers : - name : stress image : docker.io/gocrane/stress:latest command : [ \"stress\" , \"-c\" , \"1\" ] resources : requests : memory : \"1Gi\" cpu : \"1\" limits : memory : \"1Gi\" cpu : \"1\" Note \u5982\u679c\u60f3\u5c06 crane-scheduler \u7528\u4f5c\u9ed8\u8ba4\u8c03\u5ea6\u5668\uff0c\u8bf7\u5c06 crane-scheduler \u66f4\u6539\u4e3a default-scheduler \u3002 \u5982\u679c\u6d4b\u8bd5 pod \u8c03\u5ea6\u6210\u529f\uff0c\u5c06\u4f1a\u6709\u4ee5\u4e0b\u4e8b\u4ef6\uff1a Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 28s crane-scheduler Successfully assigned default/cpu-stress-7669499b57-zmrgb to vm-162-247-ubuntu","title":"\u4f7f\u7528 Crane-scheduler \u8c03\u5ea6 Pod"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/","text":"EffectiveHorizontalPodAutoscaler \u00b6 EffectiveHorizontalPodAutoscaler\uff08\u7b80\u79f0 EHPA\uff09\u662f Crane \u63d0\u4f9b\u7684\u5f39\u6027\u4f38\u7f29\u4ea7\u54c1\uff0c\u5b83\u57fa\u4e8e\u793e\u533a HPA \u505a\u5e95\u5c42\u7684\u5f39\u6027\u63a7\u5236\uff0c\u652f\u6301\u66f4\u4e30\u5bcc\u7684\u5f39\u6027\u89e6\u53d1\u7b56\u7565\uff08\u9884\u6d4b\uff0c\u89c2\u6d4b\uff0c\u5468\u671f\uff09\uff0c\u8ba9\u5f39\u6027\u66f4\u52a0\u9ad8\u6548\uff0c\u5e76\u4fdd\u969c\u4e86\u670d\u52a1\u7684\u8d28\u91cf\u3002 \u63d0\u524d\u6269\u5bb9\uff0c\u4fdd\u8bc1\u670d\u52a1\u8d28\u91cf\uff1a\u901a\u8fc7\u7b97\u6cd5\u9884\u6d4b\u672a\u6765\u7684\u6d41\u91cf\u6d2a\u5cf0\u63d0\u524d\u6269\u5bb9\uff0c\u907f\u514d\u6269\u5bb9\u4e0d\u53ca\u65f6\u5bfc\u81f4\u7684\u96ea\u5d29\u548c\u670d\u52a1\u7a33\u5b9a\u6027\u6545\u969c\u3002 \u51cf\u5c11\u65e0\u6548\u7f29\u5bb9\uff1a\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u53ef\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u7f29\u5bb9\uff0c\u7a33\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8d44\u6e90\u4f7f\u7528\u7387\uff0c\u6d88\u9664\u7a81\u523a\u8bef\u5224\u3002 \u652f\u6301 Cron \u914d\u7f6e\uff1a\u652f\u6301 Cron-based \u5f39\u6027\u914d\u7f6e\uff0c\u5e94\u5bf9\u5927\u4fc3\u7b49\u5f02\u5e38\u6d41\u91cf\u6d2a\u5cf0\u3002 \u517c\u5bb9\u793e\u533a\uff1a\u4f7f\u7528\u793e\u533a HPA \u4f5c\u4e3a\u5f39\u6027\u63a7\u5236\u7684\u6267\u884c\u5c42\uff0c\u80fd\u529b\u5b8c\u5168\u517c\u5bb9\u793e\u533a\u3002 \u4ea7\u54c1\u529f\u80fd \u00b6 \u4e00\u4e2a\u7b80\u5355\u7684 EHPA yaml \u6587\u4ef6\u5982\u4e0b\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache spec : scaleTargetRef : #(1) apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 #(2) maxReplicas : 10 #(3) scaleStrategy : Auto #(4) metrics : #(5) - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 prediction : #(6) predictionWindowSeconds : 3600 #(7) predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" ScaleTargetRef \u914d\u7f6e\u4f60\u5e0c\u671b\u5f39\u6027\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3002 MinReplicas \u6307\u5b9a\u4e86\u81ea\u52a8\u7f29\u5bb9\u7684\u6700\u5c0f\u503c\u3002 MaxReplicas \u6307\u5b9a\u4e86\u81ea\u52a8\u6269\u5bb9\u7684\u6700\u5927\u503c\u3002 ScaleStrategy \u5b9a\u4e49\u4e86\u5f39\u6027\u7684\u7b56\u7565\uff0c\u503c\u53ef\u4ee5\u662f \"Auto\" and \"Preview\". Metrics \u5b9a\u4e49\u4e86\u5f39\u6027\u9608\u503c\u914d\u7f6e\u3002 Prediction \u5b9a\u4e49\u4e86\u9884\u6d4b\u7b97\u6cd5\u914d\u7f6e\u3002 PredictionWindowSeconds \u6307\u5b9a\u5f80\u540e\u9884\u6d4b\u591a\u4e45\u7684\u6570\u636e\u3002 \u57fa\u4e8e\u9884\u6d4b\u7684\u5f39\u6027 \u00b6 \u5927\u591a\u6570\u5728\u7ebf\u5e94\u7528\u7684\u8d1f\u8f7d\u90fd\u6709\u5468\u671f\u6027\u7684\u7279\u5f81\u3002\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u6309\u5929\u6216\u8005\u6309\u5468\u7684\u8d8b\u52bf\u9884\u6d4b\u672a\u6765\u7684\u8d1f\u8f7d\u3002EHPA \u4f7f\u7528 DSP \u7b97\u6cd5\u6765\u9884\u6d4b\u5e94\u7528\u672a\u6765\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e2a\u5f00\u542f\u4e86\u9884\u6d4b\u80fd\u529b\u7684 EHPA \u6a21\u7248\u4f8b\u5b50\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : prediction : predictionWindowSeconds : 3600 predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" \u76d1\u63a7\u6570\u636e\u515c\u5e95 \u00b6 \u5728\u4f7f\u7528\u9884\u6d4b\u7b97\u6cd5\u9884\u6d4b\u65f6\uff0c\u4f60\u53ef\u80fd\u4f1a\u62c5\u5fc3\u9884\u6d4b\u6570\u636e\u4e0d\u51c6\u5e26\u6765\u4e00\u5b9a\u7684\u98ce\u9669\uff0cEHPA \u5728\u8ba1\u7b97\u526f\u672c\u6570\u65f6\uff0c\u4e0d\u4ec5\u4f1a\u6309\u9884\u6d4b\u6570\u636e\u8ba1\u7b97\uff0c\u540c\u65f6\u4e5f\u4f1a\u8003\u8651\u5b9e\u9645\u76d1\u63a7\u6570\u636e\u6765\u515c\u5e95\uff0c\u63d0\u5347\u4e86\u5f39\u6027\u7684\u5b89\u5168\u6027\u3002 \u5b9e\u73b0\u7684\u539f\u7406\u662f\u5f53\u4f60\u5728 EHPA \u4e2d\u5b9a\u4e49 spec.metrics \u5e76\u4e14\u5f00\u542f\u5f39\u6027\u9884\u6d4b\u65f6\uff0cEffectiveHPAController \u4f1a\u5728\u521b\u5efa\u5e95\u5c42\u7ba1\u7406\u7684 HPA \u65f6\u6309\u7b56\u7565\u81ea\u52a8\u751f\u6210\u591a\u6761 Metric Spec\u3002 \u4f8b\u5982\uff0c\u5f53\u7528\u6237\u5728 EHPA \u7684 yaml \u91cc\u5b9a\u4e49\u5982\u4e0b Metric Spec\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 \u5b83\u4f1a\u81ea\u52a8\u8f6c\u6362\u6210\u4e24\u6761 HPA \u7684\u9608\u503c\u914d\u7f6e\uff1a apiVersion : autoscaling/v2beta1 kind : HorizontalPodAutoscaler spec : metrics : - pods : metric : name : crane_pod_cpu_usage selector : matchLabels : autoscaling.crane.io/effective-hpa-uid : f9b92249-eab9-4671-afe0-17925e5987b8 target : type : AverageValue averageValue : 100m type : Pods - resource : name : cpu target : type : Utilization averageUtilization : 50 type : Resource \u5728\u4e0a\u9762\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u7528\u6237\u5728 EHPA \u521b\u5efa\u7684 Metric \u9608\u503c\u914d\u7f6e\u4f1a\u81ea\u52a8\u8f6c\u6362\u6210\u5e95\u5c42 HPA \u4e0a\u7684\u4e24\u6761 Metric \u9608\u503c\u914d\u7f6e\uff1a\u9884\u6d4b Metric \u9608\u503c\u548c\u5b9e\u9645\u76d1\u63a7 Metric \u9608\u503c \u9884\u6d4b Metric \u9608\u503c \u662f\u4e00\u4e2a custom metric\u3002\u503c\u901a\u8fc7 Crane \u7684 MetricAdapter \u63d0\u4f9b\u3002 \u5b9e\u9645\u76d1\u63a7 Metric \u9608\u503c \u662f\u4e00\u4e2a resource metric\uff0c\u5b83\u548c\u7528\u6237\u5728 EHPA \u4e0a\u5b9a\u4e49\u7684\u4e00\u6837\u3002\u8fd9\u6837 HPA \u4f1a\u6839\u636e\u5e94\u7528\u5b9e\u9645\u76d1\u63a7\u7684 Metric \u8ba1\u7b97\u526f\u672c\u6570\u3002 HPA \u5728\u914d\u7f6e\u4e86\u591a\u4e2a\u5f39\u6027 Metric \u9608\u503c\u65f6\uff0c\u5728\u8ba1\u7b97\u526f\u672c\u6570\u65f6\u4f1a\u5206\u522b\u8ba1\u7b97\u6bcf\u6761 Metric \u5bf9\u5e94\u7684\u526f\u672c\u6570\uff0c\u5e76\u9009\u62e9 \u6700\u5927 \u7684\u90a3\u4e2a\u526f\u672c\u6570\u4f5c\u4e3a\u6700\u7ec8\u7684\u63a8\u8350\u5f39\u6027\u7ed3\u679c\u3002 \u6c34\u5e73\u5f39\u6027\u7684\u6267\u884c\u6d41\u7a0b \u00b6 EffectiveHPAController \u521b\u5efa HorizontalPodAutoscaler \u548c TimeSeriesPrediction \u5bf9\u8c61 PredictionCore \u4ece prometheus \u83b7\u53d6\u5386\u53f2 metric \u901a\u8fc7\u9884\u6d4b\u7b97\u6cd5\u8ba1\u7b97\uff0c\u5c06\u7ed3\u679c\u8bb0\u5f55\u5230 TimeSeriesPrediction HPAController \u901a\u8fc7 metric client \u4ece KubeApiServer \u8bfb\u53d6 metric \u6570\u636e KubeApiServer \u5c06\u8bf7\u6c42\u8def\u7531\u5230 Crane \u7684 MetricAdapter\u3002 HPAController \u8ba1\u7b97\u6240\u6709\u7684 Metric \u8fd4\u56de\u7684\u7ed3\u679c\u5f97\u5230\u6700\u7ec8\u7684\u5f39\u6027\u526f\u672c\u63a8\u8350\u3002 HPAController \u8c03\u7528 scale API \u5bf9\u76ee\u6807\u5e94\u7528\u6269/\u7f29\u5bb9\u3002 \u6574\u4f53\u6d41\u7a0b\u56fe\u5982\u4e0b\uff1a \u7528\u6237\u6848\u4f8b \u00b6 \u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u751f\u4ea7\u73af\u5883\u7684\u5ba2\u6237\u6848\u4f8b\u6765\u4ecb\u7ecd EHPA \u7684\u843d\u5730\u6548\u679c\u3002 \u6211\u4eec\u5c06\u751f\u4ea7\u4e0a\u7684\u6570\u636e\u5728\u9884\u53d1\u73af\u5883\u91cd\u653e\uff0c\u5bf9\u6bd4\u4f7f\u7528 EHPA \u548c\u793e\u533a\u7684 HPA \u7684\u5f39\u6027\u6548\u679c\u3002 \u4e0b\u56fe\u7684\u7ea2\u7ebf\u662f\u5e94\u7528\u5728\u4e00\u5929\u5185\u7684\u5b9e\u9645 CPU \u4f7f\u7528\u91cf\u66f2\u7ebf\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u57288\u70b9\uff0c12\u70b9\uff0c\u665a\u4e0a8\u70b9\u65f6\u662f\u4f7f\u7528\u9ad8\u5cf0\u3002\u7eff\u7ebf\u662f EHPA \u9884\u6d4b\u7684 CPU \u4f7f\u7528\u91cf\u3002 \u4e0b\u56fe\u662f\u5bf9\u5e94\u7684\u81ea\u52a8\u5f39\u6027\u7684\u526f\u672c\u6570\u66f2\u7ebf\uff0c\u7ea2\u7ebf\u662f\u793e\u533a HPA \u7684\u526f\u672c\u6570\u66f2\u7ebf\uff0c\u7eff\u7ebf\u662f EHPA \u7684\u526f\u672c\u6570\u66f2\u7ebf\u3002 \u53ef\u4ee5\u770b\u5230 EHPA \u5177\u6709\u4ee5\u4e0b\u4f18\u52bf\uff1a \u5728\u6d41\u91cf\u6d2a\u5cf0\u6765\u4e34\u524d\u6269\u5bb9\u3002 \u5f53\u6d41\u91cf\u5148\u964d\u540e\u7acb\u523b\u5347\u65f6\u4e0d\u505a\u65e0\u6548\u7f29\u5bb9\u3002 \u76f8\u6bd4 HPA \u66f4\u5c11\u7684\u5f39\u6027\u6b21\u6570\u5374\u66f4\u9ad8\u6548\u3002 ScaleStrategy \u5f39\u6027\u7b56\u7565 \u00b6 EHPA \u63d0\u4f9b\u4e86\u4e24\u79cd\u5f39\u6027\u7b56\u7565\uff1a Auto \u548c Preview \u3002\u7528\u6237\u53ef\u4ee5\u968f\u65f6\u5207\u6362\u5b83\u5e76\u7acb\u5373\u751f\u6548\u3002 Auto \u00b6 Auto \u7b56\u7565\u4e0b EHPA \u4f1a\u81ea\u52a8\u6267\u884c\u5f39\u6027\u884c\u4e3a\u3002\u9ed8\u8ba4 EHPA \u7684\u7b56\u7565\u662f Auto\u3002\u5728\u8fd9\u4e2a\u6a21\u5f0f\u4e0b EHPA \u4f1a\u521b\u5efa\u4e00\u4e2a\u793e\u533a\u7684 HPA \u5bf9\u8c61\u5e76\u81ea\u52a8\u63a5\u7ba1\u5b83\u7684\u751f\u547d\u5468\u671f\u3002\u6211\u4eec\u4e0d\u5efa\u8bae\u7528\u6237\u4fee\u6539\u6216\u8005\u63a7\u5236\u8fd9\u4e2a\u5e95\u5c42\u7684 HPA \u5bf9\u8c61\uff0c\u5f53 EHPA \u88ab\u5220\u9664\u65f6\uff0c\u5e95\u5c42\u7684 HPA \u5bf9\u8c61\u4e5f\u4f1a\u4e00\u5e76\u5220\u9664\u3002 Preview \u00b6 Preview \u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba9 EHPA \u4e0d\u81ea\u52a8\u6267\u884c\u5f39\u6027\u7684\u80fd\u529b\u3002\u6240\u4ee5\u4f60\u53ef\u4ee5\u901a\u8fc7 EHPA \u7684 desiredReplicas \u5b57\u6bb5\u89c2\u6d4b EHPA \u8ba1\u7b97\u51fa\u7684\u526f\u672c\u6570\u3002\u7528\u6237\u53ef\u4ee5\u968f\u65f6\u5728\u4e24\u4e2a\u6a21\u5f0f\u95f4\u5207\u6362\uff0c\u5f53\u7528\u6237\u5207\u6362\u5230 Preview \u6a21\u5f0f\u65f6\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7 spec.specificReplicas \u8c03\u6574\u5e94\u7528\u7684\u526f\u672c\u6570\uff0c\u5982\u679c spec.specificReplicas \u4e3a\u7a7a\uff0c\u5219\u4e0d\u4f1a\u5bf9\u5e94\u7528\u6267\u884c\u5f39\u6027\uff0c\u4f46\u662f\u4f9d\u7136\u4f1a\u6267\u884c\u526f\u672c\u6570\u7684\u8ba1\u7b97\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e2a\u914d\u7f6e\u6210 Preview \u6a21\u5f0f\u7684 EHPA \u6a21\u7248\u4f8b\u5b50\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : scaleStrategy : Preview # ScaleStrategy indicate the strategy to scaling target, value can be \"Auto\" and \"Preview\". specificReplicas : 5 # SpecificReplicas specify the target replicas. status : expectReplicas : 4 # expectReplicas is the calculated replicas that based on prediction metrics or spec.specificReplicas. currentReplicas : 4 # currentReplicas is actual replicas from target HorizontalPodAutoscaler \u793e\u533a\u517c\u5bb9 \u00b6 EHPA \u4ece\u8bbe\u8ba1\u4e4b\u51fa\u5c31\u5e0c\u671b\u548c\u793e\u533a\u7684 HPA \u517c\u5bb9\uff0c\u56e0\u4e3a\u6211\u4eec\u4e0d\u5e0c\u671b\u91cd\u65b0\u9020\u4e00\u4e2a\u7c7b\u4f3c HPA \u7684\u8f6e\u5b50\uff0cHPA \u5728\u4e0d\u65ad\u6f14\u8fdb\u7684\u8fc7\u7a0b\u5df2\u7ecf\u89e3\u51b3\u4e86\u5f88\u591a\u901a\u7528\u7684\u95ee\u9898\uff0cEHPA \u5e0c\u671b\u5728 HPA \u7684\u57fa\u7840\u4e0a\u63d0\u4f9b\u66f4\u9ad8\u9636\u7684 CRD\uff0cEHPA \u7684\u529f\u80fd\u662f\u793e\u533a HPA \u7684\u8d85\u96c6\u3002 EHPA \u4e5f\u4f1a\u6301\u7eed\u8ddf\u8fdb\u652f\u6301 HPA \u7684\u65b0\u529f\u80fd\u3002 EffectiveHorizontalPodAutoscaler status \u00b6 EHPA \u7684 Status \u5305\u62ec\u4e86\u81ea\u8eab\u7684 Status \u540c\u65f6\u4e5f\u6c47\u805a\u4e86\u5e95\u5c42 HPA \u7684\u90e8\u5206 Status\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e2a EHPA \u7684 Status yaml\u4f8b\u5b50\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler status : conditions : - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : the HPA controller was able to get the target's current scale reason : SucceededGetScale status : \"True\" type : AbleToScale - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : Effective HPA is ready reason : EffectiveHorizontalPodAutoscalerReady status : \"True\" type : Ready currentReplicas : 1 expectReplicas : 0 Cron-based autoscaling \u00b6 EffectiveHorizontalPodAutoscaler \u652f\u6301\u57fa\u4e8e cron \u7684\u81ea\u52a8\u7f29\u653e\u3002 \u9664\u4e86\u57fa\u4e8e\u76d1\u63a7\u6307\u6807\uff0c\u6709\u65f6\u8282\u5047\u65e5\u548c\u5de5\u4f5c\u65e5\u7684\u5de5\u4f5c\u8d1f\u8f7d\u6d41\u91cf\u5b58\u5728\u5dee\u5f02\uff0c\u7b80\u5355\u7684\u9884\u6d4b\u7b97\u6cd5\u53ef\u80fd\u6548\u679c\u4e0d\u4f73\u3002\u7136\u540e\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u5468\u672b cron \u6765\u652f\u6301\u66f4\u5927\u6570\u91cf\u7684\u526f\u672c\u6765\u5f25\u8865\u9884\u6d4b\u7684\u4e0d\u8db3\u3002 \u5bf9\u4e8e\u4e00\u4e9b\u975e web \u6d41\u91cf\u7684\u5e94\u7528\uff0c\u6bd4\u5982\u4e00\u4e9b\u5e94\u7528\u4e0d\u9700\u8981\u5728\u5468\u672b\u4f7f\u7528\uff0c\u53ef\u4ee5\u628a\u5de5\u4f5c\u8d1f\u8f7d\u7684\u526f\u672c\u6570\u51cf\u5c11\u5230 1\uff0c\u4e5f\u53ef\u4ee5\u914d\u7f6e cron \u6765\u964d\u4f4e\u4f60\u7684\u670d\u52a1\u6210\u672c\u3002 \u4ee5\u4e0b\u662f EHPA Spec \u4e2d\u7684 cron \u4e3b\u8981\u5b57\u6bb5\uff1a CronSpec \uff1a\u53ef\u4ee5\u8bbe\u7f6e\u591a\u4e2a cron \u81ea\u52a8\u4f38\u7f29\u914d\u7f6e\uff0ccron cycle \u53ef\u4ee5\u8bbe\u7f6e\u5faa\u73af\u7684\u5f00\u59cb\u65f6\u95f4\u548c\u7ed3\u675f\u65f6\u95f4\uff0c\u5e76\u4e14\u5de5\u4f5c\u8d1f\u8f7d\u7684\u526f\u672c\u6570\u53ef\u4ee5\u5728\u65f6\u95f4\u8303\u56f4\u5185\u6301\u7eed\u4fdd\u6301\u4e3a\u8bbe\u5b9a\u7684\u76ee\u6807\u503c\u3002 Name \uff1acron \u6807\u8bc6\u7b26 TargetReplicas \uff1a\u6b64 cron \u65f6\u95f4\u8303\u56f4\u5185\u5de5\u4f5c\u8d1f\u8f7d\u7684\u76ee\u6807\u526f\u672c\u6570\u3002 Start \uff1acron \u7684\u5f00\u59cb\u65f6\u95f4\uff0c\u6807\u51c6 linux crontab \u683c\u5f0f End \uff1acron \u7684\u7ed3\u675f\u65f6\u95f4\uff0c\u6807\u51c6 linux crontab \u683c\u5f0f \u4e00\u4e9b\u4e91\u5382\u5546\u548c\u793e\u533a\u5f53\u524d\u7684 cron \u81ea\u52a8\u7f29\u653e\u529f\u80fd\u5b58\u5728\u4e00\u4e9b\u7f3a\u70b9\u3002 cron \u80fd\u529b\u5355\u72ec\u63d0\u4f9b\uff0c\u6ca1\u6709\u5728\u5168\u5c40\u89c6\u56fe\u4e2d\u8fdb\u884c\u81ea\u52a8\u7f29\u653e\uff0c\u4e0e HPA \u517c\u5bb9\u6027\u5dee\uff0c\u4e0e\u5176\u4ed6\u7f29\u653e\u89e6\u53d1\u5668\u51b2\u7a81\u3002 cron \u7684\u8bed\u4e49\u548c\u884c\u4e3a\u4e0d\u662f\u5f88\u5339\u914d\uff0c\u4f7f\u7528\u65f6\u751a\u81f3\u5f88\u96be\u7406\u89e3\uff0c\u5f88\u5bb9\u6613\u8bef\u5bfc\u7528\u6237\uff0c\u5bfc\u81f4\u81ea\u52a8\u4f38\u7f29\u5931\u8d25\u3002 \u4e0b\u56fe\u663e\u793a\u4e86\u5f53\u524d EHPA cron \u81ea\u52a8\u4f38\u7f29\u5b9e\u73b0\u4e0e\u5176\u4ed6 cron \u80fd\u529b\u7684\u5bf9\u6bd4\u3002 \u9488\u5bf9\u4ee5\u4e0a\u95ee\u9898\uff0cEHPA \u5b9e\u73b0\u7684 cron autoscaling \u662f\u5728\u4e0e HPA \u517c\u5bb9\u7684\u57fa\u7840\u4e0a\u8bbe\u8ba1\u7684\uff0ccron \u4f5c\u4e3a HPA \u7684\u4e00\u4e2a\u6307\u6807\uff0c\u4e0e\u5176\u4ed6\u6307\u6807\u4e00\u8d77\u4f5c\u7528\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u3002 \u53e6\u5916\uff0ccron \u7684\u8bbe\u7f6e\u4e5f\u5f88\u7b80\u5355\u3002\u5355\u72ec\u914d\u7f6e cron \u65f6\uff0c\u4e0d\u5728\u6d3b\u52a8\u65f6\u95f4\u8303\u56f4\u5185\u65f6\uff0c\u4e0d\u4f1a\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u6267\u884c\u7f29\u653e\u3002 Cron working without other metrics \u00b6 \u5047\u8bbe\u4f60\u6ca1\u6709\u914d\u7f6e\u5176\u4ed6\u6307\u6807\uff0c\u4f60\u53ea\u9700\u914d\u7f6e cron \u672c\u8eab\u5373\u53ef\u5de5\u4f5c\u3002 apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache-local spec : # ScaleTargetRef \u5173\u8054\u5230\u9700\u6269\u7f29\u5bb9\u7684\u5de5\u4f5c\u8d1f\u8f7d scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 # MinReplicas : autoscaler \u7f29\u653e\u7684\u6700\u4f4e\u526f\u672c\u6570 maxReplicas : 100 # MaxReplicas : autoscaler \u7f29\u653e\u7684\u6700\u5927\u526f\u672c\u6570 scaleStrategy : Auto # ScaleStrategy : \u7f29\u653e\u5de5\u4f5c\u8d1f\u8f7d\u65f6\u5019\uff0c\u6240\u91c7\u7528\u7684\u7b56\u7565\u3002\u53ef\u9009\u503c\u4e3a \"Auto\" \"Manual\" # \u6700\u597d\u5c06Cron Scheduling\u8bbe\u7f6e\u4e3a\u4e00\u4e2a\u5b8c\u6574\u7684\u65f6\u95f4\u5468\u671f\uff0c\u4f8b\u5982\uff1a \u4e00\u5929\uff0c\u4e00\u5468 # \u4e0b\u9762\u662f\u4e00\u5929\u7684Cron Scheduling #(targetReplicas) #80 -------- --------- ---------- # | | | | | | #10 ------------ ----- -------- ---------- #(time) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #\u672c\u5730\u65f6\u533a(timezone: \"Local\")\u610f\u5473\u7740\u60a8\u4f7f\u7528\u8fd0\u884cCraned\u6240\u5728\u7684\u670d\u52a1\u5668\uff08\u6216\u8005\u53ef\u80fd\u662f\u5bb9\u5668\uff09\u7684\u65f6\u533a\u3002\u4f8b\u5982\uff0c\u5f53Craned \u662f\u4ee5UTC\u65f6\u533a\u5f00\u59cb\uff0c\u90a3\u4e48\u5b83\u5c31\u662fUTC\u3002\u5982\u679c\u4e00\u5f00\u59cb\u662fAsia/Shanghai\uff0c\u90a3\u4e48\u5b83\u5c31\u662fAsia/Shanghai\u3002 crons : - name : \"cron1\" timezone : \"Local\" description : \"scale down\" start : \"0 0 ? * *\" end : \"0 6 ? * *\" targetReplicas : 10 - name : \"cron2\" timezone : \"Local\" description : \"scale up\" start : \"0 6 ? * *\" end : \"0 9 ? * *\" targetReplicas : 80 - name : \"cron3\" timezone : \"Local\" description : \"scale down\" start : \"00 9 ? * *\" end : \"00 11 ? * *\" targetReplicas : 10 - name : \"cron4\" timezone : \"Local\" description : \"scale up\" start : \"00 11 ? * *\" end : \"00 14 ? * *\" targetReplicas : 80 - name : \"cron5\" timezone : \"Local\" description : \"scale down\" start : \"00 14 ? * *\" end : \"00 17 ? * *\" targetReplicas : 10 - name : \"cron6\" timezone : \"Local\" description : \"scale up\" start : \"00 17 ? * *\" end : \"00 20 ? * *\" targetReplicas : 80 - name : \"cron7\" timezone : \"Local\" description : \"scale down\" start : \"00 20 ? * *\" end : \"00 00 ? * *\" targetReplicas : 10 CronSpec \u5177\u6709\u4ee5\u4e0b\u5b57\u6bb5: name \u5b9a\u4e49\u4e86 cron \u7684\u540d\u5b57\uff0ccron \u540d\u5b57\u5728\u540c\u4e00\u4e2a Ehpa \u4e2d\u5fc5\u987b\u662f\u552f\u4e00\u7684 description \u5b9a\u4e49 cron \u7684\u8be6\u7ec6\u63cf\u8ff0\u3002\u5b83\u53ef\u4ee5\u662f\u7a7a\u7684\u3002 timezone \u5b9a\u4e49Crane\u6240\u8981\u8c03\u5ea6\u7684 cron \u65f6\u533a\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5219\u9ed8\u8ba4\u4f7f\u7528 UTC \u65f6\u533a\u3002\u4f60\u53ef\u4ee5\u5c06\u5b83\u8bbe\u7f6e\u4e3a Local \uff0c\u8fd9\u5c06\u4f7f\u7528\u6b63\u5728\u8fd0\u884c\u7684Crane\u5bb9\u5668\u6240\u5728\u7684\u65f6\u533a\u3002\u5176\u5b9e\uff0c\u4f60\u5b9a\u4e49 America/Los_Angeles \u4e5f\u662f\u53ef\u4ee5\u7684\u3002 start \u5b9a\u4e49 cron \u5f00\u59cb\u8c03\u5ea6\u7684\u65f6\u95f4\uff0c\u662f crontab \u683c\u5f0f\u3002\u53c2\u8003 wiki-Cron end \u5b9a\u4e49 cron \u7ed3\u675f\u8c03\u5ea6\u7684\u65f6\u95f4\uff0c\u662f crontab \u683c\u5f0f\u3002\u53c2\u8003 wiki-Cron targetReplicas \u5b9a\u4e49\u76ee\u6807\u526f\u672c\u5728 cron \u5904\u4e8e\u6d3b\u52a8\u72b6\u6001\u65f6\u8981\u6269\u5c55\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u8fd9\u610f\u5473\u7740\u76ee\u6807\u526f\u672c\u6570\u4ecb\u4e8e\u5f00\u59cb\u65f6\u95f4\u548c\u7ed3\u675f\u65f6\u95f4\u4e4b\u95f4\u751f\u6548\u3002 \u4ee5\u4e0aYAML\u5b9a\u4e49\uff0c\u610f\u5473\u7740\u4e00\u5929\u5f53\u4e2d\uff0c\u5de5\u4f5c\u8d1f\u8f7d\u5728\u6bcf\u5c0f\u65f6\u6240\u9700\u8981\u4fdd\u6301\u7684\u526f\u672c\u6570\u3002\u5de5\u4f5c\u8d1f\u8f7d\u5c06\u4f1a\u6bcf\u5929\u6309\u7167\u8be5\u89c4\u5219\u6267\u884c\u3002 #80 -------- --------- ---------- # | | | | | | #1 ------------ ----- -------- ---------- #(time) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \u8bb0\u4f4f \u4e0d\u8981\u8bbe\u7f6e\u5f00\u59cb\u65f6\u95f4\u5728\u7ed3\u675f\u65f6\u95f4\u4e4b\u540e \u3002 \u4f8b\u5982\uff0c\u5f53\u4f60\u8bbe\u7f6e\u4ee5\u4e0b\u5185\u5bb9\u65f6\uff1a crons: - name: \"cron2\" timezone: \"Local\" description: \"scale up\" start: \"0 9 ? * *\" end: \"0 6 ? * *\" targetReplicas: 80 \u4ee5\u4e0a\u65e0\u6548\uff0c\u56e0\u4e3a\u5f00\u59cb\u603b\u662f\u665a\u4e8e\u7ed3\u675f\u3002 HPA \u63a7\u5236\u5668\u59cb\u7ec8\u6839\u636e\u5de5\u4f5c\u8d1f\u8f7d\u6240\u63cf\u8ff0\u7684\u526f\u672c\u6570\u8fdb\u884c\u6269\u5c55\uff0c\u8fd9\u610f\u5473\u7740\u4fdd\u7559\u539f\u6709\u526f\u672c\u6570\u4e0d\u53d8\u3002 Horizontal scaling process \u00b6 cron \u9a71\u52a8\u548c\u6269\u5c55\u8fc7\u7a0b\u6709\u516d\u4e2a\u6b65\u9aa4\uff1a EffectiveHPAController \u521b\u5efa HorizontalPodAutoscaler \uff0c\u5b83\u88ab\u6ce8\u5165\u5230 spec \u4e2d\u7684 external cron metrics \u4e2d\u3002 HPAController \u4ece KubeApiServer \u8bfb\u53d6 external cron metrics KubeApiServer \u5c06\u8bf7\u6c42\u8f6c\u53d1\u7ed9 MetricAdapter \u548c MetricServer MetricAdapter \u627e\u5230\u76ee\u6807 hpa \u7684 cron scaler \uff0c\u5e76\u68c0\u6d4b cron scaler \u662f\u5426\u5904\u4e8e\u6d3b\u52a8\u72b6\u6001\u3002\u8fd9\u610f\u5473\u7740\u5f53\u524d\u65f6\u95f4\u4ecb\u4e8e cron \u5f00\u59cb\u548c\u7ed3\u675f\u8ba1\u5212\u65f6\u95f4\u4e4b\u95f4\u3002\u5b83\u5c06\u8fd4\u56de TargetReplicas \u4e2d\u5b9a\u4e49\u7684 CronSpec \u3002 HPAController \u8ba1\u7b97\u6240\u6709 metrics \u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u9009\u62e9\u6700\u5927\u7684\u4e00\u4e2a\u4e3a\u76ee\u6807\u526f\u672c\u6570\u3002\u5e76\u7531\u6b64\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 scale replicas \u3002 HPAController \u4f7f\u7528 Scale Api \u7f29\u653e\u76ee\u6807 \u4f7f\u7528 EHPA \u65f6\uff0c\u7528\u6237\u53ef\u4ee5\u53ea\u914d\u7f6e cron metric\uff0c\u8ba9 EHPA \u7528\u4f5c cron hpa\u3002 \u4e00\u4e2a EHPA \u7684\u591a\u4e2a crons \u5c06\u8f6c\u6362\u4e3a\u4e00\u4e2a external metrics \u3002 HPA \u5c06\u83b7\u53d6 external metrics \u5e76\u5728\u534f\u8c03\u65f6\u8ba1\u7b97\u76ee\u6807\u526f\u672c\u3002\u5f53\u5b58\u5728\u591a\u4e2a\u6307\u6807\u7684\u5de5\u4f5c\u8d1f\u8f7d\u65f6\uff0cHPA \u5c06\u9009\u62e9\u6700\u5927\u7684\u526f\u672c\u6570\u6765\u6269\u5c55\u3002 Cron working with other metrics together \u00b6 EffectiveHorizontalPodAutoscaler \u517c\u5bb9 HorizontalPodAutoscaler \uff08\u5185\u7f6e\u5728 kubernetes\uff09\u3002\u56e0\u6b64\uff0c\u5982\u679c\u4f60\u4e3a HPA \u914d\u7f6e\u4e86\u6307\u6807\uff0c\u4f8b\u5982 cpu \u6216\u5185\u5b58\uff0c\u90a3\u4e48 HPA \u5c06\u6839\u636e\u5b83\u89c2\u5bdf\u5230\u7684\u5b9e\u65f6\u6307\u6807\u5bf9\u526f\u672c\u6570\u8fdb\u884c\u6269\u5c55\u3002 \u901a\u8fc7 EHPA\uff0c\u7528\u6237\u53ef\u4ee5\u540c\u65f6\u914d\u7f6e CronMetric \u3001 PredictionMetric \u3001 OriginalMetric \u3002 \u6211\u4eec\u5f3a\u70c8\u5efa\u8bae\u4f60\u914d\u7f6e\u6240\u6709\u7ef4\u5ea6\u7684\u6307\u6807\u3002\u5b83\u4eec\u5206\u522b\u4ee3\u8868 cron \u526f\u672c\u3001\u5148\u524d\u9884\u6d4b\u7684\u526f\u672c\u3001\u540e\u89c2\u5bdf\u7684\u526f\u672c\u3002 \u8fd9\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u529f\u80fd\u3002\u56e0\u4e3a HPA \u603b\u662f\u9009\u62e9\u7531\u6240\u6709\u7ef4\u5ea6 metrics \u8ba1\u7b97\u7684\u6700\u5927\u526f\u672c\u8fdb\u884c\u6269\u5c55\u3002 \u8fd9\u5c06\u4fdd\u8bc1\u4f60\u5de5\u4f5c\u8d1f\u8f7d\u7684 QoS\uff0c\u5f53\u4f60\u540c\u65f6\u914d\u7f6e\u4e09\u79cd\u7c7b\u578b\u7684\u81ea\u52a8\u7f29\u653e\u65f6\uff0c\u6839\u636e\u5b9e\u9645\u89c2\u5bdf\u5230\u7684\u6307\u6807\u8ba1\u7b97\u7684\u526f\u672c\u6700\u5927\uff0c\u7136\u540e\u5b83\u5c06\u4f7f\u7528\u6700\u5927\u7684\u4e00\u4e2a\u3002 \u5c3d\u7ba1\u7531\u4e8e\u67d0\u4e9b\u610f\u60f3\u4e0d\u5230\u7684\u539f\u56e0\uff0c\u5bfc\u81f4\u7531 PredictionMetric \u8ba1\u7b97\u7684\u526f\u672c\u66f4\u5c0f\u3002\u56e0\u6b64\uff0c\u4f60\u4e0d\u5fc5\u62c5\u5fc3 QoS\u3002 Mechanism \u00b6 \u5f53 metrics adapter \u5904\u7406 external cron metrics \u8bf7\u6c42\u65f6\uff0c metrics adapter \u5c06\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\u3002 graph LR A[Start] --> B{Active Cron?}; B -->|Yes| C(largest targetReplicas) --> F; B -->|No| D{Work together with other metrics?}; D -->|Yes| G(minimum replicas) --> F; D -->|No| H(current replicas) --> F; F[Result workload replicas]; \u6ca1\u6709\u6d3b\u8dc3\u7684cron\uff0c\u6709\u4e24\u79cd\u60c5\u51b5\uff1a \u6ca1\u6709\u5176\u4ed6 hpa \u6307\u6807\u4e0e cron \u4e00\u8d77\u4f7f\u7528\uff0c\u7136\u540e\u8fd4\u56de\u5f53\u524d\u5de5\u4f5c\u8d1f\u8f7d\u526f\u672c\u4ee5\u4fdd\u7559\u539f\u59cb\u6240\u9700\u7684\u526f\u672c \u5f53\u5176\u4ed6 hpa \u6307\u6807\u4e0e cron \u4e00\u8d77\u4f7f\u7528\uff0c\u5c06\u4f1a\u8fd4\u56de\u6700\u5c0f\u503c\u4ee5\u6d88\u9664cron\u5bf9\u5176\u4ed6\u6307\u6807\u7684\u5f71\u54cd\u3002\u5f53 cron \u4e0e\u5176\u4ed6\u6307\u6807\u4e00\u8d77\u5de5\u4f5c\u65f6\uff0c\u5b83\u4e0d\u5e94\u8be5\u8fd4\u56de\u5de5\u4f5c\u8d1f\u8f7d\u7684\u539f\u59cb\u526f\u672c\u6570\uff0c\u56e0\u4e3a\u53ef\u80fd\u6709\u5176\u4ed6\u6307\u6807\u60f3\u8981\u7f29\u5c0f\u5de5\u4f5c\u8d1f\u8f7d\u7684\u526f\u672c\u6570\u3002 HPA Controller \u9009\u62e9\u7531\u6240\u6709\u6307\u6807\u8ba1\u7b97\u7684\u6700\u5927\u526f\u672c\uff08\u8fd9\u662f\u786c\u4ee3\u7801\u4e2d\u7684 hpa \u9ed8\u8ba4\u7b56\u7565)\uff0ccron \u4f1a\u5f71\u54cd hpa\u3002\u6240\u4ee5\u6211\u4eec\u5e94\u8be5\u5728 cron \u4e0d\u6d3b\u52a8\u65f6\u79fb\u9664 cron \u6548\u679c\uff0c\u5b83\u5e94\u8be5\u8fd4\u56de\u6700\u5c0f\u503c\u3002 \u6709\u6d3b\u8dc3\u7684cron\u3002\u6211\u4eec\u4f7f\u7528 cron spec \u4e2d\u6307\u5b9a\u7684\u6700\u5927\u76ee\u6807\u526f\u672c\u3002\u57fa\u672c\u4e0a\uff0c\u5728\u540c\u4e00\u65f6\u95f4\u6bb5\u5185\u4e0d\u5e94\u6709\u8d85\u8fc7\u4e00\u4e2a\u6d3b\u8dc3\u7684 cron\uff0c\u8fd9\u4e0d\u662f\u6700\u4f73\u5b9e\u8df5\u3002 HPA \u5c06\u83b7\u53d6 cron external metrics \uff0c\u7136\u540e\u5b83\u4f1a\u81ea\u884c\u8ba1\u7b97\u526f\u672c\u6570\u3002 Use Case \u00b6 \u5f53\u4f60\u9700\u8981\u5728\u5348\u591c\u5c06\u5de5\u4f5c\u8d1f\u8f7d\u526f\u672c\u6570\u4fdd\u6301\u5728\u6700\u4f4e\u9650\u5ea6\uff0c\u6839\u636e\u8be5\u9700\u6c42\u914d\u7f6e\u4e86 cron\u3002 \u4f60\u9700\u8981 HPA \u6765\u83b7\u53d6\u6307\u6807\u670d\u52a1\u5668\u89c2\u5bdf\u5230\u7684\u771f\u5b9e\u6307\u6807\uff0c\u4ee5\u6839\u636e\u5b9e\u65f6\u89c2\u5bdf\u5230\u7684\u6307\u6807\u8fdb\u884c\u6269\u5c55\u3002 \u6700\u540e\uff0c\u4f60\u914d\u7f6e\u4e00\u4e2a prediction-driven metric \uff0c\u901a\u8fc7\u9884\u6d4b\u65b9\u5f0f\u63d0\u524d\u6269\u5927\u89c4\u6a21\u5e76\u5728\u672b\u671f\u7f29\u5c0f\u89c4\u6a21\u3002 apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache-multi-dimensions spec : # ScaleTargetRef \u5173\u8054\u5230\u9700\u6269\u7f29\u5bb9\u7684\u5de5\u4f5c\u8d1f\u8f7d scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 # MinReplicas : \u7f29\u653e\u7684\u6700\u5c0f\u526f\u672c\u6570 maxReplicas : 100 # MaxReplicas : \u7f29\u653e\u7684\u6700\u5927\u526f\u672c\u6570 scaleStrategy : Auto # ScaleStrategy : \u7f29\u653e\u5de5\u4f5c\u8d1f\u8f7d\u65f6\u5019\uff0c\u6240\u91c7\u7528\u7684\u7b56\u7565\u3002\u53ef\u9009\u503c\u4e3a \"Auto\" \"Manual\" # Metrics \u5305\u542b\u4e86\u7528\u4e8e\u8ba1\u7b97\u6240\u9700\u526f\u672c\u6570\u7684\u6307\u6807\u3002 metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 # Prediction \u7684\u914d\u7f6e\u5b9a\u4e49\u4e86\u9700\u8981\u9884\u6d4b\u7684\u8d44\u6e90 # \u82e5\u4e0d\u914d\u7f6e\uff0c\u5219\u9ed8\u8ba4\u4e0d\u542f\u52a8 prediction prediction : predictionWindowSeconds : 3600 # PredictionWindowSeconds \u662f\u9884\u6d4b\u672a\u6765\u6307\u6807\u7684\u65f6\u95f4\u7a97\u53e3\u3002 predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" crons : - name : \"cron1\" description : \"scale up\" start : \"0 0 ? * 6\" end : \"00 23 ? * 0\" targetReplicas : 100 \u5e38\u89c1\u95ee\u9898 \u00b6 \u9519\u8bef: unable to get metric crane_pod_cpu_usage \u00b6 \u5f53\u4f60\u67e5\u770b EffectiveHorizontalPodAutoscaler \u7684 Status \u65f6\uff0c\u53ef\u4ee5\u4f1a\u770b\u5230\u8fd9\u6837\u7684\u9519\u8bef\uff1a - lastTransitionTime : \"2022-05-15T14:05:43Z\" message : 'the HPA was unable to compute the replica count: unable to get metric crane_pod_cpu_usage: unable to fetch metrics from custom metrics API: TimeSeriesPrediction is not ready. ' reason : FailedGetPodsMetric status : \"False\" type : ScalingActive \u539f\u56e0\uff1a\u4e0d\u662f\u6240\u6709\u7684\u5de5\u4f5c\u8d1f\u8f7d\u7684 CPU \u4f7f\u7528\u7387\u90fd\u662f\u53ef\u9884\u6d4b\u7684\uff0c\u5f53\u65e0\u6cd5\u9884\u6d4b\u65f6\u5c31\u4f1a\u663e\u793a\u4ee5\u4e0a\u9519\u8bef\u3002 \u89e3\u51b3\u65b9\u6848\uff1a \u7b49\u4e00\u6bb5\u65f6\u95f4\u518d\u770b\u3002\u9884\u6d4b\u7b97\u6cd5 DSP \u9700\u8981\u4e00\u5b9a\u65f6\u95f4\u7684\u6570\u636e\u624d\u80fd\u8fdb\u884c\u9884\u6d4b\u3002\u5e0c\u671b\u4e86\u89e3\u7b97\u6cd5\u7ec6\u8282\u7684\u53ef\u4ee5\u67e5\u770b\u7b97\u6cd5\u7684\u6587\u6863\u3002 EffectiveHorizontalPodAutoscaler \u63d0\u4f9b\u4e00\u79cd\u4fdd\u62a4\u673a\u5236\uff0c\u5f53\u9884\u6d4b\u5931\u6548\u65f6\u4f9d\u7136\u80fd\u901a\u8fc7\u5b9e\u9645\u7684 CPU \u4f7f\u7528\u7387\u5de5\u4f5c\u3002","title":"\u667a\u80fd\u6c34\u5e73\u5f39\u6027"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#effectivehorizontalpodautoscaler","text":"EffectiveHorizontalPodAutoscaler\uff08\u7b80\u79f0 EHPA\uff09\u662f Crane \u63d0\u4f9b\u7684\u5f39\u6027\u4f38\u7f29\u4ea7\u54c1\uff0c\u5b83\u57fa\u4e8e\u793e\u533a HPA \u505a\u5e95\u5c42\u7684\u5f39\u6027\u63a7\u5236\uff0c\u652f\u6301\u66f4\u4e30\u5bcc\u7684\u5f39\u6027\u89e6\u53d1\u7b56\u7565\uff08\u9884\u6d4b\uff0c\u89c2\u6d4b\uff0c\u5468\u671f\uff09\uff0c\u8ba9\u5f39\u6027\u66f4\u52a0\u9ad8\u6548\uff0c\u5e76\u4fdd\u969c\u4e86\u670d\u52a1\u7684\u8d28\u91cf\u3002 \u63d0\u524d\u6269\u5bb9\uff0c\u4fdd\u8bc1\u670d\u52a1\u8d28\u91cf\uff1a\u901a\u8fc7\u7b97\u6cd5\u9884\u6d4b\u672a\u6765\u7684\u6d41\u91cf\u6d2a\u5cf0\u63d0\u524d\u6269\u5bb9\uff0c\u907f\u514d\u6269\u5bb9\u4e0d\u53ca\u65f6\u5bfc\u81f4\u7684\u96ea\u5d29\u548c\u670d\u52a1\u7a33\u5b9a\u6027\u6545\u969c\u3002 \u51cf\u5c11\u65e0\u6548\u7f29\u5bb9\uff1a\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u53ef\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u7f29\u5bb9\uff0c\u7a33\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8d44\u6e90\u4f7f\u7528\u7387\uff0c\u6d88\u9664\u7a81\u523a\u8bef\u5224\u3002 \u652f\u6301 Cron \u914d\u7f6e\uff1a\u652f\u6301 Cron-based \u5f39\u6027\u914d\u7f6e\uff0c\u5e94\u5bf9\u5927\u4fc3\u7b49\u5f02\u5e38\u6d41\u91cf\u6d2a\u5cf0\u3002 \u517c\u5bb9\u793e\u533a\uff1a\u4f7f\u7528\u793e\u533a HPA \u4f5c\u4e3a\u5f39\u6027\u63a7\u5236\u7684\u6267\u884c\u5c42\uff0c\u80fd\u529b\u5b8c\u5168\u517c\u5bb9\u793e\u533a\u3002","title":"EffectiveHorizontalPodAutoscaler"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#_1","text":"\u4e00\u4e2a\u7b80\u5355\u7684 EHPA yaml \u6587\u4ef6\u5982\u4e0b\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache spec : scaleTargetRef : #(1) apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 #(2) maxReplicas : 10 #(3) scaleStrategy : Auto #(4) metrics : #(5) - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 prediction : #(6) predictionWindowSeconds : 3600 #(7) predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" ScaleTargetRef \u914d\u7f6e\u4f60\u5e0c\u671b\u5f39\u6027\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3002 MinReplicas \u6307\u5b9a\u4e86\u81ea\u52a8\u7f29\u5bb9\u7684\u6700\u5c0f\u503c\u3002 MaxReplicas \u6307\u5b9a\u4e86\u81ea\u52a8\u6269\u5bb9\u7684\u6700\u5927\u503c\u3002 ScaleStrategy \u5b9a\u4e49\u4e86\u5f39\u6027\u7684\u7b56\u7565\uff0c\u503c\u53ef\u4ee5\u662f \"Auto\" and \"Preview\". Metrics \u5b9a\u4e49\u4e86\u5f39\u6027\u9608\u503c\u914d\u7f6e\u3002 Prediction \u5b9a\u4e49\u4e86\u9884\u6d4b\u7b97\u6cd5\u914d\u7f6e\u3002 PredictionWindowSeconds \u6307\u5b9a\u5f80\u540e\u9884\u6d4b\u591a\u4e45\u7684\u6570\u636e\u3002","title":"\u4ea7\u54c1\u529f\u80fd"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#_2","text":"\u5927\u591a\u6570\u5728\u7ebf\u5e94\u7528\u7684\u8d1f\u8f7d\u90fd\u6709\u5468\u671f\u6027\u7684\u7279\u5f81\u3002\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u6309\u5929\u6216\u8005\u6309\u5468\u7684\u8d8b\u52bf\u9884\u6d4b\u672a\u6765\u7684\u8d1f\u8f7d\u3002EHPA \u4f7f\u7528 DSP \u7b97\u6cd5\u6765\u9884\u6d4b\u5e94\u7528\u672a\u6765\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e2a\u5f00\u542f\u4e86\u9884\u6d4b\u80fd\u529b\u7684 EHPA \u6a21\u7248\u4f8b\u5b50\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : prediction : predictionWindowSeconds : 3600 predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\"","title":"\u57fa\u4e8e\u9884\u6d4b\u7684\u5f39\u6027"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#_3","text":"\u5728\u4f7f\u7528\u9884\u6d4b\u7b97\u6cd5\u9884\u6d4b\u65f6\uff0c\u4f60\u53ef\u80fd\u4f1a\u62c5\u5fc3\u9884\u6d4b\u6570\u636e\u4e0d\u51c6\u5e26\u6765\u4e00\u5b9a\u7684\u98ce\u9669\uff0cEHPA \u5728\u8ba1\u7b97\u526f\u672c\u6570\u65f6\uff0c\u4e0d\u4ec5\u4f1a\u6309\u9884\u6d4b\u6570\u636e\u8ba1\u7b97\uff0c\u540c\u65f6\u4e5f\u4f1a\u8003\u8651\u5b9e\u9645\u76d1\u63a7\u6570\u636e\u6765\u515c\u5e95\uff0c\u63d0\u5347\u4e86\u5f39\u6027\u7684\u5b89\u5168\u6027\u3002 \u5b9e\u73b0\u7684\u539f\u7406\u662f\u5f53\u4f60\u5728 EHPA \u4e2d\u5b9a\u4e49 spec.metrics \u5e76\u4e14\u5f00\u542f\u5f39\u6027\u9884\u6d4b\u65f6\uff0cEffectiveHPAController \u4f1a\u5728\u521b\u5efa\u5e95\u5c42\u7ba1\u7406\u7684 HPA \u65f6\u6309\u7b56\u7565\u81ea\u52a8\u751f\u6210\u591a\u6761 Metric Spec\u3002 \u4f8b\u5982\uff0c\u5f53\u7528\u6237\u5728 EHPA \u7684 yaml \u91cc\u5b9a\u4e49\u5982\u4e0b Metric Spec\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 \u5b83\u4f1a\u81ea\u52a8\u8f6c\u6362\u6210\u4e24\u6761 HPA \u7684\u9608\u503c\u914d\u7f6e\uff1a apiVersion : autoscaling/v2beta1 kind : HorizontalPodAutoscaler spec : metrics : - pods : metric : name : crane_pod_cpu_usage selector : matchLabels : autoscaling.crane.io/effective-hpa-uid : f9b92249-eab9-4671-afe0-17925e5987b8 target : type : AverageValue averageValue : 100m type : Pods - resource : name : cpu target : type : Utilization averageUtilization : 50 type : Resource \u5728\u4e0a\u9762\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u7528\u6237\u5728 EHPA \u521b\u5efa\u7684 Metric \u9608\u503c\u914d\u7f6e\u4f1a\u81ea\u52a8\u8f6c\u6362\u6210\u5e95\u5c42 HPA \u4e0a\u7684\u4e24\u6761 Metric \u9608\u503c\u914d\u7f6e\uff1a\u9884\u6d4b Metric \u9608\u503c\u548c\u5b9e\u9645\u76d1\u63a7 Metric \u9608\u503c \u9884\u6d4b Metric \u9608\u503c \u662f\u4e00\u4e2a custom metric\u3002\u503c\u901a\u8fc7 Crane \u7684 MetricAdapter \u63d0\u4f9b\u3002 \u5b9e\u9645\u76d1\u63a7 Metric \u9608\u503c \u662f\u4e00\u4e2a resource metric\uff0c\u5b83\u548c\u7528\u6237\u5728 EHPA \u4e0a\u5b9a\u4e49\u7684\u4e00\u6837\u3002\u8fd9\u6837 HPA \u4f1a\u6839\u636e\u5e94\u7528\u5b9e\u9645\u76d1\u63a7\u7684 Metric \u8ba1\u7b97\u526f\u672c\u6570\u3002 HPA \u5728\u914d\u7f6e\u4e86\u591a\u4e2a\u5f39\u6027 Metric \u9608\u503c\u65f6\uff0c\u5728\u8ba1\u7b97\u526f\u672c\u6570\u65f6\u4f1a\u5206\u522b\u8ba1\u7b97\u6bcf\u6761 Metric \u5bf9\u5e94\u7684\u526f\u672c\u6570\uff0c\u5e76\u9009\u62e9 \u6700\u5927 \u7684\u90a3\u4e2a\u526f\u672c\u6570\u4f5c\u4e3a\u6700\u7ec8\u7684\u63a8\u8350\u5f39\u6027\u7ed3\u679c\u3002","title":"\u76d1\u63a7\u6570\u636e\u515c\u5e95"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#_4","text":"EffectiveHPAController \u521b\u5efa HorizontalPodAutoscaler \u548c TimeSeriesPrediction \u5bf9\u8c61 PredictionCore \u4ece prometheus \u83b7\u53d6\u5386\u53f2 metric \u901a\u8fc7\u9884\u6d4b\u7b97\u6cd5\u8ba1\u7b97\uff0c\u5c06\u7ed3\u679c\u8bb0\u5f55\u5230 TimeSeriesPrediction HPAController \u901a\u8fc7 metric client \u4ece KubeApiServer \u8bfb\u53d6 metric \u6570\u636e KubeApiServer \u5c06\u8bf7\u6c42\u8def\u7531\u5230 Crane \u7684 MetricAdapter\u3002 HPAController \u8ba1\u7b97\u6240\u6709\u7684 Metric \u8fd4\u56de\u7684\u7ed3\u679c\u5f97\u5230\u6700\u7ec8\u7684\u5f39\u6027\u526f\u672c\u63a8\u8350\u3002 HPAController \u8c03\u7528 scale API \u5bf9\u76ee\u6807\u5e94\u7528\u6269/\u7f29\u5bb9\u3002 \u6574\u4f53\u6d41\u7a0b\u56fe\u5982\u4e0b\uff1a","title":"\u6c34\u5e73\u5f39\u6027\u7684\u6267\u884c\u6d41\u7a0b"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#_5","text":"\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u751f\u4ea7\u73af\u5883\u7684\u5ba2\u6237\u6848\u4f8b\u6765\u4ecb\u7ecd EHPA \u7684\u843d\u5730\u6548\u679c\u3002 \u6211\u4eec\u5c06\u751f\u4ea7\u4e0a\u7684\u6570\u636e\u5728\u9884\u53d1\u73af\u5883\u91cd\u653e\uff0c\u5bf9\u6bd4\u4f7f\u7528 EHPA \u548c\u793e\u533a\u7684 HPA \u7684\u5f39\u6027\u6548\u679c\u3002 \u4e0b\u56fe\u7684\u7ea2\u7ebf\u662f\u5e94\u7528\u5728\u4e00\u5929\u5185\u7684\u5b9e\u9645 CPU \u4f7f\u7528\u91cf\u66f2\u7ebf\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u57288\u70b9\uff0c12\u70b9\uff0c\u665a\u4e0a8\u70b9\u65f6\u662f\u4f7f\u7528\u9ad8\u5cf0\u3002\u7eff\u7ebf\u662f EHPA \u9884\u6d4b\u7684 CPU \u4f7f\u7528\u91cf\u3002 \u4e0b\u56fe\u662f\u5bf9\u5e94\u7684\u81ea\u52a8\u5f39\u6027\u7684\u526f\u672c\u6570\u66f2\u7ebf\uff0c\u7ea2\u7ebf\u662f\u793e\u533a HPA \u7684\u526f\u672c\u6570\u66f2\u7ebf\uff0c\u7eff\u7ebf\u662f EHPA \u7684\u526f\u672c\u6570\u66f2\u7ebf\u3002 \u53ef\u4ee5\u770b\u5230 EHPA \u5177\u6709\u4ee5\u4e0b\u4f18\u52bf\uff1a \u5728\u6d41\u91cf\u6d2a\u5cf0\u6765\u4e34\u524d\u6269\u5bb9\u3002 \u5f53\u6d41\u91cf\u5148\u964d\u540e\u7acb\u523b\u5347\u65f6\u4e0d\u505a\u65e0\u6548\u7f29\u5bb9\u3002 \u76f8\u6bd4 HPA \u66f4\u5c11\u7684\u5f39\u6027\u6b21\u6570\u5374\u66f4\u9ad8\u6548\u3002","title":"\u7528\u6237\u6848\u4f8b"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#scalestrategy","text":"EHPA \u63d0\u4f9b\u4e86\u4e24\u79cd\u5f39\u6027\u7b56\u7565\uff1a Auto \u548c Preview \u3002\u7528\u6237\u53ef\u4ee5\u968f\u65f6\u5207\u6362\u5b83\u5e76\u7acb\u5373\u751f\u6548\u3002","title":"ScaleStrategy \u5f39\u6027\u7b56\u7565"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#auto","text":"Auto \u7b56\u7565\u4e0b EHPA \u4f1a\u81ea\u52a8\u6267\u884c\u5f39\u6027\u884c\u4e3a\u3002\u9ed8\u8ba4 EHPA \u7684\u7b56\u7565\u662f Auto\u3002\u5728\u8fd9\u4e2a\u6a21\u5f0f\u4e0b EHPA \u4f1a\u521b\u5efa\u4e00\u4e2a\u793e\u533a\u7684 HPA \u5bf9\u8c61\u5e76\u81ea\u52a8\u63a5\u7ba1\u5b83\u7684\u751f\u547d\u5468\u671f\u3002\u6211\u4eec\u4e0d\u5efa\u8bae\u7528\u6237\u4fee\u6539\u6216\u8005\u63a7\u5236\u8fd9\u4e2a\u5e95\u5c42\u7684 HPA \u5bf9\u8c61\uff0c\u5f53 EHPA \u88ab\u5220\u9664\u65f6\uff0c\u5e95\u5c42\u7684 HPA \u5bf9\u8c61\u4e5f\u4f1a\u4e00\u5e76\u5220\u9664\u3002","title":"Auto"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#preview","text":"Preview \u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba9 EHPA \u4e0d\u81ea\u52a8\u6267\u884c\u5f39\u6027\u7684\u80fd\u529b\u3002\u6240\u4ee5\u4f60\u53ef\u4ee5\u901a\u8fc7 EHPA \u7684 desiredReplicas \u5b57\u6bb5\u89c2\u6d4b EHPA \u8ba1\u7b97\u51fa\u7684\u526f\u672c\u6570\u3002\u7528\u6237\u53ef\u4ee5\u968f\u65f6\u5728\u4e24\u4e2a\u6a21\u5f0f\u95f4\u5207\u6362\uff0c\u5f53\u7528\u6237\u5207\u6362\u5230 Preview \u6a21\u5f0f\u65f6\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7 spec.specificReplicas \u8c03\u6574\u5e94\u7528\u7684\u526f\u672c\u6570\uff0c\u5982\u679c spec.specificReplicas \u4e3a\u7a7a\uff0c\u5219\u4e0d\u4f1a\u5bf9\u5e94\u7528\u6267\u884c\u5f39\u6027\uff0c\u4f46\u662f\u4f9d\u7136\u4f1a\u6267\u884c\u526f\u672c\u6570\u7684\u8ba1\u7b97\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e2a\u914d\u7f6e\u6210 Preview \u6a21\u5f0f\u7684 EHPA \u6a21\u7248\u4f8b\u5b50\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler spec : scaleStrategy : Preview # ScaleStrategy indicate the strategy to scaling target, value can be \"Auto\" and \"Preview\". specificReplicas : 5 # SpecificReplicas specify the target replicas. status : expectReplicas : 4 # expectReplicas is the calculated replicas that based on prediction metrics or spec.specificReplicas. currentReplicas : 4 # currentReplicas is actual replicas from target","title":"Preview"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#horizontalpodautoscaler","text":"EHPA \u4ece\u8bbe\u8ba1\u4e4b\u51fa\u5c31\u5e0c\u671b\u548c\u793e\u533a\u7684 HPA \u517c\u5bb9\uff0c\u56e0\u4e3a\u6211\u4eec\u4e0d\u5e0c\u671b\u91cd\u65b0\u9020\u4e00\u4e2a\u7c7b\u4f3c HPA \u7684\u8f6e\u5b50\uff0cHPA \u5728\u4e0d\u65ad\u6f14\u8fdb\u7684\u8fc7\u7a0b\u5df2\u7ecf\u89e3\u51b3\u4e86\u5f88\u591a\u901a\u7528\u7684\u95ee\u9898\uff0cEHPA \u5e0c\u671b\u5728 HPA \u7684\u57fa\u7840\u4e0a\u63d0\u4f9b\u66f4\u9ad8\u9636\u7684 CRD\uff0cEHPA \u7684\u529f\u80fd\u662f\u793e\u533a HPA \u7684\u8d85\u96c6\u3002 EHPA \u4e5f\u4f1a\u6301\u7eed\u8ddf\u8fdb\u652f\u6301 HPA \u7684\u65b0\u529f\u80fd\u3002","title":"HorizontalPodAutoscaler \u793e\u533a\u517c\u5bb9"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#effectivehorizontalpodautoscaler-status","text":"EHPA \u7684 Status \u5305\u62ec\u4e86\u81ea\u8eab\u7684 Status \u540c\u65f6\u4e5f\u6c47\u805a\u4e86\u5e95\u5c42 HPA \u7684\u90e8\u5206 Status\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e2a EHPA \u7684 Status yaml\u4f8b\u5b50\uff1a apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler status : conditions : - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : the HPA controller was able to get the target's current scale reason : SucceededGetScale status : \"True\" type : AbleToScale - lastTransitionTime : \"2021-11-30T08:18:59Z\" message : Effective HPA is ready reason : EffectiveHorizontalPodAutoscalerReady status : \"True\" type : Ready currentReplicas : 1 expectReplicas : 0","title":"EffectiveHorizontalPodAutoscaler status"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#cron-based-autoscaling","text":"EffectiveHorizontalPodAutoscaler \u652f\u6301\u57fa\u4e8e cron \u7684\u81ea\u52a8\u7f29\u653e\u3002 \u9664\u4e86\u57fa\u4e8e\u76d1\u63a7\u6307\u6807\uff0c\u6709\u65f6\u8282\u5047\u65e5\u548c\u5de5\u4f5c\u65e5\u7684\u5de5\u4f5c\u8d1f\u8f7d\u6d41\u91cf\u5b58\u5728\u5dee\u5f02\uff0c\u7b80\u5355\u7684\u9884\u6d4b\u7b97\u6cd5\u53ef\u80fd\u6548\u679c\u4e0d\u4f73\u3002\u7136\u540e\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u5468\u672b cron \u6765\u652f\u6301\u66f4\u5927\u6570\u91cf\u7684\u526f\u672c\u6765\u5f25\u8865\u9884\u6d4b\u7684\u4e0d\u8db3\u3002 \u5bf9\u4e8e\u4e00\u4e9b\u975e web \u6d41\u91cf\u7684\u5e94\u7528\uff0c\u6bd4\u5982\u4e00\u4e9b\u5e94\u7528\u4e0d\u9700\u8981\u5728\u5468\u672b\u4f7f\u7528\uff0c\u53ef\u4ee5\u628a\u5de5\u4f5c\u8d1f\u8f7d\u7684\u526f\u672c\u6570\u51cf\u5c11\u5230 1\uff0c\u4e5f\u53ef\u4ee5\u914d\u7f6e cron \u6765\u964d\u4f4e\u4f60\u7684\u670d\u52a1\u6210\u672c\u3002 \u4ee5\u4e0b\u662f EHPA Spec \u4e2d\u7684 cron \u4e3b\u8981\u5b57\u6bb5\uff1a CronSpec \uff1a\u53ef\u4ee5\u8bbe\u7f6e\u591a\u4e2a cron \u81ea\u52a8\u4f38\u7f29\u914d\u7f6e\uff0ccron cycle \u53ef\u4ee5\u8bbe\u7f6e\u5faa\u73af\u7684\u5f00\u59cb\u65f6\u95f4\u548c\u7ed3\u675f\u65f6\u95f4\uff0c\u5e76\u4e14\u5de5\u4f5c\u8d1f\u8f7d\u7684\u526f\u672c\u6570\u53ef\u4ee5\u5728\u65f6\u95f4\u8303\u56f4\u5185\u6301\u7eed\u4fdd\u6301\u4e3a\u8bbe\u5b9a\u7684\u76ee\u6807\u503c\u3002 Name \uff1acron \u6807\u8bc6\u7b26 TargetReplicas \uff1a\u6b64 cron \u65f6\u95f4\u8303\u56f4\u5185\u5de5\u4f5c\u8d1f\u8f7d\u7684\u76ee\u6807\u526f\u672c\u6570\u3002 Start \uff1acron \u7684\u5f00\u59cb\u65f6\u95f4\uff0c\u6807\u51c6 linux crontab \u683c\u5f0f End \uff1acron \u7684\u7ed3\u675f\u65f6\u95f4\uff0c\u6807\u51c6 linux crontab \u683c\u5f0f \u4e00\u4e9b\u4e91\u5382\u5546\u548c\u793e\u533a\u5f53\u524d\u7684 cron \u81ea\u52a8\u7f29\u653e\u529f\u80fd\u5b58\u5728\u4e00\u4e9b\u7f3a\u70b9\u3002 cron \u80fd\u529b\u5355\u72ec\u63d0\u4f9b\uff0c\u6ca1\u6709\u5728\u5168\u5c40\u89c6\u56fe\u4e2d\u8fdb\u884c\u81ea\u52a8\u7f29\u653e\uff0c\u4e0e HPA \u517c\u5bb9\u6027\u5dee\uff0c\u4e0e\u5176\u4ed6\u7f29\u653e\u89e6\u53d1\u5668\u51b2\u7a81\u3002 cron \u7684\u8bed\u4e49\u548c\u884c\u4e3a\u4e0d\u662f\u5f88\u5339\u914d\uff0c\u4f7f\u7528\u65f6\u751a\u81f3\u5f88\u96be\u7406\u89e3\uff0c\u5f88\u5bb9\u6613\u8bef\u5bfc\u7528\u6237\uff0c\u5bfc\u81f4\u81ea\u52a8\u4f38\u7f29\u5931\u8d25\u3002 \u4e0b\u56fe\u663e\u793a\u4e86\u5f53\u524d EHPA cron \u81ea\u52a8\u4f38\u7f29\u5b9e\u73b0\u4e0e\u5176\u4ed6 cron \u80fd\u529b\u7684\u5bf9\u6bd4\u3002 \u9488\u5bf9\u4ee5\u4e0a\u95ee\u9898\uff0cEHPA \u5b9e\u73b0\u7684 cron autoscaling \u662f\u5728\u4e0e HPA \u517c\u5bb9\u7684\u57fa\u7840\u4e0a\u8bbe\u8ba1\u7684\uff0ccron \u4f5c\u4e3a HPA \u7684\u4e00\u4e2a\u6307\u6807\uff0c\u4e0e\u5176\u4ed6\u6307\u6807\u4e00\u8d77\u4f5c\u7528\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u3002 \u53e6\u5916\uff0ccron \u7684\u8bbe\u7f6e\u4e5f\u5f88\u7b80\u5355\u3002\u5355\u72ec\u914d\u7f6e cron \u65f6\uff0c\u4e0d\u5728\u6d3b\u52a8\u65f6\u95f4\u8303\u56f4\u5185\u65f6\uff0c\u4e0d\u4f1a\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u6267\u884c\u7f29\u653e\u3002","title":"Cron-based autoscaling"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#cron-working-without-other-metrics","text":"\u5047\u8bbe\u4f60\u6ca1\u6709\u914d\u7f6e\u5176\u4ed6\u6307\u6807\uff0c\u4f60\u53ea\u9700\u914d\u7f6e cron \u672c\u8eab\u5373\u53ef\u5de5\u4f5c\u3002 apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache-local spec : # ScaleTargetRef \u5173\u8054\u5230\u9700\u6269\u7f29\u5bb9\u7684\u5de5\u4f5c\u8d1f\u8f7d scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 # MinReplicas : autoscaler \u7f29\u653e\u7684\u6700\u4f4e\u526f\u672c\u6570 maxReplicas : 100 # MaxReplicas : autoscaler \u7f29\u653e\u7684\u6700\u5927\u526f\u672c\u6570 scaleStrategy : Auto # ScaleStrategy : \u7f29\u653e\u5de5\u4f5c\u8d1f\u8f7d\u65f6\u5019\uff0c\u6240\u91c7\u7528\u7684\u7b56\u7565\u3002\u53ef\u9009\u503c\u4e3a \"Auto\" \"Manual\" # \u6700\u597d\u5c06Cron Scheduling\u8bbe\u7f6e\u4e3a\u4e00\u4e2a\u5b8c\u6574\u7684\u65f6\u95f4\u5468\u671f\uff0c\u4f8b\u5982\uff1a \u4e00\u5929\uff0c\u4e00\u5468 # \u4e0b\u9762\u662f\u4e00\u5929\u7684Cron Scheduling #(targetReplicas) #80 -------- --------- ---------- # | | | | | | #10 ------------ ----- -------- ---------- #(time) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #\u672c\u5730\u65f6\u533a(timezone: \"Local\")\u610f\u5473\u7740\u60a8\u4f7f\u7528\u8fd0\u884cCraned\u6240\u5728\u7684\u670d\u52a1\u5668\uff08\u6216\u8005\u53ef\u80fd\u662f\u5bb9\u5668\uff09\u7684\u65f6\u533a\u3002\u4f8b\u5982\uff0c\u5f53Craned \u662f\u4ee5UTC\u65f6\u533a\u5f00\u59cb\uff0c\u90a3\u4e48\u5b83\u5c31\u662fUTC\u3002\u5982\u679c\u4e00\u5f00\u59cb\u662fAsia/Shanghai\uff0c\u90a3\u4e48\u5b83\u5c31\u662fAsia/Shanghai\u3002 crons : - name : \"cron1\" timezone : \"Local\" description : \"scale down\" start : \"0 0 ? * *\" end : \"0 6 ? * *\" targetReplicas : 10 - name : \"cron2\" timezone : \"Local\" description : \"scale up\" start : \"0 6 ? * *\" end : \"0 9 ? * *\" targetReplicas : 80 - name : \"cron3\" timezone : \"Local\" description : \"scale down\" start : \"00 9 ? * *\" end : \"00 11 ? * *\" targetReplicas : 10 - name : \"cron4\" timezone : \"Local\" description : \"scale up\" start : \"00 11 ? * *\" end : \"00 14 ? * *\" targetReplicas : 80 - name : \"cron5\" timezone : \"Local\" description : \"scale down\" start : \"00 14 ? * *\" end : \"00 17 ? * *\" targetReplicas : 10 - name : \"cron6\" timezone : \"Local\" description : \"scale up\" start : \"00 17 ? * *\" end : \"00 20 ? * *\" targetReplicas : 80 - name : \"cron7\" timezone : \"Local\" description : \"scale down\" start : \"00 20 ? * *\" end : \"00 00 ? * *\" targetReplicas : 10 CronSpec \u5177\u6709\u4ee5\u4e0b\u5b57\u6bb5: name \u5b9a\u4e49\u4e86 cron \u7684\u540d\u5b57\uff0ccron \u540d\u5b57\u5728\u540c\u4e00\u4e2a Ehpa \u4e2d\u5fc5\u987b\u662f\u552f\u4e00\u7684 description \u5b9a\u4e49 cron \u7684\u8be6\u7ec6\u63cf\u8ff0\u3002\u5b83\u53ef\u4ee5\u662f\u7a7a\u7684\u3002 timezone \u5b9a\u4e49Crane\u6240\u8981\u8c03\u5ea6\u7684 cron \u65f6\u533a\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5219\u9ed8\u8ba4\u4f7f\u7528 UTC \u65f6\u533a\u3002\u4f60\u53ef\u4ee5\u5c06\u5b83\u8bbe\u7f6e\u4e3a Local \uff0c\u8fd9\u5c06\u4f7f\u7528\u6b63\u5728\u8fd0\u884c\u7684Crane\u5bb9\u5668\u6240\u5728\u7684\u65f6\u533a\u3002\u5176\u5b9e\uff0c\u4f60\u5b9a\u4e49 America/Los_Angeles \u4e5f\u662f\u53ef\u4ee5\u7684\u3002 start \u5b9a\u4e49 cron \u5f00\u59cb\u8c03\u5ea6\u7684\u65f6\u95f4\uff0c\u662f crontab \u683c\u5f0f\u3002\u53c2\u8003 wiki-Cron end \u5b9a\u4e49 cron \u7ed3\u675f\u8c03\u5ea6\u7684\u65f6\u95f4\uff0c\u662f crontab \u683c\u5f0f\u3002\u53c2\u8003 wiki-Cron targetReplicas \u5b9a\u4e49\u76ee\u6807\u526f\u672c\u5728 cron \u5904\u4e8e\u6d3b\u52a8\u72b6\u6001\u65f6\u8981\u6269\u5c55\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u8fd9\u610f\u5473\u7740\u76ee\u6807\u526f\u672c\u6570\u4ecb\u4e8e\u5f00\u59cb\u65f6\u95f4\u548c\u7ed3\u675f\u65f6\u95f4\u4e4b\u95f4\u751f\u6548\u3002 \u4ee5\u4e0aYAML\u5b9a\u4e49\uff0c\u610f\u5473\u7740\u4e00\u5929\u5f53\u4e2d\uff0c\u5de5\u4f5c\u8d1f\u8f7d\u5728\u6bcf\u5c0f\u65f6\u6240\u9700\u8981\u4fdd\u6301\u7684\u526f\u672c\u6570\u3002\u5de5\u4f5c\u8d1f\u8f7d\u5c06\u4f1a\u6bcf\u5929\u6309\u7167\u8be5\u89c4\u5219\u6267\u884c\u3002 #80 -------- --------- ---------- # | | | | | | #1 ------------ ----- -------- ---------- #(time) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \u8bb0\u4f4f \u4e0d\u8981\u8bbe\u7f6e\u5f00\u59cb\u65f6\u95f4\u5728\u7ed3\u675f\u65f6\u95f4\u4e4b\u540e \u3002 \u4f8b\u5982\uff0c\u5f53\u4f60\u8bbe\u7f6e\u4ee5\u4e0b\u5185\u5bb9\u65f6\uff1a crons: - name: \"cron2\" timezone: \"Local\" description: \"scale up\" start: \"0 9 ? * *\" end: \"0 6 ? * *\" targetReplicas: 80 \u4ee5\u4e0a\u65e0\u6548\uff0c\u56e0\u4e3a\u5f00\u59cb\u603b\u662f\u665a\u4e8e\u7ed3\u675f\u3002 HPA \u63a7\u5236\u5668\u59cb\u7ec8\u6839\u636e\u5de5\u4f5c\u8d1f\u8f7d\u6240\u63cf\u8ff0\u7684\u526f\u672c\u6570\u8fdb\u884c\u6269\u5c55\uff0c\u8fd9\u610f\u5473\u7740\u4fdd\u7559\u539f\u6709\u526f\u672c\u6570\u4e0d\u53d8\u3002","title":"Cron working without other metrics"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#horizontal-scaling-process","text":"cron \u9a71\u52a8\u548c\u6269\u5c55\u8fc7\u7a0b\u6709\u516d\u4e2a\u6b65\u9aa4\uff1a EffectiveHPAController \u521b\u5efa HorizontalPodAutoscaler \uff0c\u5b83\u88ab\u6ce8\u5165\u5230 spec \u4e2d\u7684 external cron metrics \u4e2d\u3002 HPAController \u4ece KubeApiServer \u8bfb\u53d6 external cron metrics KubeApiServer \u5c06\u8bf7\u6c42\u8f6c\u53d1\u7ed9 MetricAdapter \u548c MetricServer MetricAdapter \u627e\u5230\u76ee\u6807 hpa \u7684 cron scaler \uff0c\u5e76\u68c0\u6d4b cron scaler \u662f\u5426\u5904\u4e8e\u6d3b\u52a8\u72b6\u6001\u3002\u8fd9\u610f\u5473\u7740\u5f53\u524d\u65f6\u95f4\u4ecb\u4e8e cron \u5f00\u59cb\u548c\u7ed3\u675f\u8ba1\u5212\u65f6\u95f4\u4e4b\u95f4\u3002\u5b83\u5c06\u8fd4\u56de TargetReplicas \u4e2d\u5b9a\u4e49\u7684 CronSpec \u3002 HPAController \u8ba1\u7b97\u6240\u6709 metrics \u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u9009\u62e9\u6700\u5927\u7684\u4e00\u4e2a\u4e3a\u76ee\u6807\u526f\u672c\u6570\u3002\u5e76\u7531\u6b64\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 scale replicas \u3002 HPAController \u4f7f\u7528 Scale Api \u7f29\u653e\u76ee\u6807 \u4f7f\u7528 EHPA \u65f6\uff0c\u7528\u6237\u53ef\u4ee5\u53ea\u914d\u7f6e cron metric\uff0c\u8ba9 EHPA \u7528\u4f5c cron hpa\u3002 \u4e00\u4e2a EHPA \u7684\u591a\u4e2a crons \u5c06\u8f6c\u6362\u4e3a\u4e00\u4e2a external metrics \u3002 HPA \u5c06\u83b7\u53d6 external metrics \u5e76\u5728\u534f\u8c03\u65f6\u8ba1\u7b97\u76ee\u6807\u526f\u672c\u3002\u5f53\u5b58\u5728\u591a\u4e2a\u6307\u6807\u7684\u5de5\u4f5c\u8d1f\u8f7d\u65f6\uff0cHPA \u5c06\u9009\u62e9\u6700\u5927\u7684\u526f\u672c\u6570\u6765\u6269\u5c55\u3002","title":"Horizontal scaling process"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#cron-working-with-other-metrics-together","text":"EffectiveHorizontalPodAutoscaler \u517c\u5bb9 HorizontalPodAutoscaler \uff08\u5185\u7f6e\u5728 kubernetes\uff09\u3002\u56e0\u6b64\uff0c\u5982\u679c\u4f60\u4e3a HPA \u914d\u7f6e\u4e86\u6307\u6807\uff0c\u4f8b\u5982 cpu \u6216\u5185\u5b58\uff0c\u90a3\u4e48 HPA \u5c06\u6839\u636e\u5b83\u89c2\u5bdf\u5230\u7684\u5b9e\u65f6\u6307\u6807\u5bf9\u526f\u672c\u6570\u8fdb\u884c\u6269\u5c55\u3002 \u901a\u8fc7 EHPA\uff0c\u7528\u6237\u53ef\u4ee5\u540c\u65f6\u914d\u7f6e CronMetric \u3001 PredictionMetric \u3001 OriginalMetric \u3002 \u6211\u4eec\u5f3a\u70c8\u5efa\u8bae\u4f60\u914d\u7f6e\u6240\u6709\u7ef4\u5ea6\u7684\u6307\u6807\u3002\u5b83\u4eec\u5206\u522b\u4ee3\u8868 cron \u526f\u672c\u3001\u5148\u524d\u9884\u6d4b\u7684\u526f\u672c\u3001\u540e\u89c2\u5bdf\u7684\u526f\u672c\u3002 \u8fd9\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u529f\u80fd\u3002\u56e0\u4e3a HPA \u603b\u662f\u9009\u62e9\u7531\u6240\u6709\u7ef4\u5ea6 metrics \u8ba1\u7b97\u7684\u6700\u5927\u526f\u672c\u8fdb\u884c\u6269\u5c55\u3002 \u8fd9\u5c06\u4fdd\u8bc1\u4f60\u5de5\u4f5c\u8d1f\u8f7d\u7684 QoS\uff0c\u5f53\u4f60\u540c\u65f6\u914d\u7f6e\u4e09\u79cd\u7c7b\u578b\u7684\u81ea\u52a8\u7f29\u653e\u65f6\uff0c\u6839\u636e\u5b9e\u9645\u89c2\u5bdf\u5230\u7684\u6307\u6807\u8ba1\u7b97\u7684\u526f\u672c\u6700\u5927\uff0c\u7136\u540e\u5b83\u5c06\u4f7f\u7528\u6700\u5927\u7684\u4e00\u4e2a\u3002 \u5c3d\u7ba1\u7531\u4e8e\u67d0\u4e9b\u610f\u60f3\u4e0d\u5230\u7684\u539f\u56e0\uff0c\u5bfc\u81f4\u7531 PredictionMetric \u8ba1\u7b97\u7684\u526f\u672c\u66f4\u5c0f\u3002\u56e0\u6b64\uff0c\u4f60\u4e0d\u5fc5\u62c5\u5fc3 QoS\u3002","title":"Cron working with other metrics together"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#mechanism","text":"\u5f53 metrics adapter \u5904\u7406 external cron metrics \u8bf7\u6c42\u65f6\uff0c metrics adapter \u5c06\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\u3002 graph LR A[Start] --> B{Active Cron?}; B -->|Yes| C(largest targetReplicas) --> F; B -->|No| D{Work together with other metrics?}; D -->|Yes| G(minimum replicas) --> F; D -->|No| H(current replicas) --> F; F[Result workload replicas]; \u6ca1\u6709\u6d3b\u8dc3\u7684cron\uff0c\u6709\u4e24\u79cd\u60c5\u51b5\uff1a \u6ca1\u6709\u5176\u4ed6 hpa \u6307\u6807\u4e0e cron \u4e00\u8d77\u4f7f\u7528\uff0c\u7136\u540e\u8fd4\u56de\u5f53\u524d\u5de5\u4f5c\u8d1f\u8f7d\u526f\u672c\u4ee5\u4fdd\u7559\u539f\u59cb\u6240\u9700\u7684\u526f\u672c \u5f53\u5176\u4ed6 hpa \u6307\u6807\u4e0e cron \u4e00\u8d77\u4f7f\u7528\uff0c\u5c06\u4f1a\u8fd4\u56de\u6700\u5c0f\u503c\u4ee5\u6d88\u9664cron\u5bf9\u5176\u4ed6\u6307\u6807\u7684\u5f71\u54cd\u3002\u5f53 cron \u4e0e\u5176\u4ed6\u6307\u6807\u4e00\u8d77\u5de5\u4f5c\u65f6\uff0c\u5b83\u4e0d\u5e94\u8be5\u8fd4\u56de\u5de5\u4f5c\u8d1f\u8f7d\u7684\u539f\u59cb\u526f\u672c\u6570\uff0c\u56e0\u4e3a\u53ef\u80fd\u6709\u5176\u4ed6\u6307\u6807\u60f3\u8981\u7f29\u5c0f\u5de5\u4f5c\u8d1f\u8f7d\u7684\u526f\u672c\u6570\u3002 HPA Controller \u9009\u62e9\u7531\u6240\u6709\u6307\u6807\u8ba1\u7b97\u7684\u6700\u5927\u526f\u672c\uff08\u8fd9\u662f\u786c\u4ee3\u7801\u4e2d\u7684 hpa \u9ed8\u8ba4\u7b56\u7565)\uff0ccron \u4f1a\u5f71\u54cd hpa\u3002\u6240\u4ee5\u6211\u4eec\u5e94\u8be5\u5728 cron \u4e0d\u6d3b\u52a8\u65f6\u79fb\u9664 cron \u6548\u679c\uff0c\u5b83\u5e94\u8be5\u8fd4\u56de\u6700\u5c0f\u503c\u3002 \u6709\u6d3b\u8dc3\u7684cron\u3002\u6211\u4eec\u4f7f\u7528 cron spec \u4e2d\u6307\u5b9a\u7684\u6700\u5927\u76ee\u6807\u526f\u672c\u3002\u57fa\u672c\u4e0a\uff0c\u5728\u540c\u4e00\u65f6\u95f4\u6bb5\u5185\u4e0d\u5e94\u6709\u8d85\u8fc7\u4e00\u4e2a\u6d3b\u8dc3\u7684 cron\uff0c\u8fd9\u4e0d\u662f\u6700\u4f73\u5b9e\u8df5\u3002 HPA \u5c06\u83b7\u53d6 cron external metrics \uff0c\u7136\u540e\u5b83\u4f1a\u81ea\u884c\u8ba1\u7b97\u526f\u672c\u6570\u3002","title":"Mechanism"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#use-case","text":"\u5f53\u4f60\u9700\u8981\u5728\u5348\u591c\u5c06\u5de5\u4f5c\u8d1f\u8f7d\u526f\u672c\u6570\u4fdd\u6301\u5728\u6700\u4f4e\u9650\u5ea6\uff0c\u6839\u636e\u8be5\u9700\u6c42\u914d\u7f6e\u4e86 cron\u3002 \u4f60\u9700\u8981 HPA \u6765\u83b7\u53d6\u6307\u6807\u670d\u52a1\u5668\u89c2\u5bdf\u5230\u7684\u771f\u5b9e\u6307\u6807\uff0c\u4ee5\u6839\u636e\u5b9e\u65f6\u89c2\u5bdf\u5230\u7684\u6307\u6807\u8fdb\u884c\u6269\u5c55\u3002 \u6700\u540e\uff0c\u4f60\u914d\u7f6e\u4e00\u4e2a prediction-driven metric \uff0c\u901a\u8fc7\u9884\u6d4b\u65b9\u5f0f\u63d0\u524d\u6269\u5927\u89c4\u6a21\u5e76\u5728\u672b\u671f\u7f29\u5c0f\u89c4\u6a21\u3002 apiVersion : autoscaling.crane.io/v1alpha1 kind : EffectiveHorizontalPodAutoscaler metadata : name : php-apache-multi-dimensions spec : # ScaleTargetRef \u5173\u8054\u5230\u9700\u6269\u7f29\u5bb9\u7684\u5de5\u4f5c\u8d1f\u8f7d scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : php-apache minReplicas : 1 # MinReplicas : \u7f29\u653e\u7684\u6700\u5c0f\u526f\u672c\u6570 maxReplicas : 100 # MaxReplicas : \u7f29\u653e\u7684\u6700\u5927\u526f\u672c\u6570 scaleStrategy : Auto # ScaleStrategy : \u7f29\u653e\u5de5\u4f5c\u8d1f\u8f7d\u65f6\u5019\uff0c\u6240\u91c7\u7528\u7684\u7b56\u7565\u3002\u53ef\u9009\u503c\u4e3a \"Auto\" \"Manual\" # Metrics \u5305\u542b\u4e86\u7528\u4e8e\u8ba1\u7b97\u6240\u9700\u526f\u672c\u6570\u7684\u6307\u6807\u3002 metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 50 # Prediction \u7684\u914d\u7f6e\u5b9a\u4e49\u4e86\u9700\u8981\u9884\u6d4b\u7684\u8d44\u6e90 # \u82e5\u4e0d\u914d\u7f6e\uff0c\u5219\u9ed8\u8ba4\u4e0d\u542f\u52a8 prediction prediction : predictionWindowSeconds : 3600 # PredictionWindowSeconds \u662f\u9884\u6d4b\u672a\u6765\u6307\u6807\u7684\u65f6\u95f4\u7a97\u53e3\u3002 predictionAlgorithm : algorithmType : dsp dsp : sampleInterval : \"60s\" historyLength : \"3d\" crons : - name : \"cron1\" description : \"scale up\" start : \"0 0 ? * 6\" end : \"00 23 ? * 0\" targetReplicas : 100","title":"Use Case"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#_6","text":"","title":"\u5e38\u89c1\u95ee\u9898"},{"location":"zh/tutorials/using-effective-hpa-to-scaling-with-effectiveness/#unable-to-get-metric-crane_pod_cpu_usage","text":"\u5f53\u4f60\u67e5\u770b EffectiveHorizontalPodAutoscaler \u7684 Status \u65f6\uff0c\u53ef\u4ee5\u4f1a\u770b\u5230\u8fd9\u6837\u7684\u9519\u8bef\uff1a - lastTransitionTime : \"2022-05-15T14:05:43Z\" message : 'the HPA was unable to compute the replica count: unable to get metric crane_pod_cpu_usage: unable to fetch metrics from custom metrics API: TimeSeriesPrediction is not ready. ' reason : FailedGetPodsMetric status : \"False\" type : ScalingActive \u539f\u56e0\uff1a\u4e0d\u662f\u6240\u6709\u7684\u5de5\u4f5c\u8d1f\u8f7d\u7684 CPU \u4f7f\u7528\u7387\u90fd\u662f\u53ef\u9884\u6d4b\u7684\uff0c\u5f53\u65e0\u6cd5\u9884\u6d4b\u65f6\u5c31\u4f1a\u663e\u793a\u4ee5\u4e0a\u9519\u8bef\u3002 \u89e3\u51b3\u65b9\u6848\uff1a \u7b49\u4e00\u6bb5\u65f6\u95f4\u518d\u770b\u3002\u9884\u6d4b\u7b97\u6cd5 DSP \u9700\u8981\u4e00\u5b9a\u65f6\u95f4\u7684\u6570\u636e\u624d\u80fd\u8fdb\u884c\u9884\u6d4b\u3002\u5e0c\u671b\u4e86\u89e3\u7b97\u6cd5\u7ec6\u8282\u7684\u53ef\u4ee5\u67e5\u770b\u7b97\u6cd5\u7684\u6587\u6863\u3002 EffectiveHorizontalPodAutoscaler \u63d0\u4f9b\u4e00\u79cd\u4fdd\u62a4\u673a\u5236\uff0c\u5f53\u9884\u6d4b\u5931\u6548\u65f6\u4f9d\u7136\u80fd\u901a\u8fc7\u5b9e\u9645\u7684 CPU \u4f7f\u7528\u7387\u5de5\u4f5c\u3002","title":"\u9519\u8bef: unable to get metric crane_pod_cpu_usage"},{"location":"zh/tutorials/using-qos-ensurance/","text":"Qos Ensurance \u00b6 Qos Ensurance \u4fdd\u8bc1\u4e86\u8fd0\u884c\u5728 Kubernetes \u4e0a\u7684 Pod \u7684\u7a33\u5b9a\u6027\u3002 \u5f53\u8f83\u9ad8\u4f18\u5148\u7ea7\u7684 Pod \u53d7\u5230\u8d44\u6e90\u7ade\u4e89\u7684\u5f71\u54cd\u65f6\uff0cDisable Schedule\u3001Throttle\u4ee5\u53caEvict \u5c06\u5e94\u7528\u4e8e\u4f4e\u4f18\u5148\u7ea7\u7684 Pod\u3002 Qos Ensurance \u67b6\u6784 \u00b6 Qos ensurance \u7684\u67b6\u6784\u5982\u4e0b\u56fe\u6240\u793a\u3002\u5b83\u5305\u542b\u4e09\u4e2a\u6a21\u5757\u3002 State Collector \uff1a\u5b9a\u671f\u6536\u96c6\u6307\u6807 Anomaly Analyzer \uff1a\u4f7f\u7528\u6536\u96c6\u6307\u6807\uff0c\u4ee5\u5206\u6790\u8282\u70b9\u662f\u5426\u53d1\u751f\u5f02\u5e38 Action Executor \uff1a\u6267\u884c\u56de\u907f\u52a8\u4f5c\uff0c\u5305\u62ec Disable Scheduling\u3001Throttle \u548c Eviction\u3002 \u4e3b\u8981\u6d41\u7a0b\uff1a State Collector \u4ece kube-apiserver \u540c\u6b65\u7b56\u7565\u3002 \u5982\u679c\u7b56\u7565\u53d1\u751f\u66f4\u6539\uff0c State Collector \u4f1a\u66f4\u65b0\u6307\u6807\u6536\u96c6\u89c4\u5219\u3002 State Collector \u5b9a\u671f\u6536\u96c6\u6307\u6807\u3002 State Collector \u5c06\u6307\u6807\u4f20\u8f93\u5230 Anomaly Analyzer \u3002 Anomaly Analyzer \u5bf9\u6240\u6709\u89c4\u5219\u8fdb\u884c\u8303\u56f4\u5206\u6790\uff0c\u4ee5\u5206\u6790\u8fbe\u5230\u7684\u56de\u907f\u9608\u503c\u6216\u6062\u590d\u9608\u503c\u3002 Anomaly Analyzer \u5408\u5e76\u5206\u6790\u7ed3\u679c\u5e76\u901a\u77e5 Action Executor \u6267\u884c\u56de\u907f\u52a8\u4f5c\u3002 Action Executor \u6839\u636e\u5206\u6790\u7ed3\u679c\u6267\u884c\u52a8\u4f5c\u3002 Disable Scheduling \u00b6 \u5b9a\u4e49 AvoidanceAction \u548c NodeQOSEnsurancePolicy \u3002 \u5f53\u8282\u70b9 CPU \u4f7f\u7528\u7387\u89e6\u53d1\u56de\u907f\u9608\u503c\u65f6\uff0c\u5c06\u8be5\u8282\u70b9\u8bbe\u7f6e\u4e3a\u7981\u7528\u8c03\u5ea6\u3002 \u793a\u4f8b YAML \u5982\u4e0b\u6240\u793a\uff1a AvoidanceAction apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : labels : app : system name : disablescheduling spec : description : disable schedule new pods to the node coolDownSeconds : 300 #(1) \u8282\u70b9\u4ece\u7981\u6b62\u8c03\u5ea6\u72b6\u6001\u5230\u6b63\u5e38\u72b6\u6001\u7684\u6700\u5c0f\u7b49\u5f85\u65f6\u95f4 NodeQOSEnsurancePolicy apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline1\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 #(1) restoreThreshold : 2 #(2) actionName : \"disablescheduling\" #(3) strategy : \"None\" #(4) metricRule : name : \"cpu_total_usage\" #(5) value : 4000 #(6) \u5f53\u8fbe\u5230\u9608\u503c\u5e76\u6301\u7eed\u591a\u6b21\uff0c\u90a3\u4e48\u6211\u4eec\u8ba4\u4e3a\u89c4\u5219\u88ab\u89e6\u53d1 \u5f53\u9608\u503c\u672a\u8fbe\u5230\u5e76\u7ee7\u7eed\u591a\u6b21, \u90a3\u4e48\u6211\u4eec\u8ba4\u4e3a\u89c4\u5219\u5df2\u6062\u590d \u5173\u8054\u5230 AvoidanceAction \u540d\u79f0 \u52a8\u4f5c\u7684\u7b56\u7565\uff0c\u4f60\u53ef\u4ee5\u5c06\u5176\u8bbe\u7f6e\u4e3a\u201c\u9884\u89c8\u201d\u4ee5\u4e0d\u5b9e\u9645\u6267\u884c \u6307\u6807\u540d\u79f0 \u6307\u6807\u7684\u9608\u503c \u8bf7\u89c2\u770b\u89c6\u9891\u4ee5\u4e86\u89e3\u66f4\u591a Disable Scheduling \u7684\u7ec6\u8282\u3002 Throttle \u00b6 \u5b9a\u4e49 AvoidanceAction \u548c NodeQOSEnsurancePolicy \u3002 \u5f53\u8282\u70b9 CPU \u4f7f\u7528\u7387\u89e6\u53d1\u56de\u907f\u9608\u503c\u65f6\uff0c\u5c06\u6267\u884c\u8282\u70b9\u7684 Throttle Action \u3002 \u793a\u4f8b YAML \u5982\u4e0b\u6240\u793a\uff1a AvoidanceAction apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : throttle labels : app : system spec : coolDownSeconds : 300 throttle : cpuThrottle : minCPURatio : 10 #(1) stepCPURatio : 10 #(2) description : \"throttle low priority pods\" CPU \u914d\u989d\u7684\u6700\u5c0f\u6bd4\u4f8b\uff0c\u5982\u679c pod \u88ab\u9650\u5236\u4f4e\u4e8e\u8fd9\u4e2a\u6bd4\u4f8b\uff0c\u5c31\u4f1a\u88ab\u8bbe\u7f6e\u4e3a\u8fd9\u4e2a\u3002 \u8be5\u914d\u7f6e\u8bbe\u7f6e\u7ed9 Throttle Action \u3002\u5b83\u5c06\u5728\u6bcf\u4e2a\u89e6\u53d1\u7684\u56de\u907f\u52a8\u4f5c\u4e2d\u51cf\u5c11\u8fd9\u4e2a CPU \u914d\u989d\u5360\u6bd4\u3002\u5b83\u4f1a\u5728\u6bcf\u4e2a\u6062\u590d\u52a8\u4f5c\u4e2d\u589e\u52a0\u8fd9\u4e2a CPU \u914d\u989d\u5360\u6bd4\u3002 NodeQOSEnsurancePolicy apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline2\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoredThreshold : 2 actionName : \"throttle\" strategy : \"None\" metricRule : name : \"cpu_total_usage\" value : 6000 Eviction \u00b6 \u4e0b\u9762\u7684 YAML \u662f\u53e6\u4e00\u79cd\u60c5\u51b5\uff0c\u5f53\u8282\u70b9 CPU \u4f7f\u7528\u7387\u89e6\u53d1\u9608\u503c\u65f6\uff0c\u8282\u70b9\u4e0a\u7684\u4f4e\u4f18\u5148\u7ea7 pod \u5c06\u88ab\u9a71\u9010\u3002 AvoidanceAction apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : eviction labels : app : system spec : coolDownSeconds : 300 eviction : terminationGracePeriodSeconds : 30 #(1) description : \"evict low priority pods\" pod \u9700\u8981\u4f18\u96c5\u7ec8\u6b62\u7684\u6301\u7eed\u65f6\u95f4\uff08\u4ee5\u79d2\u4e3a\u5355\u4f4d\uff09\u3002 NodeQOSEnsurancePolicy apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline3\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoreThreshold : 2 actionName : \"evict\" strategy : \"Preview\" #(1) metricRule : name : \"cpu_total_usage\" value : 6000 \u56de\u907f\u52a8\u4f5c\u7b56\u7565\u3002\u5f53\u8bbe\u7f6e\u4e3a Preview \u65f6\uff0c\u5c06\u4e0d\u4f1a\u88ab\u5b9e\u9645\u6267\u884c Supported Metrics \u00b6 Name Description cpu_total_usage node cpu usage cpu_total_utilization node cpu utilization","title":"Qos Ensurance"},{"location":"zh/tutorials/using-qos-ensurance/#qos-ensurance","text":"Qos Ensurance \u4fdd\u8bc1\u4e86\u8fd0\u884c\u5728 Kubernetes \u4e0a\u7684 Pod \u7684\u7a33\u5b9a\u6027\u3002 \u5f53\u8f83\u9ad8\u4f18\u5148\u7ea7\u7684 Pod \u53d7\u5230\u8d44\u6e90\u7ade\u4e89\u7684\u5f71\u54cd\u65f6\uff0cDisable Schedule\u3001Throttle\u4ee5\u53caEvict \u5c06\u5e94\u7528\u4e8e\u4f4e\u4f18\u5148\u7ea7\u7684 Pod\u3002","title":"Qos Ensurance"},{"location":"zh/tutorials/using-qos-ensurance/#qos-ensurance_1","text":"Qos ensurance \u7684\u67b6\u6784\u5982\u4e0b\u56fe\u6240\u793a\u3002\u5b83\u5305\u542b\u4e09\u4e2a\u6a21\u5757\u3002 State Collector \uff1a\u5b9a\u671f\u6536\u96c6\u6307\u6807 Anomaly Analyzer \uff1a\u4f7f\u7528\u6536\u96c6\u6307\u6807\uff0c\u4ee5\u5206\u6790\u8282\u70b9\u662f\u5426\u53d1\u751f\u5f02\u5e38 Action Executor \uff1a\u6267\u884c\u56de\u907f\u52a8\u4f5c\uff0c\u5305\u62ec Disable Scheduling\u3001Throttle \u548c Eviction\u3002 \u4e3b\u8981\u6d41\u7a0b\uff1a State Collector \u4ece kube-apiserver \u540c\u6b65\u7b56\u7565\u3002 \u5982\u679c\u7b56\u7565\u53d1\u751f\u66f4\u6539\uff0c State Collector \u4f1a\u66f4\u65b0\u6307\u6807\u6536\u96c6\u89c4\u5219\u3002 State Collector \u5b9a\u671f\u6536\u96c6\u6307\u6807\u3002 State Collector \u5c06\u6307\u6807\u4f20\u8f93\u5230 Anomaly Analyzer \u3002 Anomaly Analyzer \u5bf9\u6240\u6709\u89c4\u5219\u8fdb\u884c\u8303\u56f4\u5206\u6790\uff0c\u4ee5\u5206\u6790\u8fbe\u5230\u7684\u56de\u907f\u9608\u503c\u6216\u6062\u590d\u9608\u503c\u3002 Anomaly Analyzer \u5408\u5e76\u5206\u6790\u7ed3\u679c\u5e76\u901a\u77e5 Action Executor \u6267\u884c\u56de\u907f\u52a8\u4f5c\u3002 Action Executor \u6839\u636e\u5206\u6790\u7ed3\u679c\u6267\u884c\u52a8\u4f5c\u3002","title":"Qos Ensurance \u67b6\u6784"},{"location":"zh/tutorials/using-qos-ensurance/#disable-scheduling","text":"\u5b9a\u4e49 AvoidanceAction \u548c NodeQOSEnsurancePolicy \u3002 \u5f53\u8282\u70b9 CPU \u4f7f\u7528\u7387\u89e6\u53d1\u56de\u907f\u9608\u503c\u65f6\uff0c\u5c06\u8be5\u8282\u70b9\u8bbe\u7f6e\u4e3a\u7981\u7528\u8c03\u5ea6\u3002 \u793a\u4f8b YAML \u5982\u4e0b\u6240\u793a\uff1a AvoidanceAction apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : labels : app : system name : disablescheduling spec : description : disable schedule new pods to the node coolDownSeconds : 300 #(1) \u8282\u70b9\u4ece\u7981\u6b62\u8c03\u5ea6\u72b6\u6001\u5230\u6b63\u5e38\u72b6\u6001\u7684\u6700\u5c0f\u7b49\u5f85\u65f6\u95f4 NodeQOSEnsurancePolicy apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline1\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 #(1) restoreThreshold : 2 #(2) actionName : \"disablescheduling\" #(3) strategy : \"None\" #(4) metricRule : name : \"cpu_total_usage\" #(5) value : 4000 #(6) \u5f53\u8fbe\u5230\u9608\u503c\u5e76\u6301\u7eed\u591a\u6b21\uff0c\u90a3\u4e48\u6211\u4eec\u8ba4\u4e3a\u89c4\u5219\u88ab\u89e6\u53d1 \u5f53\u9608\u503c\u672a\u8fbe\u5230\u5e76\u7ee7\u7eed\u591a\u6b21, \u90a3\u4e48\u6211\u4eec\u8ba4\u4e3a\u89c4\u5219\u5df2\u6062\u590d \u5173\u8054\u5230 AvoidanceAction \u540d\u79f0 \u52a8\u4f5c\u7684\u7b56\u7565\uff0c\u4f60\u53ef\u4ee5\u5c06\u5176\u8bbe\u7f6e\u4e3a\u201c\u9884\u89c8\u201d\u4ee5\u4e0d\u5b9e\u9645\u6267\u884c \u6307\u6807\u540d\u79f0 \u6307\u6807\u7684\u9608\u503c \u8bf7\u89c2\u770b\u89c6\u9891\u4ee5\u4e86\u89e3\u66f4\u591a Disable Scheduling \u7684\u7ec6\u8282\u3002","title":"Disable Scheduling"},{"location":"zh/tutorials/using-qos-ensurance/#throttle","text":"\u5b9a\u4e49 AvoidanceAction \u548c NodeQOSEnsurancePolicy \u3002 \u5f53\u8282\u70b9 CPU \u4f7f\u7528\u7387\u89e6\u53d1\u56de\u907f\u9608\u503c\u65f6\uff0c\u5c06\u6267\u884c\u8282\u70b9\u7684 Throttle Action \u3002 \u793a\u4f8b YAML \u5982\u4e0b\u6240\u793a\uff1a AvoidanceAction apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : throttle labels : app : system spec : coolDownSeconds : 300 throttle : cpuThrottle : minCPURatio : 10 #(1) stepCPURatio : 10 #(2) description : \"throttle low priority pods\" CPU \u914d\u989d\u7684\u6700\u5c0f\u6bd4\u4f8b\uff0c\u5982\u679c pod \u88ab\u9650\u5236\u4f4e\u4e8e\u8fd9\u4e2a\u6bd4\u4f8b\uff0c\u5c31\u4f1a\u88ab\u8bbe\u7f6e\u4e3a\u8fd9\u4e2a\u3002 \u8be5\u914d\u7f6e\u8bbe\u7f6e\u7ed9 Throttle Action \u3002\u5b83\u5c06\u5728\u6bcf\u4e2a\u89e6\u53d1\u7684\u56de\u907f\u52a8\u4f5c\u4e2d\u51cf\u5c11\u8fd9\u4e2a CPU \u914d\u989d\u5360\u6bd4\u3002\u5b83\u4f1a\u5728\u6bcf\u4e2a\u6062\u590d\u52a8\u4f5c\u4e2d\u589e\u52a0\u8fd9\u4e2a CPU \u914d\u989d\u5360\u6bd4\u3002 NodeQOSEnsurancePolicy apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline2\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoredThreshold : 2 actionName : \"throttle\" strategy : \"None\" metricRule : name : \"cpu_total_usage\" value : 6000","title":"Throttle"},{"location":"zh/tutorials/using-qos-ensurance/#eviction","text":"\u4e0b\u9762\u7684 YAML \u662f\u53e6\u4e00\u79cd\u60c5\u51b5\uff0c\u5f53\u8282\u70b9 CPU \u4f7f\u7528\u7387\u89e6\u53d1\u9608\u503c\u65f6\uff0c\u8282\u70b9\u4e0a\u7684\u4f4e\u4f18\u5148\u7ea7 pod \u5c06\u88ab\u9a71\u9010\u3002 AvoidanceAction apiVersion : ensurance.crane.io/v1alpha1 kind : AvoidanceAction metadata : name : eviction labels : app : system spec : coolDownSeconds : 300 eviction : terminationGracePeriodSeconds : 30 #(1) description : \"evict low priority pods\" pod \u9700\u8981\u4f18\u96c5\u7ec8\u6b62\u7684\u6301\u7eed\u65f6\u95f4\uff08\u4ee5\u79d2\u4e3a\u5355\u4f4d\uff09\u3002 NodeQOSEnsurancePolicy apiVersion : ensurance.crane.io/v1alpha1 kind : NodeQOSEnsurancePolicy metadata : name : \"waterline3\" labels : app : \"system\" spec : nodeQualityProbe : timeoutSeconds : 10 nodeLocalGet : localCacheTTLSeconds : 60 objectiveEnsurances : - name : \"cpu-usage\" avoidanceThreshold : 2 restoreThreshold : 2 actionName : \"evict\" strategy : \"Preview\" #(1) metricRule : name : \"cpu_total_usage\" value : 6000 \u56de\u907f\u52a8\u4f5c\u7b56\u7565\u3002\u5f53\u8bbe\u7f6e\u4e3a Preview \u65f6\uff0c\u5c06\u4e0d\u4f1a\u88ab\u5b9e\u9645\u6267\u884c","title":"Eviction"},{"location":"zh/tutorials/using-qos-ensurance/#supported-metrics","text":"Name Description cpu_total_usage node cpu usage cpu_total_utilization node cpu utilization","title":"Supported Metrics"},{"location":"zh/tutorials/using-time-series-prediction/","text":"TimeSeriesPrediction \u00b6 Overview \u00b6 Knowing the future makes things easier for us. \u8bb8\u591a\u4e1a\u52a1\u5728\u65f6\u95f4\u5e8f\u5217\u4e0a\u5929\u7136\u5b58\u5728\u5468\u671f\u6027\u7684\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u90a3\u4e9b\u76f4\u63a5\u6216\u95f4\u63a5\u4e3a\u201c\u4eba\u201d\u670d\u52a1\u7684\u4e1a\u52a1\u3002\u8fd9\u79cd\u5468\u671f\u6027\u662f\u7531\u4eba\u4eec\u65e5\u5e38\u6d3b\u52a8\u7684\u89c4\u5f8b\u6027\u51b3\u5b9a\u7684\u3002\u4f8b\u5982\uff0c\u4eba\u4eec\u4e60\u60ef\u4e8e\u4e2d\u5348\u548c\u665a\u4e0a\u70b9\u5916\u5356\uff1b\u65e9\u665a\u603b\u6709\u4ea4\u901a\u9ad8\u5cf0\uff1b\u5373\u4f7f\u662f\u641c\u7d22\u7b49\u6a21\u5f0f\u4e0d\u90a3\u4e48\u660e\u663e\u7684\u670d\u52a1\uff0c\u591c\u95f4\u7684\u8bf7\u6c42\u91cf\u4e5f\u8fdc\u4f4e\u4e8e\u767d\u5929\u65f6\u95f4\u3002\u5bf9\u4e8e\u8fd9\u7c7b\u4e1a\u52a1\u76f8\u5173\u7684\u5e94\u7528\u6765\u8bf4\uff0c\u4ece\u8fc7\u53bb\u51e0\u5929\u7684\u5386\u53f2\u6570\u636e\u4e2d\u63a8\u65ad\u51fa\u6b21\u65e5\u7684\u6307\u6807\uff0c\u6216\u8005\u4ece\u4e0a\u5468\u4e00\u7684\u6570\u636e\u4e2d\u63a8\u65ad\u51fa\u4e0b\u5468\u4e00\u7684\u8bbf\u95ee\u91cf\u662f\u5f88\u81ea\u7136\u7684\u60f3\u6cd5\u3002\u901a\u8fc7\u9884\u6d4b\u672a\u6765 24 \u5c0f\u65f6\u5185\u7684\u6307\u6807\u6216\u6d41\u91cf\u6a21\u5f0f\uff0c\u6211\u4eec\u53ef\u4ee5\u66f4\u597d\u5730\u7ba1\u7406\u6211\u4eec\u7684\u5e94\u7528\u7a0b\u5e8f\u5b9e\u4f8b\uff0c\u7a33\u5b9a\u6211\u4eec\u7684\u7cfb\u7edf\uff0c\u540c\u65f6\u964d\u4f4e\u6210\u672c\u3002 TimeSeriesPrediction \u88ab\u7528\u4e8e\u9884\u6d4b Kubernetes \u5bf9\u8c61\u6307\u6807\u3002\u5b83\u57fa\u4e8e PredictionCore \u8fdb\u884c\u9884\u6d4b\u3002 Features \u00b6 TimeSeriesPrediction \u7684\u793a\u4f8b yaml \u5982\u4e0b\u6240\u793a\uff1a TimeSeriesPrediction apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : targetRef : kind : Node name : 192.168.56.166 predictionWindowSeconds : 600 predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" - resourceIdentifier : node-mem type : ResourceQuery resourceQuery : memory algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"1000000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" spec.targetRef \u5b9a\u4e49\u4e86\u5bf9 Kubernetes \u5bf9\u8c61\u7684\u5f15\u7528\uff0c\u5305\u62ec Node \u6216\u5176\u4ed6\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4f8b\u5982 Deployment\u3002 spec.predictionMetrics \u5b9a\u4e49\u4e86\u5173\u4e8e spec.targetRef \u7684\u6307\u6807\u3002 spec.predictionWindowSeconds \u662f\u9884\u6d4b\u65f6\u95f4\u5e8f\u5217\u6301\u7eed\u65f6\u95f4\u3002 TimeSeriesPredictionController \u5c06\u8f6e\u6362 spec.Status \u4e2d\u7684\u9884\u6d4b\u6570\u636e\uff0c\u4ee5\u4f9b\u6d88\u8d39\u8005\u4f7f\u7528\u9884\u6d4b\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002 Prediction Metrics \u00b6 TimeSeriesPrediction apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" Metric Type \u00b6 \u73b0\u5728\u6211\u4eec\u53ea\u652f\u6301 prometheus \u4f5c\u4e3a\u6570\u636e\u6e90\u3002\u6211\u4eec\u5b9a\u4e49 MetricType \u4e0e\u6570\u636e\u6e90\u8fdb\u884c\u7ed3\u5408\u3002\u4f46\u662f\u73b0\u5728\u53ef\u80fd\u6709\u4e9b\u6570\u636e\u6e90\u4e0d\u652f\u6301 MetricType \u3002 \u6307\u6807\u67e5\u8be2\u6709\u4ee5\u4e0b\u4e09\u79cd\u7c7b\u578b\uff1a ResourceQuery \u662f kubernetes \u5185\u7f6e\u7684\u8d44\u6e90\u6307\u6807\uff0c\u4f8b\u5982 cpu \u6216 memory\u3002Crane\u76ee\u524d\u53ea\u652f\u6301 CPU \u548c\u5185\u5b58\u3002 RawQuery \u662f\u901a\u8fc7 DSL \u7684\u67e5\u8be2\uff0c\u6bd4\u5982 prometheus \u67e5\u8be2\u8bed\u53e5\u3002\u73b0\u5728\u5df2\u652f\u6301 Prometheus \u3002 ExpressionQuery \u662f\u4e00\u4e2a\u8868\u8fbe\u5f0f\u67e5\u8be2\u3002 Algorithm \u00b6 Algorithm \u5b9a\u4e49\u7b97\u6cd5\u7c7b\u578b\u548c\u53c2\u6570\u6765\u9884\u6d4b\u6307\u6807\u3002\u73b0\u5728\u6709\u4e24\u79cd\u7b97\u6cd5\uff1a dsp \u662f\u4e00\u79cd\u9884\u6d4b\u65f6\u95f4\u5e8f\u5217\u7684\u7b97\u6cd5\uff0c\u5b83\u57fa\u4e8e FFT\uff08\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff09\uff0c\u64c5\u957f\u9884\u6d4b\u4e00\u4e9b\u5177\u6709\u5b63\u8282\u6027\u548c\u5468\u671f\u7684\u65f6\u95f4\u5e8f\u5217\u3002 percentile \u662f\u4e00\u79cd\u4f30\u8ba1\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u627e\u5230\u4ee3\u8868\u8fc7\u53bb\u65f6\u95f4\u5e8f\u5217\u7684\u63a8\u8350\u503c\u7684\u7b97\u6cd5\uff0c\u5b83\u57fa\u4e8e\u6307\u6570\u8870\u51cf\u6743\u91cd\u76f4\u65b9\u56fe\u7edf\u8ba1\u3002\u5b83\u662f\u7528\u6765\u4f30\u8ba1\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\u7684\uff0c\u5b83\u4e0d\u64c5\u957f\u9884\u6d4b\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\uff0c\u867d\u7136 percentile \u53ef\u4ee5\u8f93\u51fa\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\u7684\u9884\u6d4b\u6570\u636e\uff0c\u4f46\u662f\u90fd\u662f\u4e00\u6837\u7684\u503c\u3002 \u6240\u4ee5\u5982\u679c\u4f60\u60f3\u9884\u6d4b\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\uff0cdsp \u662f\u4e00\u4e2a\u66f4\u597d\u7684\u9009\u62e9\u3002 dsp params \u00b6 percentile params \u00b6","title":"Time Series Prediction"},{"location":"zh/tutorials/using-time-series-prediction/#timeseriesprediction","text":"","title":"TimeSeriesPrediction"},{"location":"zh/tutorials/using-time-series-prediction/#overview","text":"Knowing the future makes things easier for us. \u8bb8\u591a\u4e1a\u52a1\u5728\u65f6\u95f4\u5e8f\u5217\u4e0a\u5929\u7136\u5b58\u5728\u5468\u671f\u6027\u7684\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u90a3\u4e9b\u76f4\u63a5\u6216\u95f4\u63a5\u4e3a\u201c\u4eba\u201d\u670d\u52a1\u7684\u4e1a\u52a1\u3002\u8fd9\u79cd\u5468\u671f\u6027\u662f\u7531\u4eba\u4eec\u65e5\u5e38\u6d3b\u52a8\u7684\u89c4\u5f8b\u6027\u51b3\u5b9a\u7684\u3002\u4f8b\u5982\uff0c\u4eba\u4eec\u4e60\u60ef\u4e8e\u4e2d\u5348\u548c\u665a\u4e0a\u70b9\u5916\u5356\uff1b\u65e9\u665a\u603b\u6709\u4ea4\u901a\u9ad8\u5cf0\uff1b\u5373\u4f7f\u662f\u641c\u7d22\u7b49\u6a21\u5f0f\u4e0d\u90a3\u4e48\u660e\u663e\u7684\u670d\u52a1\uff0c\u591c\u95f4\u7684\u8bf7\u6c42\u91cf\u4e5f\u8fdc\u4f4e\u4e8e\u767d\u5929\u65f6\u95f4\u3002\u5bf9\u4e8e\u8fd9\u7c7b\u4e1a\u52a1\u76f8\u5173\u7684\u5e94\u7528\u6765\u8bf4\uff0c\u4ece\u8fc7\u53bb\u51e0\u5929\u7684\u5386\u53f2\u6570\u636e\u4e2d\u63a8\u65ad\u51fa\u6b21\u65e5\u7684\u6307\u6807\uff0c\u6216\u8005\u4ece\u4e0a\u5468\u4e00\u7684\u6570\u636e\u4e2d\u63a8\u65ad\u51fa\u4e0b\u5468\u4e00\u7684\u8bbf\u95ee\u91cf\u662f\u5f88\u81ea\u7136\u7684\u60f3\u6cd5\u3002\u901a\u8fc7\u9884\u6d4b\u672a\u6765 24 \u5c0f\u65f6\u5185\u7684\u6307\u6807\u6216\u6d41\u91cf\u6a21\u5f0f\uff0c\u6211\u4eec\u53ef\u4ee5\u66f4\u597d\u5730\u7ba1\u7406\u6211\u4eec\u7684\u5e94\u7528\u7a0b\u5e8f\u5b9e\u4f8b\uff0c\u7a33\u5b9a\u6211\u4eec\u7684\u7cfb\u7edf\uff0c\u540c\u65f6\u964d\u4f4e\u6210\u672c\u3002 TimeSeriesPrediction \u88ab\u7528\u4e8e\u9884\u6d4b Kubernetes \u5bf9\u8c61\u6307\u6807\u3002\u5b83\u57fa\u4e8e PredictionCore \u8fdb\u884c\u9884\u6d4b\u3002","title":"Overview"},{"location":"zh/tutorials/using-time-series-prediction/#features","text":"TimeSeriesPrediction \u7684\u793a\u4f8b yaml \u5982\u4e0b\u6240\u793a\uff1a TimeSeriesPrediction apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : targetRef : kind : Node name : 192.168.56.166 predictionWindowSeconds : 600 predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" - resourceIdentifier : node-mem type : ResourceQuery resourceQuery : memory algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"1000000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\" spec.targetRef \u5b9a\u4e49\u4e86\u5bf9 Kubernetes \u5bf9\u8c61\u7684\u5f15\u7528\uff0c\u5305\u62ec Node \u6216\u5176\u4ed6\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4f8b\u5982 Deployment\u3002 spec.predictionMetrics \u5b9a\u4e49\u4e86\u5173\u4e8e spec.targetRef \u7684\u6307\u6807\u3002 spec.predictionWindowSeconds \u662f\u9884\u6d4b\u65f6\u95f4\u5e8f\u5217\u6301\u7eed\u65f6\u95f4\u3002 TimeSeriesPredictionController \u5c06\u8f6e\u6362 spec.Status \u4e2d\u7684\u9884\u6d4b\u6570\u636e\uff0c\u4ee5\u4f9b\u6d88\u8d39\u8005\u4f7f\u7528\u9884\u6d4b\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002","title":"Features"},{"location":"zh/tutorials/using-time-series-prediction/#prediction-metrics","text":"TimeSeriesPrediction apiVersion : prediction.crane.io/v1alpha1 kind : TimeSeriesPrediction metadata : name : node-resource-percentile namespace : default spec : predictionMetrics : - resourceIdentifier : node-cpu type : ResourceQuery resourceQuery : cpu algorithm : algorithmType : \"percentile\" percentile : sampleInterval : \"1m\" minSampleWeight : \"1.0\" histogram : maxValue : \"10000.0\" epsilon : \"1e-10\" halfLife : \"12h\" bucketSize : \"10\" firstBucketSize : \"40\" bucketSizeGrowthRatio : \"1.5\"","title":"Prediction Metrics"},{"location":"zh/tutorials/using-time-series-prediction/#metric-type","text":"\u73b0\u5728\u6211\u4eec\u53ea\u652f\u6301 prometheus \u4f5c\u4e3a\u6570\u636e\u6e90\u3002\u6211\u4eec\u5b9a\u4e49 MetricType \u4e0e\u6570\u636e\u6e90\u8fdb\u884c\u7ed3\u5408\u3002\u4f46\u662f\u73b0\u5728\u53ef\u80fd\u6709\u4e9b\u6570\u636e\u6e90\u4e0d\u652f\u6301 MetricType \u3002 \u6307\u6807\u67e5\u8be2\u6709\u4ee5\u4e0b\u4e09\u79cd\u7c7b\u578b\uff1a ResourceQuery \u662f kubernetes \u5185\u7f6e\u7684\u8d44\u6e90\u6307\u6807\uff0c\u4f8b\u5982 cpu \u6216 memory\u3002Crane\u76ee\u524d\u53ea\u652f\u6301 CPU \u548c\u5185\u5b58\u3002 RawQuery \u662f\u901a\u8fc7 DSL \u7684\u67e5\u8be2\uff0c\u6bd4\u5982 prometheus \u67e5\u8be2\u8bed\u53e5\u3002\u73b0\u5728\u5df2\u652f\u6301 Prometheus \u3002 ExpressionQuery \u662f\u4e00\u4e2a\u8868\u8fbe\u5f0f\u67e5\u8be2\u3002","title":"Metric Type"},{"location":"zh/tutorials/using-time-series-prediction/#algorithm","text":"Algorithm \u5b9a\u4e49\u7b97\u6cd5\u7c7b\u578b\u548c\u53c2\u6570\u6765\u9884\u6d4b\u6307\u6807\u3002\u73b0\u5728\u6709\u4e24\u79cd\u7b97\u6cd5\uff1a dsp \u662f\u4e00\u79cd\u9884\u6d4b\u65f6\u95f4\u5e8f\u5217\u7684\u7b97\u6cd5\uff0c\u5b83\u57fa\u4e8e FFT\uff08\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff09\uff0c\u64c5\u957f\u9884\u6d4b\u4e00\u4e9b\u5177\u6709\u5b63\u8282\u6027\u548c\u5468\u671f\u7684\u65f6\u95f4\u5e8f\u5217\u3002 percentile \u662f\u4e00\u79cd\u4f30\u8ba1\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u627e\u5230\u4ee3\u8868\u8fc7\u53bb\u65f6\u95f4\u5e8f\u5217\u7684\u63a8\u8350\u503c\u7684\u7b97\u6cd5\uff0c\u5b83\u57fa\u4e8e\u6307\u6570\u8870\u51cf\u6743\u91cd\u76f4\u65b9\u56fe\u7edf\u8ba1\u3002\u5b83\u662f\u7528\u6765\u4f30\u8ba1\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\u7684\uff0c\u5b83\u4e0d\u64c5\u957f\u9884\u6d4b\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\uff0c\u867d\u7136 percentile \u53ef\u4ee5\u8f93\u51fa\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\u7684\u9884\u6d4b\u6570\u636e\uff0c\u4f46\u662f\u90fd\u662f\u4e00\u6837\u7684\u503c\u3002 \u6240\u4ee5\u5982\u679c\u4f60\u60f3\u9884\u6d4b\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\uff0cdsp \u662f\u4e00\u4e2a\u66f4\u597d\u7684\u9009\u62e9\u3002","title":"Algorithm"},{"location":"zh/tutorials/using-time-series-prediction/#dsp-params","text":"","title":"dsp params"},{"location":"zh/tutorials/using-time-series-prediction/#percentile-params","text":"","title":"percentile params"}]}